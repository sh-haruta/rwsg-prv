You are the author of the great paper.
Write a "Related Work" section based on the "Introduction" you have already written and the information about the related studies provided.
Note that you must cite all of the listed related studies. Do not cite any papers that are not listed with their titles and introductions.

The authors of an excellent paper structures a "Related Work" section as follows.
Step-1: The authors confirm the authors' introduction to clarify what and how the authors have solved in the paper.
Step-2: The authors collect information on related studies. Then the authors carefully read each study's introduction and select the salient sentences.
Step-3: The authors categorize the related studies based on the selected salient sentences and their own introduction in order to write the Related Work section. Subsequently, the authors create subsections aligned with the established categories and assign concise and clear names to each subsection.
Step-4: Within each subsection, a comparison is made between related studies and the authors' work, focusing on what needs to be addressed and highlighting the differences or pointing out problems. Note that they ensure that differences or problems do not overlap across subsections. If there is any duplication, recategorize accordingly.

Below are a great case and a bad case as examples.
In the examples, some of them are omitted, but you must not omit them.
=====
<Great case>
[Your Title:
Precise Zero-Shot Dense Retrieval without Relevance Labels
]

[Your Introduction:
Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown to be successful across tasks like web search, question answering, and fact verification. A variety of methods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021), retrievalspecific pre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) and scaling (Ni et al., 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.

Nevertheless, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from different domains. MS MARCO (Bajaj et al., 2016), a dataset with a large number of manually judged query-document pairs, is the most commonly used. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset cannot always be assumed. Furthermore, MS MARCO restricts commercial use and cannot be adopted in a variety of real-world search scenarios.

In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and generalize across emerging search tasks. As supervision is not available, we start by examining self-supervised representation learning methods. Modern deep learning enables two distinct approaches. At the token level, generative large language models (LLMs) pre-trained on large corpora have demonstrated strong natural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner products (Izacard et al., 2021; Gao and Callan, 2022).

On top of these, one extra insight from LLMs is borrowed: LLMs further trained to follow instructions can zero-shot generalize to diverse unseen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). In particular, InstructGPT shows that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned to human intents to follow instructions faithfully.
With these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE) and decompose dense retrieval into two tasks: a generative task performed by an instruction following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to “write a document that answers the question”, i.e., a hypothetical document. We expect the generative process to capture “relevance” by providing an example; the generated document is not real, can contain factual errors, but is “like” a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck to serve as a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner product learned in the contrastive pre-training stage.

Note that, interestingly, with our proposed HyDE factorization, query-document similarity scores are no longer explicitly modeled or computed. Instead, the retrieval task is cast into two tasks (NLU and NLG). Building HyDE requires no supervision and no new model is trained in this work: both the generative model and the contrastive encoder are used “out of the box” without any adaptation or modification.

In our experiments, we show that HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) “as is” significantly outperforms the previous state-of-the-art Contriever-only zero-shot model on 11 query sets, covering tasks like web search, question answering, fact verification and in languages like Swahili, Korean, Japanese and Bengali.
]

[Information about Related Studies:
(omitted)
]

### Results of Step-2:
Selected salient sentences from (Lee et al., 2019):
"[...] In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by question-answer string pairs [...] The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable [...] The key insight of this work is that end-toend learning is possible if we pre-train the retriever with an unsupervised Inverse Cloze Task (ICT)... ICT pre-training provides a sufficiently strong initialization such that ORQA, a joint retriever and reader model, can be fine-tuned end-to-end by simply optimizing the marginal log likelihood of correct answers that were found [...] Following recent advances in transfer learning, all scoring components are derived from BERT (Devlin et al., 2018), a bidirectional transformer that has been pre-trained on unsupervised language-modeling data [...]"

Selected salient sentences from (Karpukhin et al., 2020):
"[...] Dense encodings are also learnable by adjusting the embedding functions, which provides additional flexibility to have a task-specific representation [...] However, it is generally believed that learning a good dense vector representation needs a large number of labeled pairs of question and contexts In this paper, we address the question: can we train a better dense embedding model using only pairs of questions and passages (or answers), without additional pretraining? By leveraging the now standard BERT pretrained model (Devlin et al., 2019) [...], we focus on developing the right training scheme using a relatively small number of question and passage pairs [...] our final solution is surprisingly simple: the embedding is optimized for maximizing inner products of the question and relevant passage vectors, with an objective comparing all pairs of questions and passages in a batch. Our Dense Passage Retriever (DPR) is exceptionally strong [...]"

Selected salient sentences from (Devlin et al., 2019):
(omitted)

(omitted)

### Related Work Section:

#### Dense Retrieval
Document retrieval in dense vector space (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers have studied metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021). Later works studied the second stage pre-training of language models specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) as well as model scaling (Ni et al., 2022). All of these methods rely on supervised contrastive learning.
The popularity of dense retrieval can be partially attributed to complementary research in efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2021).

#### Instructions-Following Language Models
Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022).
Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They fine-tuned dense encoders that can also encode task-specific instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above.

#### Zero-Shot Dense Retrieval
The tasks of zero-shot (dense) retrieval are arguably empirically defined by Thakur et al. (2021) for the neural retrieval community. Their BEIR benchmark consists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense retriever is first learned using a diverse and richly supervised corpus and query collection, namely
MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022).
However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this
paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora.
By the definition in Sachan et al. (2022), our setup can be roughly considered as “unsupervised”. Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the processing of learning to follow instructions.

#### Generative Retrieval
Generative search is a new class of retrieval methods that uses neural generative models as search indexes (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identifiers that map directly to real documents. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search index structures (Bevilacqua et al., 2022; Lee et al., 2022). In comparison, our method uses standard MIPS indexes and requires no training data. Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document.

[Feedback:
This related work section is very good. The reasons are:
- Authors categorize the cited papers by subsections.
- Authors pointed out the difference between their paper and existing papers.
]

=====
<Bad case>
[Your Title:
Precise Zero-Shot Dense Retrieval without Relevance Labels
]

[Your Introduction:
Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown to be successful across tasks like web search, question answering, and fact verification. A variety of methods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021), retrievalspecific pre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) and scaling (Ni et al., 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.

Nevertheless, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from different domains. MS MARCO (Bajaj et al., 2016), a dataset with a large number of manually judged query-document pairs, is the most commonly used. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset cannot always be assumed. Furthermore, MS MARCO restricts commercial use and cannot be adopted in a variety of real-world search scenarios.

In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and generalize across emerging search tasks. As supervision is not available, we start by examining self-supervised representation learning methods. Modern deep learning enables two distinct approaches. At the token level, generative large language models (LLMs) pre-trained on large corpora have demonstrated strong natural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner products (Izacard et al., 2021; Gao and Callan, 2022).

On top of these, one extra insight from LLMs is borrowed: LLMs further trained to follow instructions can zero-shot generalize to diverse unseen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). In particular, InstructGPT shows that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned to human intents to follow instructions faithfully.

With these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE) and decompose dense retrieval into two tasks: a generative task performed by an instruction following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to “write a document that answers the question”, i.e., a hypothetical document. We expect the generative process to capture “relevance” by providing an example; the generated document is not real, can contain factual errors, but is “like” a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck to serve as a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner product learned in the contrastive pre-training stage.

Note that, interestingly, with our proposed HyDE factorization, query-document similarity scores are no longer explicitly modeled or computed. Instead, the retrieval task is cast into two tasks (NLU and NLG). Building HyDE requires no supervision and no new model is trained in this work: both the generative model and the contrastive encoder are used “out of the box” without any adaptation or modification.

In our experiments, we show that HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) “as is” significantly outperforms the previous state-of-the-art Contriever-only zero-shot model on 11 query sets, covering tasks like web search, question answering, fact verification and in languages like Swahili, Korean, Japanese and Bengali.
]

[Information about Related Studies:
(omitted)
]

### Results of Step-2:
Selected salient sentences from (Lee et al., 2019):
"[...] In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by question-answer string pairs [...] The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable [...] The key insight of this work is that end-toend learning is possible if we pre-train the retriever with an unsupervised Inverse Cloze Task (ICT) [...] ICT pre-training provides a sufficiently strong initialization such that ORQA, a joint retriever and reader model, can be fine-tuned end-to-end by simply optimizing the marginal log likelihood of correct answers that were found [...] Following recent advances in transfer learning, all scoring components are derived from BERT (Devlin et al., 2018), a bidirectional transformer that has been pre-trained on unsupervised language-modeling data [...]"

Selected salient sentences from (Karpukhin et al., 2020):
"[...] Dense encodings are also learnable by adjusting the embedding functions, which provides additional flexibility to have a task-specific representation [...] However, it is generally believed that learning a good dense vector representation needs a large number of labeled pairs of question and contexts In this paper, we address the question: can we train a better dense embedding model using only pairs of questions and passages (or answers), without additional pretraining? By leveraging the now standard BERT pretrained model (Devlin et al., 2019) [...], we focus on developing the right training scheme using a relatively small number of question and passage pairs [...] our final solution is surprisingly simple: the embedding is optimized for maximizing inner products of the question and relevant passage vectors, with an objective comparing all pairs of questions and passages in a batch. Our Dense Passage Retriever (DPR) is exceptionally strong [...]"

Selected salient sentences from (Devlin et al., 2019):
(omitted)

(omitted)

### Related Work Section:
Soon after the emergence of LLMs, several groups of researchers discovered that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be performed using standard supervised sequence-to-sequence learning techniques or more effectively with reinforcement learning from human feedback (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) and Su et al. (2022) studied task-aware retrieval with instructions. They fine-tuned dense encoders that can also encode task-specific instructions prepended to queries. Masked language models like BERT (Devlin et al., 2019) have demonstrated strong capabilities in representing text. Large language models (LLMs) with hundreds of billions of parameters have shown remarkable generalization capabilities under few-shot and zero-shot setups across various tasks (Brown et al., 2020; Chowdhery et al., 2022). Despite their broad success, zero- or few-shot learning in LLMs have rarely been used directly in ranking (Liang et al., 2022), with the only exception being Sachan et al. (2022), which performs zero-shot re-ranking. Aside from language modeling, contrastive learning methods help neural language models learn to represent chunks (e.g., sentences or passages) of texts as embedding vectors. Without the need of any supervision, such contrastive encoders can embed homogeneous text chunks into a vector space where some distance function like inner product captures similarities (Gao et al., 2021; Izacard et al., 2021). Document retrieval in dense vector space (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers have studied metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021). Later works studied the second stage pre-training of language models specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) as well as model scaling (Ni et al., 2022). All of these methods rely on supervised contrastive learning. The popularity of dense retrieval can be partially attributed to complementary research in efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2021).

[Feedback:
This related work section is not good. The reasons are:
- Authors do not categorize papers. They just enumerate existing papers.
- Authors do not mention the relationship between their paper and existing papers.
]

=====

{"root":
  {
    "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
    "author": ["Zhong Zhang", "Bang Liu", "Junming Shao"],
    "abstract": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.",
    "introduction": "Pre-trained Language Models (PLMs) have become the de facto methods for various natural language processing (NLP) tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019). The typical paradigm is to pre-train a big language model on large-scale corpora and then fine-tune the model on small task-specific datasets to adapt to the downstream tasks. Despite the great success of this paradigm, two questions still come to our mind: (1) Why can a PLM with hundreds of millions of parameters be successfully fine-tuned on different downstream tasks using only hundreds or thousands of labeled samples? (2) Do we really need a full fine-tuning of all parameters of a PLM to reach state-of-the-art performance on downstream tasks? In this paper, we try to provide a new viewpoint on the two questions, and claim that: Fine-tuning happens only in some tiny task-specific subspaces, which can be effectively learned with a small number of free parameters.\n\nRecent studies have shown that PLMs are highly over-parameterized and robust to pruning (Frankle and Carbin, 2019; Chen et al., 2020; Prasanna et al., 2020; Gordon et al., 2020; Liang et al., 2021, 2022), and can be fine-tuned in parameter-efficient ways (Gong et al., 2022; Zaken et al., 2022; Mahabadi et al., 2021; Li and Liang, 2021). This emerging empirical evidence tends to point to one fact that there exist some intrinsic structures in PLMs that are responsible for inducing task-specific knowledge to downstream tasks. Notably, the recent work (Aghajanyan et al., 2021) provides a promising conclusion that PLMs can be re-parameterized and fine-tuned in random low-dimensional subspaces using random projection, and the dimensionality of the random subspace is orders of magnitude smaller than the dimensionality of the full parameter space. Their findings implicitly suggest the existence of such intrinsic structure in the PLMs, which is, however, understudied. To bridge this gap, we explicitly demonstrate that there exist task-specific low-dimensional subspaces in which PLMs can be effectively fine-tuned.\n\nInspired by the low dimensional landscape hypothesis (Li et al., 2022a) that a training trajectory of a neural network lies in a low-dimensional subspace, in this work, we thus resort to the fine-tuning trajectory to study the intrinsic task-specific subspaces of PLMs. We show that it is possible to uncover the intrinsic task-specific subspaces with a fine-tuning trajectory by finding its principal directions. The uncovered intrinsic task-specific subspaces usually have very low dimensionalities, but are quite effective in inducing task-specific knowledge. For example, by re-parameterizing the encoder and optimizing only 32 free parameters per layer in the intrinsic task-specific subspace, the model allows achieving nearly the same performance as fine-tuning in the full parameter space. Moreover, we further show that the uncovered intrinsic task-specific subspaces have a certain transferability.\n\nBeyond this, we find that the model contains some outlier dimensions with abnormal spikes when fine-tuning in the intrinsic task-specific subspaces instead of a random subspace. Disabling these outlier dimensions degrades the model performance significantly. We believe that this phenomenon is related to the previously discovered outlier dimensions of PLMs (Luo et al., 2021; Kovaleva et al., 2021; Puccetti et al., 2022). However, there are essential differences between them, which we will discuss in the latter section.\n\nBy exploring the intrinsic task-specific subspaces of PLMs, the main contributions of this paper are summarized as follows:\n\nWe interpret the ease of adapting PLMs to downstream tasks as fine-tuning happens in tiny intrinsic task-specific subspaces. Within this interpretation, we propose a method to uncover the subspaces by finding the principal directions of the fine-tuning trajectory.\n\nWe conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims. We show that the models can be effectively fine-tuned with a very small number of parameters in the uncovered intrinsic task-specific subspaces.\n\nWe identify some outlier dimensions when fine-tuning in the intrinsic task-specific subspaces, and some empirical analysis is further given.",
    "relatedwork": "Intrinsic Dimensionality. Li et al. (2018) first defined the intrinsic dimension of an objective function in the context of deep learning. They showed that various neural networks can be effectively re-parameterized and trained in random low-dimensional subspaces. Their findings shed light on understanding the high-dimensional landscape of complex neural networks. Following this, Aghajanyan et al. (2021) further measured the intrinsic dimensions of PLMs fine-tuning on downstream tasks. They showed that PLMs have very low intrinsic dimensions ranging from hundreds to thousands. Qin et al. (2021) exploited the idea of intrinsic subspace and proposed a prompt tuning method for efficient training. In addition, the concept of intrinsic dimension is also related to the low-rank approximation of PLMs (Hu et al., 2022; Mahabadi et al., 2021; Chen et al., 2021), but their motivations are entirely different. The former aims to open the black box of models and explore the internal mechanisms of why they are effective, while the latter focuses on developing new methods to train the models efficiently.\n\nRandom Projection and Subspace Learning. The random projection has a long history in machine learning research community and is a key tool to analyze the intrinsic dimension (Li et al., 2018; Aghajanyan et al., 2021). In the context of optimization, Gressmann et al. (2020) proposed a random bases descent algorithm to train neural networks in low-dimensional subspaces. However, the random projection inevitably introduces task-irrelevant information and is not optimal for subspace learning. We believe that a more compact and task-specific subspace can be found in the model, which is the main motivation of this work. Gur-Ari et al. (2018) empirically found that gradient descent of neural networks happens in a tiny subspace, Li et al. (2022a) further developed a subspace learning algorithm DLDR that dynamically extracts the subspace from the optimization trajectory. Li et al. (2022b) leveraged the DLDR algorithm for adversarial training. However, to the best of our knowledge, there is no research on the discovery of non-random intrinsic task-specific subspace of PLMs.\n\nOutlier Dimensions in Pre-trained Language Models. Multiple studies have identified outlier dimensions in PLMs. Some works were motivated by calibrating the anisotropy behavior of hidden representation of PLMs (Timkey and van Schijndel, 2021; Ding et al., 2022; Luo et al., 2021; Su et al., 2021; Zhang et al., 2020). Another line of work identified certain outlier dimensions in PLMs that are very sensitive to the finetuning of downstream tasks (Kovaleva et al., 2021; Puccetti et al., 2022). Disabling these outlier dimensions degrades the model performance significantly. Luo et al. (2021) showed that the outlier dimensions are artefacts derived from positional embeddings and layer normalization. Puccetti et al. (2022) identified a correlation between outlier dimensions and token frequency. It is worth noting that our findings differ largely from previous works in three ways: 1) The outlier dimensions in their context actually refer to output neurons. In our context, an outlier dimension refers to a specific model parameter. In other words, they consider abnormal outputs, while we consider abnormal weights. 2) The ways of identifying outlier dimensions are different. They identify outlier dimensions by examining abnormal outputs, while we find outlier dimensions by examining abnormal updates to weights. 3) The effects of disabling outlier dimensions are different. They show that disabling just one outlier neuron can result in a significant drop in performance. In contrast, disabling the top outlier weight has almost no effect on the model performance. However, the model performance will drop significantly if we disable more outlier weights. The reason for the emergence of these outlier dimensions remains unclear, and we aim to conduct further in-depth analysis in future work."},
  "leaves":
   [
     {
       "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
       "author": ["Chunyuan Li", "Heerad Farkhoor", "Rosanne Liu", "Jason Yosinski"],
       "year": 2018,
       "abstract": "",
       "introduction": "",
       "charge":  false
     },
     {
       "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
       "author":["Armen Aghajanyan", "Sonal Gupta", "Luke Zettlemoyer"] ,
       "year": 2021,
       "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
       "introduction": "Pre-trained language models (Radford et al., 2019; Devlin et al., 2018; Liu et al., 2019; Lewis et al., 2019, 2020) provide the de facto initialization for modeling most existing NLP tasks. However, the process of fine-tuning them on often very small target task datasets remains somewhat mysterious. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples?\n\nWe propose intrinsic dimensionality as a new lens through which fine-tuning can be analyzed (Li et al., 2018). An objective function’s intrinsic dimensionality describes the minimum dimension needed to solve the optimization problem it defines to some precision level. In the context of pre-trained language models, measuring intrinsic dimensionality will tell us how many free parameters are required to closely approximate the optimization problem that is solved while fine-tuning for each end task. For example, we will show that 200 parameters (randomly projected back into the full parameter space) are enough to represent the problem of tuning a RoBERTa model to within 90% of the performance of the full model. More generally, we also describe a set of strong empirical and theoretical connections between intrinsic dimensionality, the number of parameters, pre-training, and generalization.\n\nWe first empirically show that standard pre-trained models can learn a large set of NLP tasks with very few parameters and that the process of pre-training itself implicitly minimizes the intrinsic dimension of later tuning for different NLP tasks. We study over a dozen different pre-trained models to show that the number of parameters strongly inversely correlates with intrinsic dimensionality, at least in part justifying the extreme effectiveness of such models. We interpret pre-training as providing a framework that learns how to compress the average NLP task. Finally, we connect intrinsic dimensionality with low-dimensional task representations and compression-based generalization bounds to provide intrinsic-dimension-based generalization bounds independent of the full parameter count, further justifying why these methods generalize so well in practice across tasks.\n\nThe contributions of our paper are the following:\n\nWe empirically show that common NLP tasks within the context of pre-trained representations have an intrinsic dimension several orders of magnitudes less than the full parameterization.\nWe propose a new interpretation of intrinsic dimension as the downstream fine-tuning task’s minimal description length within the framework of the pre-trained model. Within this interpretation, we empirically show that the process of pre-training implicitly optimizes the description length over the average of NLP tasks, without having direct access to those same tasks.\nWe measure the intrinsic dimension of a large set of recently developed pre-training methods, and how larger models tend to have smaller intrinsic dimension.\nLastly, we show that compression-based generalization bounds can be applied to our intrinsic dimension framework to provide generalization bounds for large pre-trained models independent of the pre-trained model parameter count.",
       "charge": false
     },
     {
       "title": "Exploring Universal Intrinsic Task Subspace via Prompt Tuning",
       "author":["Yujia Qin", "Xiaozhi Wang", "Yusheng Su", "Yankai Lin", "Ning Ding", "Jing Yi", "Weize Chen", "Zhiyuan Liu", "Juanzi Li", "Lei Hou", "Peng Li", "Maosong Sun", "Jie Zhou"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "LoRA: Low-Rank Adaptation of Large Language Models",
       "author":["Edward J. Hu", "Yelong Shen", "Phillip Wallis", "Zeyuan Allen-Zhu", "Yuanzhi Li", "Shean Wang", "Lu Wang", "Weizhu Chen"] ,
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
       "author":["Rabeeh Karimi Mahabadi", "James Henderson", "Sebastian Ruder"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "DRONE: Data-aware Low-rank Compression for Large NLP Models",
       "author":["Patrick Chen", "Hsiang-Fu Yu", "Inderjit Dhillon", "Cho-Jui Hsieh"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Improving Neural Network Training in Low Dimensional Random Bases",
       "author":["Frithjof Gressmann", "Zach Eaton-Rosen", "Carlo Luschi"] ,
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Gradient Descent Happens in a Tiny Subspace",
       "author":["Guy Gur-Ari", "Daniel A. Roberts", "Ethan Dyer"] ,
       "year": 2018,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Low Dimensional Trajectory Hypothesis is True: DNNs Can Be Trained in Tiny Subspaces",
       "author":["Tao Li", "Lei Tan", "Zhehao Huang", "Qinghua Tao", "Yipeng Liu", "Xiaolin Huang"] ,
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": true
     },
     {
       "title": "Subspace Adversarial Training",
       "author":["Tao Li", "Yingwen Wu", "Sizhe Chen", "Kun Fang", "Xiaolin Huang"] ,
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false,
       "remarks": ""
     },
     {
       "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality",
       "author":["William Timkey", "Marten van Schijndel"] ,
       "year": 2021,
       "abstract": "Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in semantic space. Recently, these measures have been applied to embeddings from contextualized models such as BERT and GPT-2. In this work, we call into question the informativity of such measures for contextualized language models. We find that a small number of rogue dimensions, often just 1-3, dominate these measures. Moreover, we find a striking mismatch between the dimensions that dominate similarity measures and those which are important to the behavior of the model. We show that simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality. We argue that accounting for rogue dimensions is essential for any similarity-based analysis of contextual language models.",
       "introduction": "By mapping words into continuous vector spaces, we can reason about human language in geometric terms. For example, the cosine similarity of pairs of word embeddings in Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) shows a robust correlation with human similarity judgments, and embeddings cluster into natural semantic classes in Euclidean space (Baroni et al., 2014; Wang et al., 2019). In recent years, static embeddings have given way to their contextual counterparts, with language models based on the transformer architecture (Vaswani et al., 2017) such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2020), XLNet (Yang et al., 2019) and GPT-2 (Radford et al., 2019) achieving state-of-the-art results on many language understanding tasks.\n\nDespite their success, relatively little is known about how these models represent and process language. Recent work has employed measures such as cosine similarity and Euclidean distance to contextual representations with unclear and counterintuitive results. For example, similarity/distance measures in BERT are extremely sensitive to word position, leading to inconsistent results on evaluation benchmarks (Mickus et al., 2020; May et al., 2019). Additionally, representational quality appears to degrade severely in later layers of each network, with the final layers of BERT, RoBERTa, GPT-2 and XLNet showing little to no correlation with the semantic similarity/relatedness judgments of humans (Bommasani et al., 2020).\n\nRecent work that probes the representational geometry of contextualized embedding spaces using cosine similarity has found that contextual embeddings have several counterintuitive properties (Ethayarajh, 2019). For example:\n\nWord representations are highly anisotropic: randomly sampled words tend to be highly similar to one another when measured by cosine similarity. In the final layer of GPT-2, for example, any two words are almost perfectly similar.\n\nEmbeddings have extremely low self-similarity: In later layers of transformer-based language models, random words are almost as similar to one another as instances of the same word in different contexts.\n\nIn this work, we critically examine the informativity of standard similarity/distance measures (particularly cosine similarity and Euclidean distance) in contextual embedding spaces. We find that these measures are often dominated by 1-5 dimensions across all the contextual language models we tested, regardless of the specific pretraining objective. It is this small subset of dimensions that drive anisotropy, low self-similarity, and the apparent drop in representational quality in later layers. These dimensions, which we refer to as rogue dimensions, are centered far from the origin and have disproportionately high variance. The presence of rogue dimensions can cause cosine similarity and Euclidean distance to rely on less than 1% of the embedding space. Moreover, we find that the rogue dimensions which dominate cosine similarity do not likewise dominate model behavior and show a strong correlation with absolute position and punctuation.\n\nFinally, we show that these dimensions can be accounted for using a trivially simple transformation of the embedding space: standardization. Once applied, cosine similarity more closely reflects human word similarity judgments, and we see that representational quality is preserved across all layers rather than degrading/becoming task-specific. Taken together, we argue that accounting for rogue dimensions is essential when evaluating representational similarity in transformer language models.",
       "charge": false
     },
     {
       "title": "On Isotropy Calibration of Transformer Models",
       "author":["Yue Ding", "Karolis Martinkus", "Damian Pascual", "Simon Clematide", "Roger Wattenhofer"] ,
       "year": 2022,
       "abstract": "Different studies of the embedding space of transformer models suggest that the distribution of contextual representations is highly anisotropic - the embeddings are distributed in a narrow cone. Meanwhile, static word representations (e.g., Word2Vec or GloVe) have been shown to benefit from isotropic spaces. Therefore, previous work has developed methods to calibrate the embedding space of transformers in order to ensure isotropy. However, a recent study (Cai et al. 2021) shows that the embedding space of transformers is locally isotropic, which suggests that these models are already capable of exploiting the expressive capacity of their embedding space. In this work, we conduct an empirical evaluation of state-of-the-art methods for isotropy calibration on transformers and find that they do not provide consistent improvements across models and tasks. These results support the thesis that, given the local isotropy, transformers do not benefit from additional isotropy calibration.",
       "introduction": "The impressive performance of transformer models (Vaswani et al., 2017) across almost all areas of Natural Language Processing (NLP) has sparked in-depth investigations of these models. A remarkable finding is that the contextual representations computed by transformers are strongly anisotropic (Ethayarajh, 2019), i.e., they are unevenly distributed and localized in a narrow cone of the embedding space. This discovery, labeled as the representation degeneration problem by Gao et al. (2019), is surprising since it suggests that most of the expressive capacity of these high-dimensional spaces is neglected by transformers.\n\nFurthermore, previous work on static word representations, e.g., GloVE (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013), established that isotropy is a desirable property in non-contextual embedding spaces (Mu and Viswanath, 2018). Indeed, Mu and Viswanath (2018) and Liu et al. (2019a) showed that post-processing static word embeddings to increase isotropy improves their performance in downstream tasks. Based on these results, recent work has developed methods to correct the anisotropy of the contextual representations generated by transformers (Gao et al., 2019; Wang et al., 2019b; Li et al., 2020). These isotropy calibration methods have been reported to produce small gains in performance on some NLP tasks.\n\nHowever, in a recent study, Cai et al. (2021) show that the space of contextual embeddings of transformers is locally isotropic. By analyzing low-dimensional sub-spaces, the authors identify isolated clusters and manifolds and argue that isotropy does exist in these manifolds. In the same line, Luo et al. (2021) and Kovaleva et al. (2021) find that in BERT (Devlin et al., 2019) almost all of the embeddings present large values in the same two components of the embedding vector. These large components distort our understanding of the embedding spaces by making all the representations have high cosine similarity. In this work, we perform an extensive empirical evaluation of isotropy calibration methods across different tasks and models to determine if they provide consistent improvements. Our results question the utility of isotropy calibration in transformers, implicitly supporting the argument that transformers do already benefit from local isotropy (Cai et al., 2021).",
       "charge": false
     },
     {
       "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings",
       "author":["Ziyang Luo", "Artur Kulmizev", "Xiaoxi Mao"] ,
       "year": 2021,
       "abstract": "In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa’s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings. We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings. These outliers, we find, are the major cause of anisotropy of encoders’ raw vector spaces, and clipping them leads to increased similarity across vectors. We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling. In three supervised tasks, we find that clipping does not affect the performance.",
       "introduction": "A major area of NLP research in the deep learning era has concerned the representation of words in low-dimensional, continuous vector spaces. Traditional methods for achieving this have included word embedding models such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). However, though influential, such approaches all share a uniform pitfall in assigning a single, static vector to a word type. Given that the vast majority of words are polysemous (Klein and Murphy, 2001), static word embeddings cannot possibly represent a word’s changing meaning in context.\n\nIn recent years, deep language models, like ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019b), have achieved great success across many NLP tasks. Such models introduce a new type of word vectors, deemed the contextualized variety, where the representation is computed with respect to the context of the target word. Since these vectors are sensitive to context, they can better address the polysemy problem that hinders traditional word embeddings. Indeed, studies have shown that replacing static embeddings (e.g. word2vec) with contextualized ones (e.g. BERT) can benefit many NLP tasks, including constituency parsing (Kitaev and Klein, 2018), coreference resolution (Joshi et al., 2019) and machine translation (Liu et al., 2020).\n\nHowever, despite the major success in deploying these representations across linguistic tasks, there remains little understanding about information embedded in contextualized vectors and the mechanisms that generate them. Indeed, an entire research area central to this core issue — the interpretability of neural NLP models — has recently emerged (Linzen et al., 2018, 2019; Alishahi et al., 2020). A key theme in this line of work has been the use of linear probes in investigating the linguistic properties of contextualized vectors (Tenney et al., 2019; Hewitt and Manning, 2019). Such studies, among many others, show that contextualization is an important factor that sets these embeddings apart from static ones, the latter of which are unreliable in extracting features central to context or linguistic hierarchy.\n\nNonetheless, much of this work likewise fails to engage with the raw vector spaces of language models, preferring instead to focus its analysis on the transformed vectors. Indeed, the fraction of work that has done the former has shed some curious insights: that untransformed BERT sentence representations still lag behind word embeddings across a variety of semantic benchmarks (Reimers and Gurevych, 2019) and that the vector spaces of language models are explicitly anisotropic (Ethayarajh, 2019; Li et al., 2020a). Certainly, an awareness of the patterns inherent to models’ untransformed vector spaces — even if shallow — can only benefit the transformation-based analyses outlined above.\n\nIn this work, we shed light on a persistent pattern that can be observed for contextualized vectors produced by BERT and RoBERTa. Namely, we show that, across all layers, select neurons in BERT and RoBERTa consistently bear extremely large values. We observe this pattern across vectors for all words in several datasets, demonstrating that these singleton dimensions serve as major outliers to the distributions of neuron values in both encoders’ representational spaces. With this insight in mind, the contributions of our work are as follows:\n\nWe introduce a neuron-level method for analyzing the origin of a model’s outliers. Using this, we show that they are closely related to positional information.\nIn investigating the effects of clipping the outliers (zeroing-out), we show that the degree of anisotropy in the vector space diminishes significantly.\nWe show that after clipping the outliers, the BERT representations can better distinguish between a word’s potential senses in the word-in-context (WiC) dataset (Pilehvar and Camacho-Collados, 2019), as well as lead to better sentence embeddings when mean pooling.",
       "charge": false
     },
     {
       "title": "Whitening Sentence Representations for Better Semantics and Faster Retrieval",
       "author":["Jianlin Su", "Jiarun Cao", "Weijie Liu", "Yangyiwen Ou"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Revisiting Representation Degeneration Problem in Language Modeling",
       "author":["Zhong Zhang, Chongming Gao, Cong Xu, Rui Miao, Qinli Yang, Junming Shao"] ,
       "year": 2020,
       "abstract": "Weight tying is now a common setting in many language generation tasks such as language modeling and machine translation. However, a recent study reveals that there is a potential flaw in weight tying. They find that the learned word embeddings are likely to degenerate and lie in a narrow cone when training a language model. They call it the representation degeneration problem and propose a cosine regularization to solve it. Nevertheless, we prove that the cosine regularization is insufficient to solve the problem, as the degeneration is still likely to happen under certain conditions. In this paper, we revisit the representation degeneration problem and theoretically analyze the limitations of the previously proposed solution. Afterward, we propose an alternative regularization method called Laplacian regularization to tackle the problem. Experiments on language modeling demonstrate the effectiveness of the proposed Laplacian regularization.",
       "introduction": "Language modeling is a fundamental task in natural language processing, applications include machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), image captioning (Vinyals et al., 2015; Xu et al., 2015) and speech recognition (Yu and Deng, 2016), to name a few. In the era of deep learning, a general model architecture usually contains a word embedding layer as input, multiple layers to encode word context as a fixed-size hidden state, and a softmax layer to transform the hidden-state into a categorical distribution of the next word (Merity et al., 2018; Yang et al., 2018; Gong et al., 2018; Wang et al., 2019; Gao et al., 2019). While in practice, the parameters of the embedding layer and the softmax layer are usually shared, which is called weight tying (Inan et al., 2017; Press and Wolf, 2017). Despite the improvements from weight tying, a recent work (Gao et al., 2019) discovers that, with weight tying, the learned word embeddings are positively correlated and spread in a narrow cone as visualized in Figure 1(a). A similar phenomenon is observed in Gong et al. (2018). Thus, the semantic expressiveness of word embeddings is limited. They call it the representation degeneration problem. To tackle the problem, the authors propose a cosine regularization that minimizes the cosine similarities between any two word embeddings to enlarge the aperture of the cone. They show that it improves the language modeling performance and eases the degeneration as visualized in Figure 1(b). However, we argue that the cosine regularization might not be the best choice for solving this problem, and the reasons are: i) The cosine regularization minimizes similarities between any two word embeddings without considering whether they are semantically close or not. But we wish two words with similar semantics stay close in the embedding space. ii) Although the cosine regularization improves language generation performance, it does not fundamentally solve the representation degeneration problem. We prove that the degeneration still exists when there exists a certain regularization structure. Finally, we analyze the general condition of degeneration and show that there still are many low-frequency words that meet the condition and thus degenerate. Therefore, we argue that the degeneration is still likely to happen even with cosine regularization. Motivated by these issues, we propose an alternative Laplacian regularization to tackle the representation degeneration problem. As the distributional hypothesis (Harris, 1954) states: two words that occur in similar contexts tend to have similar meanings. The general idea of Laplacian regularization is to minimize the squared Euclidean distance between two word embeddings when they have large context similarity. In contrast to cosine regularization, Laplacian regularization prevents minimizing all similarities of word pairs indiscriminately. Although the Laplacian regularization does not theoretically solve the degeneration problem either, we empirically demonstrate that it achieves better performance in most cases of language modeling experiments, and word embeddings are less likely to degenerate. In summary, the main contributions of our work are listed as follows. • We revisit the representation degeneration problem and theoretically analyze the limitations of the previously proposed cosine regularization solution. • We propose an alternative Laplacian regularization to tackle the representation degeneration problem. We show that it eases the degeneration to an extent comparing with cosine regularization. • We conduct experiments on language modeling task to demonstrate the effectiveness of our method.",
       "charge": false
     },
     {
       "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers",
       "author":["Olga Kovaleva", "Saurabh Kulshreshtha", "Anna Rogers", "Anna Rumshisky"] ,
       "year": 2021,
       "abstract": "Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet, and ELECTRA; we also show a similar effect in GPT-2.",
       "introduction": "Pre-trained Transformer-based models (Vaswani et al., 2017) have become widely popular in a variety of NLP applications. Multiple studies of BERT-family models (Devlin et al., 2019) showed that Transformers are remarkably robust to pruning (Gordon et al., 2020; Prasanna et al., 2020; Chen et al., 2020; Michel et al., 2019). This work presents a different and unexpected result: it is possible to dramatically disrupt the performance of BERT and other Transformer-based architectures by modifying very few weights (less than 0.0001% for BERT).\n\nIn particular, we show that there is a very small number of outlier dimensions that regularly appear in the same position in the pre-trained encoder layers of a given Transformer model. We demonstrate that this effect holds for different Transformer-family architectures, including multiple variants of BERT, as well as ELECTRA (Clark et al., 2020), BART (Lewis et al., 2020), and XLNet (Yang et al., 2019). A similar phenomenon is also present in the decoder layers of GPT-2 (Radford et al.). When these dimensions are disabled throughout the model in the concluding transformation of each layer, they can drastically reduce the overall model performance. With the exception of GPT-2, the last transformation in each layer of these models is normalization (LayerNorm), which is what we mainly focus on in this study.\n\nThe contributions of this work are as follows:\n• We identify certain outlier dimensions in Transformer layer outputs and show that they play a crucial role in both language modeling and downstream task performance. Disabling the weights for these output dimensions drastically degrades performance (up to 44 points).\n• We show that this effect holds for the encoder layers of six different models of the BERT family, as well as other popular pre-trained Transformer-based models including ELECTRA, BART, and XLNet. In GPT-2, a similar phenomenon is observed in the output dense transformation of the decoder layers.\n• We demonstrate that outlier weights emerge gradually and begin to emerge early in pretraining, causing abnormal spikes at select dimensions in the output embedding vectors.\n\n To our knowledge, this is the first work to establish the presence of very few regular outliers in the output Transformer representations and their importance for the model performance. It is not clear why these features emerge, but the final transformations clearly play a larger role in the Transformer layers than is usually assumed, and this needs further investigation. This paper is organized as follows. After a brief overview of related work (§2), we introduce the methodology for defining, locating, and disabling the BERT outlier weights in §3. In §4.1 and §4.2, we quantify the effect of disabling these weights both in pre-training and in downstream tasks. In §4.3, we demonstrate that other Transformers (BART, ELECTRA, XLNet, and GPT2) also exhibit similar behavior. §5.1 evaluates magnitude- and position-based criteria for identifying the outlier dimensions and compares them with our proposed criteria. In §5.2, we replicate the outlier effect in a BERT model during pretraining and study its dynamics.",
       "charge": false
     },
     {
       "title": "Outlier Dimensions that Disrupt Transformers are Driven by Frequency",
       "author":["Giovanni Puccetti", "Anna Rogers", "Aleksandr Drozd", "Felice Dell’Orletta"] ,
       "year": 2022,
       "abstract": "While Transformer-based language models are generally very robust to pruning, there is the recently discovered outlier phenomenon: disabling only 48 out of 110M parameters in BERT-base drops its performance by nearly 30% on MNLI. We replicate the original evidence for the outlier phenomenon and we link it to the geometry of the embedding space. We find that in both BERT and RoBERTa the magnitude of hidden state coefficients corresponding to outlier dimensions correlate with the frequencies of encoded tokens in pre-training data, and they also contribute to the “vertical” self-attention pattern enabling the model to focus on the special tokens. This explains the drop in performance from disabling the outliers, and it suggests that to decrease anisotopicity in future models we need pre-training schemas that would better take into account the skewed token distributions.",
       "introduction": "The current Transformer-based language models are heavily overparametrized, which explains why it is possible to prune these models by up to 30-40% (Gordon et al., 2020; Sanh et al., 2020; Prasanna et al., 2020; Chen et al., 2020, inter alia) without a significant drop in performance. However, it has recently been shown that multiple Transformer-based language models (LMs) are highly sensitive to the removal of outlier dimensions (Kovaleva et al., 2021): the parameters (weights and biases) in the output element of a Transformer layer, the magnitude of which is unusually large within the layer (consistently in the same dimension across the model layers). For the BERT model family, the output element is the LayerNorm, as shown in Figure 1.\n\nAlthough these parameters constitute less than 0.0001% of the full BERT (Devlin et al., 2019) model, removing them significantly degrades BERT’s performance. Puccetti et al. (2021) find that the same parameters are particularly relevant in several linguistic probing tasks. These dimensions affect the vector representation of different tokens in the same way, making the embedding space less isotropic and thus reducing its representational power (Liang et al., 2021). Outlier dimensions have also been found to make model quantization challenging (Bondarenko et al., 2021; Dettmers et al., 2022) as they need to be treated separately from others when defining quantization schemes. Thus, there are both conceptual and practical reasons supporting a deeper study of this phenomenon.\n\nWhat is not clear at this point is the mechanism behind the emergence of outliers. We replicate the original findings in BERT and RoBERTa, and we contribute new evidence directly linking the outlier phenomenon with the frequency of encoded tokens in the pre-training data, as well as the self-attention pattern focusing on special tokens. We also present evidence for two kinds of outliers: some of them affect the Masked Language Model (MLM) performance the most in the middle layers (where the correlation with token frequency is at its peak), and for others, the impact grows towards the final layers (although the correlation with token frequency decreases). This work contributes to a mechanistic understanding of Transformer-based LMs, and it might be useful for future research on decreasing anisotropy in pre-trained LMs.",
       "charge": false
     }
  ]
}
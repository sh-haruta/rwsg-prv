{"root":
  {
    "title": "Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection",
    "author": ["Ziwei Chen", "Linmei Hu", "Weixin Li", "Yingxia Shao", "Liqiang Nie"],
    "abstract": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.",
    "introduction": "Fake news quietly sneaks into people’s daily life, mixed with massive information, causing serious impact and harm to society. Fake news often utilizes multimedia information such as text and images to mislead readers, spreading and expanding its influence. Thus, it is crucial and urgent to find a way to discern multi-modal fake news.\n\nToday, most existing methods train on known fake news instances expecting to capture the label-specific features for judging the authenticity of unseen news (Singhal et al., 2020; Wu et al., 2021; Qian et al., 2021b; Qi et al., 2021). However, such label-specific features may expose the models to hidden data bias when confronted with unseen fake news samples (Wang et al., 2018; Cheng et al., 2021; Zhu et al., 2022). To address the problem, we investigate the biases underlying the multi-modal fake news detection data and identify the psycholinguistic bias in the text and the bias of inferring news label based on image features only (i.e. image-only bias). These biases could lead to spurious correlations between the news and labels, thus impairing the model performance on testing data.\n\nTo explicitly explain the biases, we first formulate the process of fake news detection as a causal graph as shown in Figure 2(a). In addition to the impact of fused features C on news label Y that most multi-modal fake news detection methods focus on, two other edges are pointing to Y, starting from text features T, and image features I, respectively. Generally speaking, the publishers of fake news would try their best to fabricate confusing text or use certain techniques to forge fake images. This makes the text and image can individually affect the news label.\n\nFor the T → Y branch, we observe that the linguistic characteristics of the text have obvious emotional preferences, such as the usage of psycholinguistic words \"crazy\" and \"amazing,\" which play a critical role in fake news detection. To deeply analyze the linguistic characteristics of the text, we present a mathematical analysis of the psycholinguistic word distribution of real news and fake news based on the LIWC 2015 dictionary (Pennebaker et al., 2015). Take the Twitter dataset as an example, as shown in Figure 1, we can observe that the word frequency distribution of fake news is quite different from that of real news, especially for words expressing anxiety, negative emotions, positive emotions, tentative, and netspeak. It seems that we can draw a conclusion that fake news prefers to use loaded language to stir up the reader’s emotions and attract more attention. Consequently, the model could be prone to relying on such psycholinguistic features as a shortcut to judge news authenticity. However, we analyze the training set and testing set and find that there exist significant differences in the frequency of these psycholinguistic words. The manifest differences between the training set and testing set have proven that this shortcut appears to be unreliable evidence. As shown in Figure 2(b) where U denotes the confounder (i.e. the psycholinguistic features in the text), there exist a backdoor path T ← U → Y which will introduce spurious correlations among the text features and news label. In order to remove the psycholinguistic bias, we apply causal intervention by adopting the backdoor adjustment (Glymour et al., 2016) with do-calculus P(Y |do(T)) to calculate the causal effect in the training stage, which is fundamentally different from the conventional likelihood P(Y |T).\n\nFor the I → Y branch, we observe from the datasets that two different news pieces sharing the same image could have contrary labels. This shows that sometimes even if the image is real, the text could be fabricated, and the news could thus be fake. We can take advantage of images as an additional modality to provide more detection evidence, but it is unreliable to infer the authenticity of the news based on the image features alone. In this case, we argue that the image-only bias (i.e., the direct causal effect from image features alone to news label) should be eliminated. Towards this end, we use counterfactual reasoning by imagining a counterfactual world (Figure 2(c)) where both text features T and fused features C are not given (represented by reference values t* and c*), except for image features I. In this way, the bias can be estimated by computing the direct causal effect of I on Y and we can conduct the debiasing by subtracting it from the total effect on Y.\n\nWe instantiate our proposed debiasing framework on three strong baseline models that can handle both text and image features as inputs. Extensive experiments on two widely used real-world benchmark datasets show the effectiveness of our framework. Overall, our contributions can be summarized as follows:\n\nWe analyze each modality of fake news detection data and identify the underlying psycholinguistic bias in the text and the image-only bias. And we propose a novel Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection.\nIn our debiasing framework CCD, we conduct causal interventions via backdoor adjustment to remove spurious correlations introduced by the psycholinguistic confounder. For addressing the image-only bias, we apply counterfactual reasoning to pursue the indirect causal effect as the inference prediction.\nOur causal framework CCD can be applied to any fake news detection model with image and text features as inputs. We implement the proposed framework on three strong baseline models and conduct extensive experiments on two widely used benchmark datasets, validating the effectiveness of CCD.",
    "relatedwork": "In this section, we review the related work, including fake news detection and causal inference.\n\n5.1 Multi-modal Fake News Detection\nExisting fake news detection work generally falls into two categories: content-based methods and propagation-based methods. The multi-modal approaches fall into the former category.\n\nMost works on multi-modal fake news detection make efforts to fully incorporate cross-modal features. For instance, Jin et al. (2017) proposed a recurrent neural network with an attention mechanism to fuse text, social context, and image features. Singhal et al. (2020) utilized pre-trained encoders and applied multiple-layer feature transformation to achieve deep fusion. Chen et al. (2022b) calculated the ambiguity score of different modalities to control the contribution of mono-modal features and inter-modal correlations to the final prediction. To capture fine-grained cross-modal correlations, Wu et al. (2021) employed multiple rounds of co-attention mechanisms to model cross-modal interactions. Qian et al. (2021b) leveraged a contextual attention network to model both intra- and inter-modality information and captured the hierarchical semantic information of the text. Some methods also leverage external knowledge to provide powerful evidence or enrich feature representations (Hu et al., 2021; Qi et al., 2021). For example, Hu et al. (2021) compared each news with the external knowledge base through entities to utilize consistencies for detection.\n\nIn this work, we improve fake news detection from the perspective of causality and propose a novel framework that eliminates hidden biases in each modality.\n\n5.2 Causal Inference\nCausal inference (Glymour et al., 2016), including causal intervention and counterfactual reasoning, has been widely used in various fields such as recommendation (Zhang et al., 2021b; Wang et al., 2021), natural language inference (Tian et al., 2022), text classification (Qian et al., 2021a), named entity recognition (Zhang et al., 2021a), pre-trained language models (Li et al., 2022), etc. It provides a powerful tool that can scientifically identify the causal correlations between variables and remove hidden biases in the data. Concerning fake news detection, Zhu et al. (2022) eliminated entity bias (the distribution of entities in the text) by counterfactual reasoning.\n\nIn this work, we discover psycholinguistic bias and image-only bias in fake news detection and propose a novel debiasing framework that eliminates these biases using causal intervention and counterfactual reasoning to enhance detection performance."},
  "leaves":
   [
     {
       "title": "Multimodal Fusion with Recurrent Neural Networks for Rumor Detection on Microblogs",
       "author": ["Zhiwei Jin", "Juan Cao", "Han Guo", "Yongdong Zhang", "Jiebo Luo"],
       "year": 2017,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "SpotFake+: A Multimodal Framework for Fake News Detection via Transfer Learning",
       "author":["Shivangi Singhal", "Anubha Kabra", "Mohit Sharma", "Rajiv Ratn Shah", "Tanmoy Chakraborty", "Ponnurangam Kumaraguru"] ,
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Cross-modal Contrastive Learning for Multimodal Fake News Detection",
       "author":["Longzheng Wang", "Chuang Zhang", "Hongbo Xu", "Yongxiu Xu", "Xiaohan Xu", "Siqi Wang"],
       "year": 2022,
       "abstract": "Automatic detection of multimodal fake news has gained a widespread attention recently. Many existing approaches seek to fuse unimodal features to produce multimodal news representations. However, the potential of powerful cross-modal contrastive learning methods for fake news detection has not been well exploited. Besides, how to aggregate features from different modalities to boost the performance of the decision-making process is still an open question. To address that, we propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment. To further capture the fine-grained alignment between vision and language, we leverage an auxiliary task to soften the loss term of negative samples during the contrast process. A cross-modal fusion module is developed to learn the cross-modality correlations. An attention mechanism with an attention guidance module is implemented to help effectively and interpretably aggregate the aligned unimodal representations and the cross-modality correlations. Finally, we evaluate the COOLANT and conduct a comparative study on two widely used datasets, Twitter and Weibo. The experimental results demonstrate that our COOLANT outperforms previous approaches by a large margin and achieves new state-of-the-art results on the two datasets.",
       "introduction": "With the proliferation of Online Social Networks (OSNs) such as Twitter and Weibo, individuals can freely share daily information and express their opinions and emotions. However, the misuse of OSNs and the lack of proper supervision to verify the credibility of online posts have given rise to the widespread dissemination of considerable fake news [41]. Therefore, fake news detection has gained widespread attention and has become a top priority recently.\n\nExisting studies on automatic fake news detection mainly focus on textual content, either with traditional learning methods such as decision tree classifiers [23] or deep learning approaches such as convolutional neural networks (CNN) [36]. However, most posts on social media commonly contain rich multimodal information, and the detection based on unimodal features is far from sufficient. Figure 1 shows some examples from Twitter illustrating the reasons why these four news items were determined to be fake. Recent works seek to fuse textual and visual features to produce multimodal post representations and then boost the performance of fake news detection [19, 33]. Nevertheless, we argue that more advanced multimodal representation learning paradigms should be appropriately applied, since acquiring more sophisticated aligned unimodal representations and cross-modal features is a prerequisite for effective multimodal fake news detection. Besides, cross-modal features might not necessarily play a critical role in some cases [6, 29]. For instance, the textual contents in Figure 1(a) are preposterous enough to indicate that it is fake. In contrast, the cross-modal information gap in Figure 1(d) can help improve classification accuracy. Therefore, how features from different modalities affect the decision-making process and how we can make it more effective and interpretable remain open questions.\n\nRecently, several contrastive learning-based multimodal pretraining methods have achieved great success, suggesting that contrastive learning may be a powerful paradigm for multimodal representation learning [1, 12, 16, 21, 22, 28, 37]. A contrastive loss aims to align the image features and the text features by pushing the embeddings of positive image-text pair together while pushing those of negative image-text pair apart. It has been shown to be an effective objective for improving the unimodal encoders to better understand the semantic meaning of images and texts. While effective, the one-hot labels in contrastive learning penalize all negative predictions regardless of their correctness [10, 22]. Therefore, this contrastive framework for multimodal fake news detection suffers from several key limitations: (1) A huge number of image-text pairs in fake news are inherently not matched (e.g. Figure 1d), and the contrastive objective may overfit to those data and degrade the model’s generalization performance; (2) Different image-text pairs may have potential correlation (especially in the case of different multimodal news about the same event), existing contrastive objectives directly treat those pairs as negative, which may confuse the model. Therefore, although these advanced technologies can be beneficial in multimodal representation learning, their application in multimodal fake news detection remains to be explored.\n\nTaking the consideration above, we propose COOLANT, a Crossmodal Contrastive Learning framework for Multimodal Fake News Detection. We utilize a simple dual-encoder framework to construct a visual semantics level and a linguistic semantics level. Then we use the image-text contrastive (ITC) learning objective to ensure the alignment between image and text modalities. As mentioned above, the contrastive learning framework utilized for detecting multimodal fake news is subject to certain constraints, primarily stemming from the one-hot labeling method. To alleviate this problem and further improve the alignment precision, we leverage an auxiliary task, called cross-modal consistency learning, to introduce more supervisions and bring in more fine-grained semantic information. Specifically, the contrastive learning objective ensures that the image-text pairs are in perfect one-to-one correspondence, and the consistency learning task can derive the potential semantic similarity features to soften the loss of negative samples (unpaired samples). After that, we feed the aligned unimodal representations into a cross-modal fusion module to learn the cross-modality correlations. Finally, we design an attention mechanism module to help effectively aggregate the aligned unimodal representations and the cross-modality correlations. Inspired by [6], we introduce an attention guidance module to quantify the ambiguity between text and image by estimating the divergence of their representation distributions, which can help guide the attention mechanism to assign reasonable weights to modalities. In this way, COOLANT can acquire more sophisticated aligned unimodal representations and cross-modal features, and then effectively aggregate these features to boost the performance of multimodal fake news detection.\n\nThe main contributions of this paper are as follows:\n\n• We propose COOLANT, a cross-modal contrastive learning framework for multimodal fake news detection, aiming to achieve more accurate image-text alignment.\n\n• We soften the loss term of negative samples during the contrast process to ease the strict constraint, so as to make it more compatible with our task.\n\n• We introduce an attention mechanism with an attention guidance module to help effectively and interpretably aggregate features from different modalities.\n\n• We conduct experiments on two widely used datasets, Twitter and Weibo. Experimental results demonstrate that our model outperforms previous systems by a large margin and achieves new state-of-the-art results on the two datasets.",
       "charge": false
     },
     {
       "title": "Multimodal Fusion with Co-Attention Networks for Fake News Detection",
       "author":["Yang Wu", "Pengwei Zhan", "Yunjian Zhang", "Liming Wang", "Zhen Xu"] ,
       "year": 2021,
       "abstract": "Fake news with textual and visual contents has a better story-telling ability than text-only contents, and can be spread quickly with social media. People can be easily deceived by such fake news, and traditional expert identification is labor-intensive. Therefore, automatic detection of multimodal fake news has become a new hot-spot issue. A shortcoming of existing approaches is their inability to fuse multimodality features effectively. They simply concatenate unimodal features without considering inter-modality relations. Inspired by the way people read news with image and text, we propose a novel Multimodal Co-Attention Networks (MCAN) to better fuse textual and visual features for fake news detection. Extensive experiments conducted on two real-world datasets demonstrate that MCAN can learn inter-dependencies among multimodal features and outperforms state-of-the-art methods.",
       "introduction": "The rapid growth of social media has created fertile soil for the emergence and fast spread of fake news (Zhao et al., 2015), resulting in serious consequences. For example, during the U.S. 2016 presidential election, the most popular fake news was more widely spread than the most popular authentic news on Facebook, which confused people and broke the authenticity balance of the news ecosystem (Shu et al., 2017). To mitigate the negative effects caused by fake news, it is crucial to detect fake news on social media automatically.\n\nTweets with images are getting popular on social media recently, which have richer information and attract more viewers than tweets with only texts (Jin et al., 2017). Fake news also makes full use of this advantage to draw and mislead readers. Figure 1 shows three examples of fake news from Twitter. In the left example, both text and image indicate it is likely to be fake. The text of the middle one provides little evidence that it is fake news, but the image is obviously forged. In the right example, the image seems normal, while the textual contents indicate that it is probably fake. A hypothesis drawn from these examples is that combining text and the attached image is more conducive to detecting fake news.\n\nRecent works have a growing interest in using multimodal (text + image) information to detect fake news. Jin et al. (2017) utilize local attention mechanisms to fuse features of image, text, and social context. Some studies explore to learn the joint representations of text and image, based on auxiliary adversarial networks (Wang et al., 2018) and variational autoencoders (Khattar et al., 2019). Nevertheless, they are not fine-grained enough in feature extraction and feature fusion. First, some studies require labor-intensive extra information, such as social context (Jin et al., 2017) and event category (Wang et al., 2018), to help detect fake news, which increases the cost of the detection. Second, except for texts in tweets, the methods mentioned above all focus on characteristics of images at the semantic level (e.g., emotional provocations), which can be reflected in the spatial domain. However, these methods ignore the individual information of fake images at the physical level, e.g., re-compression artifacts, which is reflected in the frequency domain (Qi et al., 2019). Third, some models (Wang et al., 2018; Khattar et al., 2019) obtain fused representations by simply concatenating multi-modality features. Although leverages local attention mechanism, the attention values of attRNN (Jin et al., 2017) are only obtained from joint textual-social representations, which cannot reflect the similarity between textual-social representations and visual representations. Intuitively, when people judge news credibility with text and image, they often observe image first and then read text (Wang et al., 2020). This process may be repeated several times. In this process, people understand image according to the textual information, and understand text according to the associated image information. So the information of one modality is conditionally fused with that of another modality for once or multiple times. Intuitively, there are inter-modality attention relations between image and text. However, existing state-of-the-art methods are weak to fuse multimodal features due to their neglect of inter-modality interactions.\n\nTo address the aforementioned challenges, we propose the Multimodal Co-Attention Networks (MCAN) for fake news detection by considering multimodal features. In our proposed model, we first extract spatial-domain features and frequency-domain features from image, as well as textual features from text. Then we develop a novel fusion approach with multiple co-attention layers to learn inter-modality relations, which fuses visual features first, and then the textual features. The fused representation obtained from the last co-attention layer is used for fake news detection.\n\nThe contributions of this paper can be summarized as follows: (1) We propose a novel end-to-end approach to detect fake news on social media only using the text and the attached image, without any extra information and auxiliary tasks. (2) The proposed MCAN model stacks multiple co-attention layers to fuse the multimodal features, which can learn inter-dependencies among them. (3) Our MCAN model is a general framework for fake news detection, and the components of MCAN are flexible. The sub-networks used to extract multimodal features can be replaced by different models. Moreover, the modular fusion process of MCAN allows our model to handle more modalities conveniently. (4) We evaluate MCAN on two large scale real-world datasets. The results demonstrate that our model outperforms the state-of-the-art models.\n\nThe rest of the paper is organized as follows: In Section 2, we summarize previous related work on fake news detection. In Section 3, we detail our proposed model. The datasets, baselines, and experiment results are presented in Section 4. We conclude the study in Section 5.",
       "charge": false
     },
     {
       "title": "Hierarchical Multi-modal Contextual Attention Network for Fake News Detection",
       "author":["Shengsheng Qian", "Jinguang Wang", "Jun Hu", "Quan Fang", "Changsheng Xu"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge",
       "author":["Linmei Hu", "Tianchi Yang", "Luhao Zhang", "Wanjun Zhong", "Duyu Tang", "Chuan Shi", "Nan Duan", "Ming Zhou"] ,
       "year": 2021,
       "abstract": "Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection. Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation. Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities. Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content. The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB. Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier. Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.",
       "introduction": "With the rapid development of the Internet, there are increasingly huge opportunities for fake news production, dissemination and consumption. Fake news are news documents that are intentionally and verifiably false, and could mislead readers (Allcott and Gentzkow, 2017). Fake news can easily misguide public opinion, cause the crisis of confidence, and disturb the social order (Vosoughi et al., 2018). It is well known that fake news exerted an influence in the past 2016 US presidential elections (Allcott and Gentzkow, 2017). Thus, it is very important to develop effective methods for early fake news detection based on the textual content of the news document.\n\nSome existing fake news detection methods rely heavily on various hand-crafted linguistic and semantic features for differentiating between news documents (Conroy et al., 2015; Rubin et al., 2016; Rashkin et al., 2017; Khurana and Intelligentie, 2017; Shu et al., 2020). To avoid feature engineering, deep neural models such as Bi-LSTM and convolutional neural networks (CNN) have been employed (Oshikawa et al., 2020; Wang, 2017; Rodr´ıguez and Iglesias, 2019). However, they fail to consider the sentence interactions in the document. Vaibhav et al. showed that trusted news and fake news have different patterns of sentence interactions (Vaibhav et al., 2019). They modeled a news document as a fully connected sentence graph and proposed a graph attention model for fake news detection. Although these existing approaches can be effective, they fail to fully exploit external KB which could help determine whether the news is fake or trusted.\n\nExternal KB such as Wikipedia contains a large amount of high-quality structured subject-predicate-object triplets and unstructured entity descriptions, which could serve as evidence for detecting fake news. As shown in Figure 4, the news document about \"mammograms are not effective at detecting breast tumors\" is likely to be detected as fake news with the knowledge that \"The goal of mammography is the early detection of breast cancer\" in the Wikipedia entity description page 1. Pan et al. proposed to construct knowledge graphs from positive and negative news, and apply TransE to learn triplet scores for fake news detection (Pan et al., 2018). Nevertheless, the performance is largely influenced by construction of the knowledge graph. In this paper, to take full advantage of the external knowledge, we propose a novel end-to-end graph neural model CompareNet which directly compares the news to the KB through entities for fake news detection. In CompareNet, we also consider using topics to enrich the news document representation for improving fake news detection, since fake news detection and topics are highly correlated (Zhang et al., 2020; Jin et al., 2016). For example, the news documents in the \"health\" topic are inclined towards false, while the documents belonging to the \"economy\" topic are biased to be trusted instead.\n\nParticularly, we first construct a directed heterogeneous document graph for each news document, containing sentences, topics and entities as nodes. The sentences are fully connected in bi-direction. Each sentence is also connected with its top relevant topics in bi-direction. If a sentence contains an entity, one directed link is built from the sentence to the entity. The reason for building one-way links from sentences to entities is to ensure that we can learn contextual entity representations that encode the semantics of the news, while avoiding the influence of the true entity knowledge to the news representation. Based on the directed heterogeneous document graph, we develop a heterogeneous graph attention network to learn topic-enriched news representations and contextual entity representations. The learned contextual entity representations are then compared to the corresponding KB-based entity representations with a carefully designed entity comparison network, in order to capture the semantic consistency between the news content and external KB. Finally, the topic-enriched news representations and the entity comparison features are combined for fake news classification. To facilitate related researches, we release both our code and dataset to the public. In summary, our main contributions include:\n\nIn this paper, we propose a novel end-to-end graph neural model CompareNet which compares the news to the external knowledge through entities for fake news detection.\n\nIn CompareNet, we also consider the useful topic information. We construct a directed heterogeneous document graph incorporating topics and entities. Then we develop heterogeneous graph attention networks to learn topic-enriched news representations. A novel entity comparison network is designed to compare the news to the KB.\n\nExtensive experiments on two benchmark datasets demonstrate that our model significantly outperforms state-of-the-art models on fake news detection by effectively incorporating external knowledge and topic information.",
       "charge": false
     },
     {
       "title": "Improving fake news detection by using an entity-enhanced framework to fuse diverse multimodal clues",
       "author":["Peng Qi", "Juan Cao", "Xirong Li", "Huan Liu", "Qiang Sheng", "Xiaoyue Mi", "Qin He", "Yongbiao Lv", "Chenyang Guo", "Yingchao Yu"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false,
       "remarks": "ACM but, arxiv available"
     },
     {
       "title": "CAUSAL INFERENCE IN STATISTICS A PRIMER",
       "author":["Madelyn Glymour", "Judea Pearl", "Nicholas P Jewell"] ,
       "year": 2016,
       "abstract": "",
       "introduction": "",
       "charge": false,
       "remarks": "book"
     },
     {
       "title": "Causal Intervention for Leveraging Popularity Bias in Recommendation",
       "author":["Yang Zhang", "Fuli Feng", "Xiangnan He", "Tianxin Wei","Chonggang Song", "Guohui Ling", "Yongdong Zhang"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false,
       "remarks": ""
     },
     {
       "title": "Clicks can be Cheating: Counterfactual Recommendation for Mitigating Clickbait Issue",
       "author":["Wenjie Wang", "Fuli Feng", "Xiangnan He", "Hanwang Zhang", "Tat-Seng Chua"],
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning",
       "author":["Bing Tian", "Yixin Cao", "Yong Zhang", "Chunxiao Xing"] ,
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Counterfactual Inference for Text Classification Debiasing",
       "author":["Chen Qian", "Fuli Feng", "Lijie Wen", "Chunping Ma", "Pengjun Xie"] ,
       "year": 2021,
       "abstract": "Today’s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models’ generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework – Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases ‘poison’ the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair’s effectiveness, generalizability and fairness.",
       "introduction": "Text classification, mapping text documents to a set of predefined categories, is a fundamental and important technique serving for many applications such as sentiment analysis (Qian et al., 2020b), partisanship recognition (Kiesel et al., 2019), and spam detection (Castillo et al., 2007). Machine learning models have become the default choice of solving text classification, owing to their ability to recognize the textual patterns from the labeled documents (Kim, 2014; Howard and Ruder, 2018). Nevertheless, they are at the risk of inadvertently capturing and even amplifying the unintended dataset biases (Zhao et al., 2017; Zhang et al., 2020; Feder et al., 2020; Blodgett et al., 2020), which can be at the document-level (i.e., label bias) and word-level (i.e., keyword bias).\n\nThe label bias issue occurs in the scenarios where a portion of the categories possesses a majority of training examples than others. For example, the label distribution of a binary sentiment analysis dataset could be 95%:5% (Dixon et al., 2018). Many previous studies found that the models trained on such data are potentially at the risk of simply predicting the majority answers (Dixon et al., 2018; Zhang et al., 2020). The keyword bias issue occurs in the situation where trained models exhibit excessive correlations between certain words and categories, e.g., some sentiment-irrelevant words – “black” or “islam” – are always connected to the negative category. As such, models always lean to unfairly predict any document containing those keywords to a specific category according to the biased statistical information instead of intrinsic textual semantics (Waseem and Hovy, 2016; Liu and Avci, 2019). The serious disadvantages limit models’ generalization, especially in the scenarios where the training data is differently distributed with the testing data (Niu et al., 2021; Goyal et al., 2017).\n\nTo resolve the issues, an effective solution is to perform data-level manipulations (e.g., resampling (Qian et al., 2020b)), which effectively transforms a training set to a relatively balanced one before training. Another line of debiasing work typically designs model-level balancing mechanisms (e.g., reweighting (Zhang et al., 2020)), aiming to adaptively decrease the influence of majority categories while increasing the minority during training. The core of the two types of solutions is to explicitly or implicitly recover unbiased distributions and prevent models from capturing the unintended biases. Unfortunately, the data-level strategy typically suffers from the extra manual cost of data collection, selection, and annotation (Zhang et al., 2020), requires much longer training time, and normally enlarges the gap between training and testing data distributions. The model-level strategy typically needs elaborate selection or definition of balancing strategies and needs relearning from scratch once certain balancing mechanisms (e.g., an unbiased training objective) are redesigned.\n\nMust machine learning models perform debiasing before or during training? Think about the difference in the decision-making processes between machines and humans. Machine learning systems are forced to imitate the behavior from observations via maximizing the prior probability, from which the decision is directly drawn during inference. By contrast, we humans, although born and raised in a biased nature, have the ability of counterfactual inference to make unbiased decisions with biased observations (Niu et al., 2021). To illustrate, we briefly compare the traditional factual inference and the counterfactual inference in text classification:\n\nFactual Inference: What will the prediction be if seeing an input document?\nCounterfactual Inference: What will the prediction be if seeing the main content of an input document only and had not seen the confounding dataset biases?\nThe counterfactual inference essentially gifts humans the imagination ability (i.e., had not done) to make decisions with a collaboration of the main content and the confounding biases (Tang et al., 2020), as well as to introspect whether our decision is deceived (Niu et al., 2021), i.e., counterfactual inference leads to debiased prediction.\n\nInspired by this, we propose a novel model-agnostic paradigm (CORSAIR), which adopts factual learning before mitigating the negative influence of the dataset biases in inference (i.e., after training), without the need for employing data manipulations or designing balancing mechanisms. Concretely, in training, CORSAIR directly trains a base model on an original training set, allowing the unintended dataset biases to \"poison\" the model. To \"rescue\" the testing documents from the poisonous model, in testing, for each factual input document, CORSAIR imagines its two types of counterfactual counterparts to produce two counterfactual outputs as the distilled label bias and keyword bias. Lastly, CORSAIR performs a bias removal operation to produce a counterfactual prediction that corresponds to a debiased decision. To verify, we perform extensive experiments on multiple public benchmark datasets. The results demonstrate our proposed framework’s effectiveness, generalizability, and fairness, proving that CORSAIR, when employed on four different types of base models, is significantly helpful to mitigate the two types of dataset biases.",
       "charge": false
     },
     {
       "title": "De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention",
       "author":["Wenkai Zhang", "Hongyu Lin", "Xianpei Han", "Le Sun"] ,
       "year": 2021,
       "abstract": "Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes. Based on the SCM, we learn de-biased DS-NER via causal interventions. For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries. Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER.",
       "introduction": "Named entity recognition (NER) aims to identify text spans pertaining to specific semantic types, which is a fundamental task of information extraction and enables various downstream applications such as Relation Extraction (Lin et al., 2016) and Question Answering (Bordes et al., 2015). The past several years have witnessed the remarkable success of supervised NER methods using neural networks (Lample et al., 2016; Ma and Hovy, 2016; Lin et al., 2020), which can automatically extract effective features from data and conduct NER in an end-to-end manner. Unfortunately, supervised methods rely on high-quality labeled data, which is very labor-intensive and thus severely restricts the application of current NER models. To resolve the data bottleneck, a promising approach is distant supervision based NER (DS-NER). DS-NER automatically generates training data by matching entities in easily-obtained dictionaries with plain texts. Then this distantly-labeled data is used to train NER models, commonly accompanied by a denoising step. DS-NER significantly reduces the annotation cost for building an effective NER model and therefore has attracted great attention in recent years (Yang et al., 2018; Shang et al., 2018; Peng et al., 2019; Cao et al., 2019; Liang et al., 2020; Zhang et al., 2021).\n\nHowever, the learning of DS-NER is dictionary-biased, which severely harms the generalization and the robustness of the learned DS-NER models. Specifically, entity dictionaries are often incomplete (missing entities), noisy (containing wrong entities), and ambiguous (a name can be of different entity types, such as Washington). And DS will generate positively-labeled instances from the in-dictionary names but ignore all other names. Such a biased dataset will inevitably mislead the learned models to overfit in-dictionary names and underfit out-of-dictionary names. We refer to this as intra-dictionary bias. To illustrate this bias, Figure 1 (a) shows the predicting likelihood of a representative DS-NER model (RoBERTa + Classifier (Liang et al., 2020)). We can see that there is a remarkable likelihood gap between in-dictionary mentions and out-of-dictionary mentions: the average likelihoods of out-of-dictionary mentions are < 0.2, which means that a great majority of them cannot be recalled. Furthermore, such a skewed distribution makes DS-NER models very sensitive to slight perturbations. We refer to this as inter-dictionary bias, i.e., different dictionaries can result in very different model behaviors. In the example shown in Figure 1 (b), we train the same DS-NER model by respectively using 4 dictionaries sampled from the same original dictionary, where each of them covers 90% of entities in the original one. We can see that the predicting likelihood diverges significantly even these 4 dictionaries share the majority part. Consequently, the dictionary-biased learning will undermine both the effectiveness and robustness of DS-NER models.\n\nIn this paper, we propose a causal framework to fundamentally explain and resolve the dictionary bias problem in DS-NER. We first formulate the procedure of DS-NER from the causal view with a Structural Causal Model (SCM) (Pearl et al., 2000), which is shown in the left part of Figure 2. From the SCM, we identified that the intra-dictionary bias stems from the dictionary which serves as a confounder during the model learning. The dictionary confounder will introduce two backdoor paths, one from positively-labeled instances (Xp) to entity labels (Y) and the other from negatively-labeled instances (Xn) to entity labels. These backdoor paths introduce spurious correlations during learning, therefore result in the intra-dictionary bias. Furthermore, the current learning criteria of DS-NER models is to optimize over the correlations between the instances (X) and entity types (Y) given one specific dictionary (D), namely P(Y |X, D). Such criteria, however, diverges from the primary goal of learning a dictionary-free NER model (i.e., P(Y |X)), and results in the inter-dictionary bias. Based on the above analysis, unbiased DS-NER should remove the spurious correlations introduced by backdoor paths and capture the true dictionary-free causal relations.\n\nTo this end, we conduct causal interventions to de-bias DS-NER from the biased dictionary. For intra-dictionary bias, we intervene on the positive instances and the negative instances to block the backdoor paths in SCM, then the spurious correlations introduced by dictionary confounder will be removed. Specifically, we conduct backdoor adjustment to learn de-biased DS-NER models, i.e., we optimize the DS-NER model based on the causal distribution, rather than from the spurious correlation distribution. For inter-dictionary bias, we propose to leverage causal invariance regularizer (Mitrovic et al., 2021), which will make the learned representation more robust to the perturbation of dictionaries. For each instance in the training data, causal invariance regularizer will preserve the underlying causal effects unchanged across different dictionaries. The proposed method is model-free, which can be used to resolve the dictionary bias in different DS-NER models by being applied as a plug-in during model training. We conducted experiments on four standard DS-NER datasets: CoNLL2003, Twitter2005, Webpage, and Wikigold. Experiments on three state-of-the-art DS-NER models show that the proposed de-biasing method can effectively solve both intra-dictionary and inter-dictionary biases and therefore significantly improve the performance and the robustness of DS-NER in almost all settings. Generally, the main contributions of this paper are:\n\nWe proposed a causal framework, which not only fundamentally formulates the DS-NER process but also explains the causes of both intra-dictionary bias and inter-dictionary bias.\nBased on the causal framework, we conducted causal interventions to de-bias DS-NER. For intra-dictionary bias, we conduct causal interventions via backdoor adjustment to remove spurious correlations introduced by the dictionary confounder. For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries.\nExperimental results on four standard DS-NER datasets and three DS-NER models demonstrate that our method can significantly improve the performance and the robustness of DS-NER.",
       "charge": false
     },
     {
       "title": "How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis",
       "author":["Shaobo Li", "Xiaoguang Li", "Lifeng Shang", "Zhenhua Dong", "Chengjie Sun", "Bingquan Liu", "Zhenzhou Ji", "Xin Jiang", "Qun Liu"] ,
       "year": 2022,
       "abstract": "Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs' ability to fill in the missing factual words in cloze-style prompts such as \"Dante was born in [MASK].\" However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.",
       "introduction": "Do Pre-trained Language Models (PLMs) capture factual knowledge? LAMA benchmark (Petroni et al., 2019) answers this question by quantitatively measuring the factual knowledge captured in PLMs: query PLMs with cloze-style prompts such as “Dante was born in [MASK]?” Filling in the mask with the correct word “Florence” is considered a successful capture of the corresponding factual knowledge. The percentage of correct fillings over all the prompts can be used to estimate the amount of factual knowledge captured. PLMs show a surprisingly strong ability to capture factual knowledge in such probings (Jiang et al., 2020; Shin et al., 2020; Zhong et al., 2021), which elicits further research on a more in-depth question (Cao et al., 2021; Elazar et al., 2021a): How do PLMs capture the factual knowledge?\n\nIn this paper, we try to answer this question with a two-fold analysis:\n\nResearch Question 1: Which association do PLMs depend on to capture factual knowledge?\n\nResearch Question 2: Is the association on which PLMs depend effective in capturing factual knowledge?\n\nWe use association to refer to the explicit association between the missing words and the remaining words in the context. We define three typical associations between words. Figure 1 illustrates these associations in a mask-filling sample.\n\nDefinition 1 Knowledge-Dependent (KD): According to a Knowledge Base (KB), the missing words can be deterministically predicted when providing the remaining words.\nDefinition 2 Positionally Close (PC): The remaining words are positionally close to the missing words.\nDefinition 3 Highly Co-occurred (HC): The remaining words have a higher co-occurrence frequency with the missing words.\nQuestion 1 investigates how much PLMs depend on a specific group of remaining words to predict the missing words in pre-training samples. We select the remaining words to be investigated according to their association with the missing words. We propose a causal-inspired method to quantify the word-level dependence in each sample. The average dependence on the remaining words that hold the same association with the missing words over all the samples indicates how PLMs rely on this association to predict the missing words. We refer to this average dependence as dependence on the association. The above analysis is named dependence measure.\n\nIn Question 2, we reveal the effectiveness of dependence by the correlation between the quantified dependence on associations and the factual knowledge capturing performance. The performance is probed with additionally crafted cloze-style prompts (Elazar et al., 2021a). The more the dependence on an association positively correlates with the probing performance, the more effective this association is. We refer to the second analysis as the effectiveness measure. According to the experiment results, we have the following observations:\n\nObservation 1: The PLMs depend more on the positional close and highly co-occurred associations than the knowledge-dependent association to capture factual knowledge.\n\nObservation 2: Depending on the knowledge-dependent association is more effective for factual knowledge capture than positional close and highly co-occurred associations.\n\nBy connecting the two observations, we can answer the question of “how PLMs capture factual knowledge” as: The PLMs are capturing factual knowledge ineffectively since the PLMs depend more on the PC and HC association than the KD association.\n\nThe contribution of this paper can be summarized as follows:\n\nWe quantify the word-level dependence for mask filling with a causal-inspired method, revealing the word-level patterns that PLMs use to predict the missing words quantitatively.\nWe compare the effectiveness of the dependence on different associations, which provides direct insights for improving PLMs for factual knowledge capture.\nThis paper introduces causal theories into PLMs by formulating the effect measurement process in mask language modeling. It paves the path to measure the causal effects between entities or events described in natural language.",
       "charge": false
     },
     {
       "title": "Generalizing to the future: Mitigating entity bias in fake news detection",
       "author":["Yongchun Zhu", "Qiang Sheng", "Juan Cao", "Shuokai Li", "Danding Wang", "Fuzhen Zhuang"] ,
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false
     }
  ]
}
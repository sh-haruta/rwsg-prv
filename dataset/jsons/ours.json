{"root":
  {
    "title": "LLM-based Related Work Section Generation Framework Incorporating Perspectives Researchers Value",
    "author": ["anonymous"],
    "abstract": "This paper proposes a Large Language Model (LLM)-based framework to generate paper's related work section, incorporating perspectives valued by researchers. While LLMs excel at summarization, ambiguous instructions limit the clarity of related work section for researchers. Through the surveys, we identified the preferred perspectives for a related work section: ``categorization'', ``comparison'', and ``pointing problems''. We incorporate these perspectives into a prompt with few-shot examples. Furthermore, we adopt a mechanism to extract evidence from cited papers, providing the framework with explainability and aiding in hallucination checks. Experimental results with human evaluation demonstrate that the generated related work section is of competitive quality with human-written sections and has fewer hallucinations. The codes of the proposed framework and dataset we collected are available at https://github.com/hogefuga}.",
    "introduction": "Scholarly papers serve as one of the most essential cornerstones in the development of science and technology (Doumont et al., 2014). Papers clearly convey new discoveries and ideas, being crucial means for the accumulation of human knowledge. Within sections of papers, the “Related Work” plays a pivotal role for it. A related work section not only presents a list of existing research but also provides the context for the current work. For authors, the related work section entails extensive reading, sorting, and analyzing numerous publications, making it a laborious and time-consuming task.\n\nTo alleviate this situation, recent studies focus on the task of “related work section generation” (Li and Ouyang, 2022). The field of this research is pioneered by Hoang and Kan (2010), and researchers utilize the natural language processing (NLP) techniques to generate related work sections. The extractive methods identify the key sentence of cited papers based on importance scores and generate the related work section by concatenating sentences (Deng et al., 2021; Hu and Wan, 2014). In the abstractive methods, authors mainly utilize Transformer (Vaswani et al., 2017)-based architectures and try to summarize the contents of cited papers (Liu et al., 2023; Chen et al., 2022, 2021). While showing promising results, these methods inevitably include low-quality data in training process. For example, it is the related work section with merely enumerating methodologies or having scattered similar claims. Realistically, eliminating low-quality data like this is difficult since training Transformer-based architecture requires a substantial amount of data. As a result, the output can be uncomfortable forms for readers (researchers).\n\nSince scholarly papers are meant for humans so far, we believe that explicitly capturing the writing style preferred by the researchers is crucial. Large language models (LLMs) like ChatGPT (OpenAI, 2023) shed light on this perspective. By teaching its role, LLMs can change its behavior depending on the prompts. According to the report, the text summarization capability of GPT4 is on par with human-level performance (Pu et al., 2023). However, generating the related work section is a specialized task, and LLMs tend to disregard human writing conventions without clear instructions. Creating a prompt explicitly tailored for this task is required.\n\nIn this paper, we propose an LLM-based framework to generate a related work section, which incorporates perspectives valued by researchers. To identify the perspective researchers value, we investigate two surveys. The first one is a questionnaire-based survey. We asked researchers to itemize what they are careful about when writing a related work section using a free-response format. As a result, we identify five perspectives: “Quality”, “Freshness”, “Categorization”, “Comparison”, and “Problem”. In the second survey, we investigate papers published in the top conference to verify the above result. As we expected, these five perspectives are covered at a high rate in many papers. In particular, we focus on three perspectives – categorization, comparison, and problem – that can be explicitly instructed to LLMs. We incorporate them into a prompt with few-shot examples. In addition, we adopt a mechanism to extract evidence from cited papers, providing the framework with explainability and aiding in hallucination checks. Finally, through the experimental results with human evaluation, we demonstrate that the generated related work section is of competitive quality with human-written sections and has fewer hallucinations.\n\nThe contributions of this paper are as follows:\n• We identify the perspectives researchers value via surveys. The results are useful for not only researchers in this field but also researchers who would like to write a good related work section.\n• Based on findings of surveys, we propose an LLM-based framework that can generate a related work section for given cited papers. In addition, to the best of our knowledge, this paper is the first one that generates a related work section via GPT4-turbo. We input the contents in introductions of cited papers to GPT4-turbo, utilizing the extended token size.\n• For the development of this research field, we make our codes and the collected dataset publicly available.\n\nThe rest of this paper is organized as follows. We describe the preliminaries in Section 2. The proposed framework is presented in Section 3. Section 4 demonstrates the experimental results with an example of the generated related work section. Finally, we conclude the paper in Section 5.",
    "relatedwork": "will be generated"},
  "leaves":
   [
     {
       "title": "Towards Automated Related Work Summarization",
       "author": ["Cong Duy Vu Hoang", "Min-Yen Kan"],
       "year": 2010,
       "abstract": "We introduce the novel problem of automatic related work summarization. Given multiple articles (e.g., conference/journal papers) as input, a related work summarization system creates a topic-biased summary of related work specific to the target paper. Our prototype Related Work Summarization system, ReWoS, takes in a set of keywords arranged in a hierarchical fashion that describes a target paper’s topics, to drive the creation of an extractive summary using two different strategies for locating appropriate sentences for general topics as well as detailed ones. Our initial results show an improvement over generic multi-document summarization baselines in a human evaluation.",
       "introduction": "In many fields, a scholar needs to show an understanding of the context of his problem and relate his work to prior community knowledge. A related work section is often the vehicle for this purpose; it contextualizes the scholar’s contributions and helps readers understand the critical aspects of the previous works that current work addresses. Creating such a summary requires the author to position his own work within the contextual research to showcase the advantages of his method.\n\nWe envision an NLP application that assists in creating a related work summary. We propose this related work summarization task as a challenge to the automatic summarization community. In its full form, it is a topic-biased, multi-document summarization problem that takes as input a target scientific document for which a related work section needs to be drafted. The output goal is to create a related work section that finds the relevant related works and contextually describes them in relationship to the scientific document at hand.\n\nWe dissect the full challenge as bringing together work of disparate interests; 1) in finding relevant documents; 2) in identifying the salient aspects of these documents in relation to the current work worth summarizing; and 3) in generating the final topic-biased summary. While it is clear that current NLP technology does not let us build a complete solution for this task, we believe that tackling the individual components will help bring us towards an eventual solution.\n\nIn fact, existing works in the NLP and recommendation systems communities have already begun work that fits towards the completion of the first two tasks. Citation prediction (Nallapati et al., 2008) is a growing research area that has aimed both at predicting citation growth over time within a community and at individual paper citation patterns. This year, an automatic keyphrase extraction task from scientific articles was first fielded in SemEval-2, partially addressing Task 1. Also, automatic survey generation (Mohammad et al., 2009) is becoming a growing field within the summarization community. However, to date, we have not yet seen any work that examines topic-biased summarization of multiple scientific articles. For these reasons, we focus on Task 3 here – the creation of a related work section, given a structured input of the topics for summary. The remaining contributions of our paper consist of work towards this goal:\n\n• We conduct a study of the argumentative patterns used in related work sections, to describe the plausible summarization tactics for their creation in Section 3.\n\n• We describe our approach to generate an extractive related work summary given an input topic hierarchy tree, using two separate strategies to differentiate between summarizing shallow internal nodes from deep detailed leaf nodes of the topic tree in Section 4.",
       "charge":  false
     },
     {
       "title": "Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach",
       "author":["Yue Hu", "Xiaojun Wan"] ,
       "year": 2014,
       "abstract": "In this paper, we investigate a challenging task of automatic related work generation. Given multiple reference papers as input, the task aims to generate a related work section for a target paper. The generated related work section can be used as a draft for the author to complete his or her final related work section. We propose our Automatic Related Work Generation system called ARWG to address this task. It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts, and then applies regression models to learn the importance of the sentences. At last, it employs an optimization framework to generate the related work section. Our evaluation results on a test set of 150 target papers along with their reference papers show that our proposed ARWG system can generate related work sections with better quality. A user study is also performed to show ARWG can achieve an improvement over generic multi-document summarization baselines.",
       "introduction": "The related work section is an important part of a paper. An author often needs to help readers understand the context of his or her research problem and compare his or her current work with previous works. A related work section is often used for this purpose to show the differences and advantages of his or her work, compared with related research works. In this study, we attempt to automatically generate a related work section for a target academic paper with its reference papers. This kind of related work sections can be used as a basis to reduce the author’s time and effort when he or she wants to complete his or her final related work section.\n\nAutomatic related work section generation is a very challenging task. It can be considered a topic-biased, multiple-document summarization problem. The input is a target academic paper, which has no related work section, along with its reference papers. The goal is to create a related work section that describes the related works and addresses the relationship between the target paper and the reference papers. Here we assume that the set of reference papers has been given as part of the input. Existing works in the NLP and recommendation systems communities have already focused on the task of finding reference papers. For example, citation prediction (Nallapati et al., 2008) aims at finding individual paper citation patterns.\n\nGenerally speaking, automatic related work section generation is a strikingly different problem and it is much more difficult in comparison with general multi-document summarization tasks. For example, multi-document summarization of news articles aims at synthesizing contents of similar news and removing the redundant information contained by the different news articles. However, each scientific paper has much specific content to state its own work and contribution. Even for the papers that investigate the same research topic, their contributions and contents can be totally different. The related work section generation task needs to find the specific contributions of individual papers and arrange them into one or several paragraphs.\n\nIn this study, we focus on the problem of automatic related work section generation and propose a novel system called ARWG to address the problem. For the target paper, we assume that the abstract and introduction sections have already been written by the author and they can be used to help generate the related work section. For the reference papers, we only consider and extract the abstract, introduction, related work, and conclusion sections, because other sections like the method and evaluation sections always describe the extreme details of the specific work and they are not suitable for this task. Then we generate the related work section using both sentence sets which are extracted from the target paper and reference papers, respectively.\n\nFirstly, we use a PLSA model to group both sentence sets of the target paper and its reference papers into different topic-biased clusters. Secondly, the importance of each sentence in the target paper and the reference papers is learned by using two different Support Vector Regression (SVR) models. At last, a global optimization framework is proposed to generate the related work section by selecting sentences from both the target paper and the reference papers. Meanwhile, the framework selects sentences from different topic-biased clusters globally.\n\nExperimental results on a test set of 150 target papers show our method can generate related work sections with better quality than those of several baseline methods. With the ROUGE toolkit, the results indicate the related work sections generated by our system can get higher ROUGE scores. Moreover, our related work sections can get higher rating scores based on a user study. Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections.",
       "charge": false
     },
     {
       "title": "Causal Intervention for Abstractive Related Work Generation",
       "author":["Jiachang Liu", "Qi Zhang", "Chongyang Shi", "Usman Naseem", "Shoujin Wang", "Liang Hu", "Ivor Tsang"] ,
       "year": 2023,
       "abstract": "Abstractive related work generation has attracted increasing attention in generating coherent related work that helps readers grasp the current research. However, most existing models ignore the inherent causality during related work generation, leading to spurious correlations which downgrade the models’ generation quality and generalizability. In this study, we argue that causal intervention can address such limitations and improve the quality and coherence of generated related work. To this end, we propose a novel Causal Intervention Module for Related Work Generation (CaM) to effectively capture causalities in the generation process. Specifically, we first model the relations among the sentence order, document (reference) correlations, and transitional content in related work generation using a causal graph. Then, to implement causal interventions and mitigate the negative impact of spurious correlations, we use do-calculus to derive ordinary conditional probabilities and identify causal effects through CaM. Finally, we subtly fuse CaM with Transformer to obtain an end-to-end related work generation framework. Extensive experiments on two real-world datasets show that CaM can effectively promote the model to learn causal relations and thus produce related work of higher quality and coherence.",
       "introduction": "A comprehensive related work usually covers abundant reference papers, which costs authors plenty of time in reading and summarization and even forces authors to pursue ever-updating advanced work (Hu and Wan, 2014). Fortunately, the task of related work generation emerged and attracted increasing attention from the community of text summarization and content analysis in recent years (Chen et al., 2021, 2022). Related work generation can be considered as a variant of the multi-document summarization task (Li and Ouyang, 2022). Distinct from multi-document summarization, related work generation entails comparison after the summarization of a set of references and needs to sort out the similarities and differences between these references (Agarwal et al., 2011).\n\nRecently, various abstractive text generation methods have been proposed to generate related work based on the abstracts of references. For example, Xing et al. (2020a) used the context of citation and the abstract of each cited paper as the input to generate related work. Ge et al. (2021) encoded the citation network and used it as external knowledge to generate related work. Chen et al. (2022) proposed a target-aware related work generator that captures the relations between reference papers and the target paper through a target-centered attention mechanism. Equipped with well-designed encoding strategies, external knowledge, or novel training techniques, these studies have made promising progress in generating coherent related work.\n\nHowever, those models are inclined to explore and exploit spurious correlations, such as high-frequency word/phrase patterns, writing habits, or presentation skills, to build superficial shortcuts between reference papers and the related work of the target paper. Such spurious correlations may harm the quality of the generated related work, especially when distribution shift exists between the testing set and training set. This is because spurious correlations are different from genuine causal relations. They often do not intrinsically contribute to the related work generation and easily cause the robustness problem and impair the models’ generalizability (Arjovsky et al., 2019).\n\nFigure 1 illustrates the difference between causality and spurious correlation. The phrases \"for example\" and \"later\" are often used to bridge two sentences in related work. Their usage may be attributed to writers’ presentation habits about organizing sentence orders or the reference document relations corresponding to the sentences. Ideally, a related work generation model is expected to learn the reference relation and distinguish it from the writing habits. However, previous generation models easily capture the superficial habitual sentence organization (a spurious correlation) instead of learning complex causal reference relations, especially when the habitual patterns frequently occur in the training set. In this case, the transitional phrases generated mainly based on writing habits are likely to be unsuitable and subsequently affect the content generation of related work during testing when the training and testing sets are not distributed uniformly.\n\nFortunately, causal intervention can effectively remove spurious correlations and focus on causal relations by intervening in the learning process. It not only observes the impact of the sentence order and document relation on generating transitional content but also probes the impact of each possible order on the whole generation of related work, thereby removing the spurious correlations (Pearl, 2009a). Accordingly, causal intervention serving as an effective solution allows causal relations to exert a greater impact and instruct the model to produce the correct content.\n\nAccordingly, to address the aforementioned gaps in existing work for related work generation, we propose a novel Causal Intervention Module for Related Work Generation (CaM), which effectively removes spurious correlations by performing the causal intervention. Specifically, we first model the relations among sentence order, document relation, and transitional content in related work generation and identify the confounder that raises spurious correlations (see Figure 2). Then, we implement causal intervention that consists of three components: 1) Primitive Intervention cuts off the connection that induces spurious correlations in the causal graph by leveraging do-calculus and backdoor criterion (Pearl, 2009a), 2) Context-aware Remapping smoothens the distribution of intervened embeddings and injects contextual information, and 3) Optimal Intensity Learning learns the best intensity of overall intervention by controlling the output from different parts. Finally, we strategically fuse CaM with Transformer (Vaswani et al., 2017) to deliver an end-to-end causal related work generation model. Our main contributions are as follows:\n\nTo the best of our knowledge, this work is the first attempt to introduce causality theory into the related work generation task.\nWe propose a novel Causal Intervention Module for Related Work Generation (CaM) which utilizes causal intervention to mitigate the impact of spurious correlations. CaM is subtly fused with Transformer to derive an end-to-end causal related work generation model, enabling the propagation of intervened information.\nExtensive experiments on two real-world benchmark datasets demonstrate that our proposed model can generate related works of high quality and verify the effectiveness and rationality of bringing causality theory into the related work generation task.",
       "charge": false
     },
     {
       "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning",
       "author":["Xiuying Chen", "Hind Alamro", "Mingzhe Li", "Shen Gao", "Rui Yan", "Xin Gao", "Xiangliang Zhang"] ,
       "year": 2022,
       "abstract": "The related work section is an important component of a scientific paper, which highlights the contribution of the target paper in the context of the reference papers. Authors can save their time and effort by using the automatically generated related work section as a draft to complete the final related work. Most of the existing related work section generation methods rely on extracting off-the-shelf sentences to make a comparative discussion about the target work and the reference papers. However, such sentences need to be written in advance and are hard to obtain in practice. Hence, in this paper, we propose an abstractive target-aware related work generator (TAG), which can generate related work sections consisting of new sentences. Concretely, we first propose a target-aware graph encoder, which models the relationships between reference papers and the target paper with target-centered attention mechanisms. In the decoding process, we propose a hierarchical decoder that attends to the nodes of different levels in the graph with keyphrases as semantic indicators. Finally, to generate a more informative related work, we propose multi-level contrastive optimization objectives, which aim to maximize the mutual information between the generated related work with the references and minimize that with non-references. Extensive experiments on two public scholar datasets show that the proposed model brings substantial improvements over several strong baselines in terms of automatic and tailored human evaluations.",
       "introduction": "Good scientific papers often need to help readers comprehend the contribution by contextualizing the proposed work in the scientific fields. The related work section serves as a pivot for this goal, which compares and connects the innovation and novelty of the current work with previous studies. The task of Related Work section Generation (RWG) is thus proposed [17], which aims to generate a related work section for a target paper given multiple cited papers as reference. The generated related work section can be used as a draft for the author to complete his or her final related work section. Hence, RWG can greatly reduce the author’s time and effort when writing a paper [19].\n\nThe unique characteristics of the task and the use of rare, technical terms in scientific articles make RWG challenging. Conventional extractive RWG methods rely on off-the-shelf sentences to generate a target-aware related work section. For example, Hu and Wan [19] split the sentences of the references and the target paper into different topic-biased parts, and then apply importance scores to each sentence. Subsequently, Chen and Zhuge [4] collected sentences from the target paper and papers that cite the reference papers to form a related work. In such cases, sentences that make comparative discussion need to be written in advance, which still require human efforts and are hard to obtain. Abstractive text generation technique is a natural solution to this problem, which has sophisticated abilities such as paraphrasing and generalization. Xing et al. [39] and Ge et al. [14] proposed to generate a short text to explain the connection between a pair of papers. However, they only considered sentences with a single citation, and their proposed model also hard-coded the setting of two input sources, instead of a variable number of input sources. Chen et al. [5] aimed to generate related work for a set of reference papers, but they didn’t consider the target paper information. Thus, the generated related work cannot describe how the target relates to the cited works.\n\nTo tackle the above problems and generate a more comprehensive related work, we propose a target-aware related work generator (TAG), which generates abstractive and contextual related work sections. Concretely, we first propose a target-aware graph encoder, where the target paper interacts with the references papers under target-centered attention mechanisms. Herein, the interaction consists of direct information flow between papers and indirect flow with keyphrases as an intermediary. The former allows global information exchange, while the latter lets information interact under a specific topic. In the decoding phase, we introduce a hierarchical decoder that first attends to the keyphrase nodes in the graph and then projects the attention weights on the reference papers, and keyphrases. In this way, we let the keyphrases be the semantic indicator and guide the generation process as a more abstract summarization clue. To generate more informative related work sections with substantial content, we adapt the contrastive learning mechanism to the RWG task, where we aim to maximize the mutual information between the generated text with reference papers and minimize that with non-references.\n\nWe evaluate our model on two large-scale benchmark scholar datasets collected from S2ORC and Delve. S2ORC consists of papers in multiple domains (physics, math, computer science, etc.), and Delve consists of computer science papers. We show that TAG outperforms the state-of-the-art summarization and RWG baselines on both datasets in terms of both ROUGE metrics and tailored human evaluations. Since our contrastive learning module can be an add-on module, we also demonstrate its effectiveness on various baseline models. A part of the generated related work for this submission of our model and the most recent baseline is shown in Figure 1, illustrating the advantage of TAG over the baseline.\n\nOur contributions can be summarized as follows:\n\nWe first propose to incorporate the target paper information into the abstractive related work generation task with multiple citations.\nTo solve this task, we propose a target-aware related work generator that first models the relationship between target paper and the cited papers by keyphrases, and then generates the related work guided by contrastive learning. The generated related work can contextualize the target work in related fields.\nExperiments conducted on two benchmark datasets show that our model outperforms all the state-of-the-art approaches. Human evaluations also demonstrate that most of the generated related works are more constructive than those produced by the baseline methods.",
       "charge": false
     },
     {
       "title": "Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation",
       "author":["Xiuying Chen", "Hind Alamro", "Mingzhe Li", "Shen Gao", "Xiangliang Zhang", "Dongyan Zhao", "Rui Yan"] ,
       "year": 2021,
       "abstract": "Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area. Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph. The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process. We also contribute two public datasets composed of related work sections and their corresponding papers. Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines. We hope that this work will promote advances in related work generation task.",
       "introduction": "The related work section generation task aims to automatically generate a summary of the most relevant works in a specific research area, which can help researchers familiarize themselves with the state of the art in the field. Several methods (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019) have been proposed to study how to obtain the related work section automatically by extracting important sentences from multiple original papers. However, extractive approaches lack the sophisticated abilities that are crucial to high-quality summarization such as paraphrasing and generalization, and often lead to a related work section with poor coherence and readability (See et al., 2017; Hsu et al., 2018). For example, as shown in Table 1, the extracted sentences share the pattern \"We find...\" as the subject of sentences, which, in fact, refer to different authors. On the contrary, the abstractive related work in Table 1 reveals that the works are conducted by different scholars. It also has conjunction words such as \"Furthermore\" and \"However,\" which can explain the logical relationship between the cited works and thus form an elegant narration. Hence, in this paper, we target the abstractive related work generation task, which generates related work including novel words and phrases not copied from the source text.\n\nThere are two main challenges in this task: (1) the related work should summarize the contribution of each paper, and (2) explain the relationship between different papers such as parallel, turning, and progressive relation, so as to introduce them in a logical order. While existing summarization models can address the first problem, they do not target comparing and explaining the relationship between these articles. Hence, to tackle the above challenges, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work given multiple scientific papers in the same research area.\n\nFirstly, we encode the multiple input articles in a hierarchical manner, obtaining the overall representation for each document. Then, we propose a relation-aware multi-document encoder that relates multiple input documents in a relation graph. In the training process, the relation graph and the document representation interact and are refined iteratively, complementing each other. Finally, in the decoder part, we utilize the relation graph information to assist the decoding process, where the model learns to decide whether to pay attention to the input documents or the relationship between them.\n\nTo evaluate our model, we introduce two large-scale related work generation datasets, which are composed of related work sections and their corresponding papers. Extensive experimental results show that RRG outperforms several strong baselines in terms of ROUGE metrics and human evaluations on both datasets.\n\nIn summary, our contributions include:\n\nWe address an abstractive related work generation task, which aims to generate an abstractive related work with novel words and phrases.\nWe propose a relation-aware multi-document encoder that relates one of the multiple input documents to another and establishes a relation graph storing the dependency between documents.\nWe contribute two public large-scale related work generation datasets that are beneficial for the community.",
       "charge": false
     },
     {
       "title": "Automatic Related Work Section Generation by Sentence Extraction and Reordering",
       "author":["Zekun Deng", " Zixin Zeng", "Weiye Gu", "Jiawen Ji", "Bolin Hu"] ,
       "year": 2021,
       "abstract": "Related work section is essential in a scientific publication, for it elaborates past studies relevant to the topic in comparison with the current one. The automatic generation of related work section in scientific papers is a meaningful yet challenging task. While prior works have gained encouraging results, they have not fully addressed the issue of informativeness and the difficulty of obtaining citation sentences due to delay of publication. In this paper, we introduce SERGE, a novel and effective system for generating descriptive related work section automatically by sentence extraction and reordering. Our system first employs a BERT-based ensemble model to select the most salient sentences in reference papers, and then uses a similar model to reorder these sentences for better readability. Automatic evaluation results show that SERGE significantly outperforms existing baselines on ROUGE metrics, gaining an improvement of 18% to 56% on recall and 4% to 33% on F-score. Human evaluation shows that SERGE gains a higher informativeness score than human-written gold standard as well as the baseline, indicating its ability to provide valuable information that matches the real interest of researchers. In contrast to existing methods, since our system is free from delayed citation problem and yields high informativeness, it shows a great potential for various applications.",
       "introduction": "Scientific papers usually contain a related work section, which is also known as a literature review. It summarizes previous works relevant to the research topic in order to establish the link between existing knowledge and new findings[1]. Very often, authors of scientific papers cite existing papers in this section to show the appropriateness of their research question, to justify their adopted methods, and/or to present the creativeness and superiority of their ideas. However, it is quite challenging to produce a high-quality related work section, since it involves identifying crucial points from a long piece of paper and reorganizing them in a neat and logical way. It is generally accepted that there are two distinct styles of literature reviews: descriptive and integrative[2, 3]. Descriptive literature reviews focus on individual papers and provide more detailed description of the methods, results, and implications of each study. They illustrate previous researches with high accuracy and are thus more objective and rigorous. In contrast, integrative literature reviews focus more on synthesis of ideas. Although including fewer details of individual studies, integrative literature reviews provide more high-level critical summaries of topics and are thus more condensed and structurally complex.[4] In this paper, we particularly focus on the generation of descriptive related work section. On this matter, Cohan and Goharian[5] have proposed a sentence ranking algorithm that takes advantage of citation context to summarize scientific papers. Abura’ed et al.[6] have proposed a citation-based summarizer for scientific documents based on supervised learning and achieved competitive results in CLScisumm-17 challenges. However, most of the existing studies require citing sentences (a.k.a., “citances”) of citing publications as inputs. Thus, these strategies are limited by delay of publications—mostly a new publication may not be widely recognized and cited within a short period of time and, therefore, it is quite hard to obtain the citing sentences mentioning the publication. To this end, in this paper, we propose a novel method for automatic generation of descriptive related work section in scientific papers by extracting salient sentences from scientific literature and rearranging them into a logical order. In contrast to most existing methods which suffer from citation delay problem, our method does not require any citances to achieve its goal, making it applicable even when no citation data is available. The main contributions of this paper are as follows: 1. We propose a novel and effective approach to automatic descriptive related work section generation based on extractive document summarization techniques, including sentence extraction and reordering. 2. Our method does not need any citation data to achieve its goal, which implies that the method does not suffer from the delay of citing publications or require the input of citation data. Such a characteristic offers more potentials of our proposed method with various applications.",
       "charge": false
     },
     {
       "title": "Automatic Related Work Generation: A Meta Study",
       "author":["Xiangci Li", "Jessica Ouyang"] ,
       "year": 2022,
       "abstract": "Academic research is an exploration activity to solve problems that have never been resolved before. By this nature, each academic research work is required to perform a literature review to distinguish its novelties that have not been addressed by prior works. In natural language processing, this literature review is usually conducted under the 'Related Work' section. The task of automatic related work generation aims to automatically generate the 'Related Work' section given the rest of the research paper and a list of cited papers. Although this task was proposed over 10 years ago, it received little attention until very recently, when it was cast as a variant of the scientific multi-document summarization problem. However, even till today, the problems of automatic related work and citation text generation are not yet standardized. In this survey, we conduct a meta-study to compare the existing literature on related work generation from the perspectives of problem formulation, dataset collection, methodological approach, performance evaluation, and future prospects to provide the reader insight into the progress of the state-of-the-art studies, as well as and how future studies can be conducted. We also survey relevant fields of study that we suggest future work to consider integrating.",
       "introduction": "Academic research is an exploration activity to solve problems that have never been resolved before. By this nature, each academic research work must sit at the frontier of the field and present novelties that have not been addressed by prior works. While the format may vary among different fields, in order to convince the reviewers and readers of the novelty of the current work, the authors must perform a literature review to compare their work with the prior works. In natural language processing, this literature review is usually conducted under the “Related Work” section.\n\nSince each author is obligated to review the relevant prior work in their field, many of which are shared among papers with the same task or topic, many related work sections in the same field are similar in both content and format. Moreover, the related work section is usually a stand-alone section that is relatively independent of the main thread of the paper and is usually written after the authors finish the main part of the paper. Therefore, it is a natural motivation to develop a system for generating related work sections automatically.\n\nAs Hoang and Kan [40] envision, the automatic related work generation task assists authors in creating a draft of a related work section. To generate the topic-biased related work, the task is a variant of multi-document summarization problems that take a target scientific document for which a related work section needs to be drafted, as well as a list of papers to be cited as the input. As Agarwal et al. [3] point out, multi-document summarization of scientific articles has unique characteristics compared to summarization of generic texts: summarizing multiple scientific articles requires understanding different arguments for each study, even if their research directions are the same. Moreover, an accurate description of a literature has to consider and combine all other different viewpoints toward the literature. The complexity of the concepts and the use of rare, technical terms in scientific articles further make this task challenging [119].\n\nHowever, automatic related work generation is not only about the multi-document summarization in the scientific domain. The challenge of automatic related work generation depends on the development of related, natural language understanding subtasks, such as citation and discourse analysis. Since related work generation not only requires the understanding of the main idea of a single document but the relationships and interactions among multiple documents, only when such relationships are modeled can we accurately generate a proper summary. Therefore, it is crucial to have a higher-level view that includes tasks and techniques that are beyond the central summarization task. Unfortunately, the existing studies have not reached this step yet.\n\nAlthough the problem of automatic related work generation was proposed over 10 years ago, this problem received little attention until very recently, when a few works revisited this problem with neural network-based systems [2, 18, 25, 37, 75, 117]. There are only a few existing papers that explicitly study related work generation. Further, the problems of automatic related work and citation text generation are not yet standardized, as each work has slightly different assumptions about the task, datasets used, and methods of evaluation, so that it is hard to compare the existing work in parallel. In this survey, we present a critical review of the state-of-the-art studies of the task of automatic related work generation by comparing from multiple perspectives such as problem formulation, dataset collection, methodological approaches, experimental results and evaluation across the limited number of available prior works. We also survey relevant fields of study such as citation analysis to inspire readers to conduct future studies on related work generation by considering and incorporating these related tasks.\n\nWe organize the rest of this paper as a meta-study of the task of related work generation, by providing an overview of the directly related works (§3), and a series of parallel comparisons of these works from the perspective of problem formulation (§4), dataset collection (§5), methodological approaches (§6), experimental results and evaluation (§7), proposed future work (§8), then we review related tasks that potentially benefit related work generation (§9), and finally we perform a general discussion (§10).",
       "charge": false,
       "remarks": "survey"
     },
     {
       "title": "ToC-RWG: Explore the Combination of Topic Model and Citation Information for Automatic Related Work Generation",
       "author":["Pancheng Wang", "Shasha Li" , "Haifang Zhou", "Jintao Tang", "Ting Wang"] ,
       "year": 2019,
       "abstract": "Automatic related work generation is a new challenge in multi-document scientific summarization focusing on refining a related work section for a given scientific paper. In this paper, we propose a brand new framework ToC-RWG for related work generation by incorporating topic model and citation information. We present an unsupervised generative probabilistic model, called QueryTopicSum, which utilizes an LDA-style model to characterize the generative process of both the scientific paper and its reference papers. We also take advantage of citations of reference papers to identify Cited Text Spans (CTS) from reference papers. This approach provides us with a perspective of annotating the importance of the reference papers from the academic community. With QueryTopicSum and the identified CTS as candidate sentences, an optimization framework based on minimizing KL divergence is exerted to select the most representative sentences for related work generation. Our evaluation results on a set of 50 scientific papers along with their corresponding reference papers show that ToC-RWG achieves a considerable improvement over generic multi-document summarization and scientific summarization baselines.",
       "introduction": "The related work section is a significant component of a scientific paper. Scholars need to contextualize their work in the related research scope and highlight their contributions in this section. A high-quality related work section requires scholars doing a survey of relevant researches by reading amounts of papers, summarizing relevant aspects of these researches, and pointing out their weaknesses compared with own work, which tends to be an arduous and time-consuming job for scholars.\n\nIn view of this, automatic related work generation is proposed to generate a related work section for a paper being written. This kind of related work section can be used as a fundamental tool to reduce scholars’ time and effort when he or she wants to complete his or her final related work section. This problem is formalized as follows: Given a target paper A without a related work section and a set of reference papers B that A cites, generate a related work section for A. An interpretation of related work generation task is shown in Figure 1.\n\nWhile a lot of extractive summarization approaches have been proposed for related work generation, few techniques have considered the relevance between the target paper A and the reference papers B. Hoang et al. [1] carried out the selection of sentences by an artificial hierarchy topic tree; Hu and Wan [2] utilized a global optimization framework for related work generation; Wang et al. [3] considered the contextual relevance within the target paper and the references among kinds of objects such as papers, authors and keywords. All those works took no consideration of explicitly modeling the relevance between the target paper and the reference papers, which we regard as a foremost component for related work generation.\n\nGenerally speaking, in the related work section, there are several facets from a reference paper in B that a target paper A could cite [4]: model and method citing, result and statistic citing, conclusion and implication citing. For each candidate reference paper in B, which content to choose for related work generation intuitively depends on its relevance to the target paper rather than the reference paper itself.\n\nRecently, Bayesian-based generative probabilistic models have shown considerable performance among multiple summarization tasks, including general multi-document summarization [5], update summarization [6] and query-focused summarization [7]. Bayesian-based models have the ability to model the text-level information by offering clear and rigorous probabilistic interpretations. To this end, we utilize the Bayesian-based model for related work generation.\n\nIn this paper, we propose a novel Bayesian model based on latent topics that link the target paper and the reference papers in a fully probabilistic and automated manner. Our model, which we call QueryTopicSum model, is a variation of Latent Dirichlet Allocation (LDA) [8]. Our model is inspired by TopicSum [5], where the author postulates each word of the document set is generated by a single topic which could be a corpus-wide background distribution over frequent words, a distribution of document-specific words or a distribution of common words across the document set. Because TopicSum is designed for general multi-document summarization, it cannot portray the relationship between the target paper and the reference papers. Hence, to capture the relationship between the target paper and the reference papers, QueryTopicSum constructs a Target-reference distribution for each reference paper, in addition to TopicSum-like background distribution and document-specific distribution.\n\nApart from a novel topic model QueryTopicSum designed to capture text characteristics, we introduce a notion called Cited Text Spans (CTS), which means the matched text spans in the reference paper that are most related to a given citation. Considering that different citations to an article often provide different aspects of that article, we can view citations as a synopsis of the article’s key points and contributions within an academic community [9]. Hence, for each reference paper of a target paper, we artificially collect some citations, and then employ CTS identification techniques we developed before [10], [11] to identify those significant sentences as candidate sentences for related work generation. Our motivation is when citing the same reference paper, former researchers provide the writer of the target paper with their insights into the reference paper via citations. And CTS is of vital importance, as they work as the only connection between the target paper and citations.\n\nTo shed light on this question, the contributions of our work are threefold, listed as follows:\n\nWe propose a novel topic model QueryTopicSum to describe the text characteristics of the target paper and reference papers. Above all, QueryTopicSum bridges the connection between the target paper and reference papers, which makes subsequent sentence selection justified.\n\nWe bring in the conception of CTS to annotate the importance of reference sentences from the viewpoint of the academic community, which has never been considered by previous research.\n\nWe conduct a thorough experimental analysis on the effectiveness of our ToC-RWG model for related work generation.\n\nThe remainder of this paper is organized as follows. Section II introduces the related work. In section III, we describe the detailed methodology for related work generation, including the framework of Toc-RWG and each component, QueryTopicSum and CTS, in detail. Section IV shows the experimental results. In section V, we make our concluding remarks.",
       "charge": false
     },
     {
       "title": "AutoCite: Multi-Modal Representation Fusion for Contextual Citation Generation",
       "author":["Qingqin Wang", "Yun Xiong", "Yao Zhang", "Jiawei Zhang", "Yangyong Zhu"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "BACO: A Background Knowledge- and Content-Based Framework for Citing Sentence Generation",
       "author":["Yubin Ge", "Ly Dinh", "Xiaofeng Liu", "Jinsong Su", "Ziyao Lu", "Ante Wang", "Jana Diesner"] ,
       "year": 2021,
       "abstract": "In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper. We present BACO, a BAckground knowledge- and COntent-based framework for citing sentence generation, which considers two types of information: (1) background knowledge by leveraging structural information from a citation network; and (2) content, which represents in-depth information about what to cite and why to cite. First, a citation network is encoded to provide background knowledge. Second, we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper. During the decoding stage, both types of information are combined to facilitate the text generation, and then we conduct a joint training for the generator and citation function classification to make the model aware of why to cite. Our experimental results show that our framework outperforms comparative baselines.",
       "introduction": "A citation systematically, strategically, and critically synthesizes content from a cited paper in the context of a citing paper (Smith, 1981). A paper’s text that refers to prior work, which we herein refer to as citing sentences, forms the conceptual basis for a research question or problem; identifies issues, contradictions, or gaps with state-of-the-art solutions; and prepares readers to understand the contributions of a citing paper, e.g., in terms of theory, methods, or findings (Elkiss et al., 2008). Writing meaningful and concise citing sentences that capture the gist of cited papers and identify connections between citing and cited papers is not trivial (White, 2004). Learning how to write up information about related work with appropriate and meaningful citations is particularly challenging for new scholars (Mansourizadeh and Ahmad, 2011).\n\nTo assist scholars with note-taking on prior work when working on a new research problem, this paper focuses on the task of citing sentence generation, which entails identifying salient information from cited papers and capturing connections between cited and citing papers. With this work, we hope to reduce scientific information overload for researchers by providing examples of concise citing sentences that address information from cited papers in the context of a new research problem and related write-up. While this task cannot and is not meant to replace the scholarly tasks of finding, reading, and synthesizing prior work, the proposed computational solution is intended to support especially new researchers in practicing the process of writing effective and focused reflections on prior work given a new context or problem.\n\nA number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing paper A, and the sentences before and after the citing sentences in paper A are considered as context. However, previous work has mainly utilized limited information from citing and cited papers to solve this task. We acknowledge that any such solution, including ours, is a simplification of the intricate process of how scholars write citing sentences.\n\nGiven this motivation, we explore two sets of information to generate citing sentences, namely background knowledge in the form of citation networks, and content from both citing and cited papers, as shown in Figure 1. Using citation networks was inspired by the fact that scholars have analyzed such networks to identify the main themes and research developments in domain areas such as information sciences (Hou et al., 2018), business modeling (Li et al., 2017), and pharmaceutical research (Chen and Guan, 2011).\n\nWe use the content of citing and cited papers as a second set of features to capture two more in-depth content features: (1) What to cite - while the overall content of a cited paper needs to be understood by the authors of the citing paper, not all content is relevant for writing citing sentences. Therefore, we follow the example of estimating salient sentences (Yasunaga et al., 2019) and use the predicted salience to filter crucial information that should be integrated into the resulting citing sentence; (2) Why to cite - we define “citation function” as an approximation of an author’s reason for citing a paper (Teufel et al., 2006). A number of previous research on citation functions has used citing sentences and their context for classification (Zhao et al., 2019; Cohan et al., 2019). Our paper involves citation functions into citing sentence generation so that the generated citing sentences can be coherent given their context, and can still contain the motivation for a specific citation.\n\nIn this paper, we propose a BAckground knowledge- and COntent-based framework, named BACO. Specifically, we encode a citation network based on citation relations among papers to obtain background knowledge, and the given citing and cited papers to provide content information. We extend a standard pointer-generator (See et al., 2017) to copy words from cited and citing papers and determine what to cite by estimating sentence salience in the cited paper. The various pieces of captured information are then combined as the context for the decoder. Furthermore, we extend our framework to include why to cite by jointly training the generation with citation function classification and facilitate the acquisition of the content information.\n\nAs for the dataset, we extended the ACL Anthology Network corpus (AAN) (Radev et al., 2013) with extracted citing sentences by using RegEx. We then hand-annotated the citation functions on a subset of the dataset and trained a citation function labeling model based on SciBERT (Beltagy et al., 2019). The resulting labeling model was then used to automatically label the rest of the data to build a large-scale dataset.\n\nWe summarize our contributions as follows:\n\nWe propose a BAckground knowledge- and COntent-based framework, named BACO, for citing sentence generation.\nWe manually annotated a subset of citing sentences with citation functions to train a SciBERT-based model to automatically label the rest of the data for citing sentence generation.\nBased on the results from experiments, we show that BACO outperforms comparative baselines by at least 2.57 points on ROUGE-2.",
       "charge": false
     }
  ]
}
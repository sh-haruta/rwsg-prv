{
  "root": {
    "title": "Subset Retrieval Nearest Neighbor Machine Translation",
    "author": [
      "Hiroyuki Deguchi",
      "Taro Watanabe",
      "Yusuke Matsui",
      "Masao Utiyama",
      "Hideki Tanaka",
      "Eiichiro Sumita"
    ],
    "abstract": "k-nearest-neighbor machine translation (kNN-MT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In this paper, we propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efficient distance computation technique that is suitable for subset neighbor search using a look-up table. Our proposed method achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 De-En translation task and the domain adaptation tasks in De-En and En-Ja.",
    "introduction": "Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Vaswani et al., 2017) has achieved state-of-the-art performance and become the focus of many studies. Recently, kNNMT (Khandelwal et al., 2021) has been proposed, which addresses the problem of performance degradation in out-of-domain data by incorporating example-search into the decoding algorithm. kNN-MT stores translation examples as a set of key–value pairs called “datastore” and retrieves k-nearest-neighbor target tokens in decoding. The method improves the translation performance of NMT models without additional training. However, decoding is seriously timeconsuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In particular, in a realistic opendomain setting, kNN-MT may be significantly slower because it needs to retrieve neighbor tokens from a large datastore that covers various domains. We propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efficient distance computation technique that is suitable for subset neighbor search using a lookup table. When retrieving neighbor sentences for a given input, we can employ arbitrary sentence representations, e.g., pre-trained neural encoders or TF-IDF vectors, to reduce the kNN search space. When retrieving target tokens in each decoding step, the search space in subset kNN-MT varies depending on the input sentence; therefore, the clustering-based search methods used in the original kNN-MT cannot be used. For this purpose, we use asymmetric distance computation (ADC) (Jégou et al., 2011) in subset neighbor search. Our subset kNN-MT achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 German-to-English general domain translation task and the domain adaptation tasks in German-to-English and English-to-Japanese with open-domain settings.",
    "relatedwork": "The first type of example-based machine translation method was analogy-based machine translation (Nagao, 1984). Zhang et al. (2018); Gu et al. (2018) incorporated example-based methods into NMT models, which retrieve examples according to edit distance. Bulte and Tezcan (2019) and Xu et al. (2020) concatenated an input sentence and translations of sentences similar to it. Both kNNMT and subset kNN-MT retrieve kNN tokens according to the distance of intermediate representations and interpolate the output probability.\nTo improve the decoding speed of kNN-MT, fast kNN-MT (Meng et al., 2022) constructs additional datastores for each source token, and reduces the kNN search space using their datastores and word alignment. Subset kNN-MT requires a sentence datastore that is smaller than source token datastores and does not require word alignment. Martins et al. (2022) decreased the number of query times by retrieving chunked text; their model led to a speed-up of up to 4 times, compared with kNN-MT. In contrast, subset kNN-MT reduces the search space. Dai et al. (2023) reduced the kNN search space by retrieving the neighbor sentences of the input sentence. They searched for neighboring sentences by BM25 scores with ElasticSearch4 , so our subset kNN-MT with BM25 can be regarded as an approximation of their method. They also proposed “adaptive lambda”, which dynamically computes the weights of the lambda of linear interpolation in Equation 2 from the distance between the query and the nearest neighbor ey vectors. However, adaptive lambda requires an exact distance and cannot employ datastore quantization and the ADC lookup. To improve the translation performance of kNN-MT, Zheng et al. (2021) computed the weighted average of kNN probabilities pkNN over multiple values of k. Each weight is predicted by “meta-k network”, trained to minimize cross-entropy in the training data. For the other tasks, kNN-LM (Khandelwal et al., 2020), Efficient kNN-LM (He et al., 2021), and RETRO (Borgeaud et al., 2022) used kNN search for language modeling (LM). Our subset search method cannot be applied to LM because the entire input cannot be obtained.\nIn the field of kNN search, Matsui et al. (2018) allowed search in dynamically created subsets, whereas conventional search methods assume only full search. Subset kNN-MT retrieves kNN tokens from a subset depending on a given input. In our subset kNN-MT, the decoding speed is slow when the subset size n is large. The bottleneck is the lookup in the distance table, and this can be improved by efficient look-up methods that uses SIMD (André et al., 2015; Matsui et al., 2022).",
    "remarks": ""},
    "leaves": [
      {
        "title": "A framework of a mechanical translation between japanese and english by analogy principle",
        "author": [
          "Makoto Nagao"
        ],
        "year": 1984,
        "abstract": "",
        "introduction": "",
        "charge": false,
        "remarks": "open access"
      },
      {
        "title": "Guiding Neural Machine Translation with Retrieved Translation Pieces",
        "author": [
          "Jingyi Zhang",
          "Masao Utiyama",
          "Eiichro Sumita",
          "Graham Neubig",
          "Satoshi Nakamura"
        ],
        "year": 2018,
        "abstract": "One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call “translation pieces”. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation.",
        "introduction": "Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sentence. While both of these paradigms have been proven effective, they both add significant complexity and computational/memory cost to the decoding process, and also to the training procedure. The first requires the running of several training iterations and rolling back of the model, which is costly at test time, and the second requires entirely changing the model structure which requires training the model separately, and also increases testtime computational cost by adding additional encoders. In this paper, we propose a simple and efficient model for using retrieved sentence pairs to guide an existing NMT model at test time. Specifically, the model collects n-grams occurring in the retrieved target sentences that also match words that overlap between the input and retrieved source sentences, which we will refer to as “translation pieces” (e.g., in Figure 1, the blue part of the retrieved target sentence is collected as translation pieces for the input sentence). The method then calculates a pseudo-probability score for each of the retrieved example sentence pairs and weights the translation pieces according to this value. Finally, we up-weight NMT outputs that contain the collected translation pieces. Unlike the previous methods, this requires no change of the underlying NMT model and no updating of the NMT parameters, making it both simple and efficient to apply at test time. We show our method improved NMT translation results up to 6 BLEU points on three translation tasks and caused little increase in the translation time. Further, we find that accuracies are comparable with the model of Gu et al. (2017), despite being significantly simpler to implement and faster at test time.",
        "charge": false,
        "remarks": ""
      },
      {
        "title": "Search engine guided neural machine translation",
        "author": [
          "Jiatao Gu",
          "Yong Wang",
          "Kyunghyun Cho",
          "Victor O.K. Li"
        ],
        "year": 2018,
        "abstract": "",
        "introduction": "",
        "charge": false
      },
      {
        "title": "Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation",
        "author": [
          "Bram Bulte",
          "Arda Tezcan"
        ],
        "year": 2019,
        "abstract": "We present a simple yet powerful data augmentation method for boosting Neural Machine Translation (NMT) performance by leveraging information retrieved from a Translation Memory (TM). We propose and test two methods for augmenting NMT training data with fuzzy TM matches. Tests on the DGT-TM data set for two language pairs show consistent and substantial improvements over a range of baseline systems. The results suggest that this method is promising for any translation environment in which a sizeable TM is available and a certain amount of repetition across translations is to be expected, especially considering its ease of implementation.",
        "introduction": "Even though Machine Translation (MT) quality may have increased considerably over the past years, most notably with advances in the field of Neural Machine Translation (NMT), Translation Memories (TMs) still offer some advantages over MT systems. They are not only able to translate previously seen sentences ‘perfectly’ but they also offer ‘near perfect’ translation quality when highly similar source sentences are retrieved from the TM. As a result, in Computer-Assisted Translation (CAT) workflows, the MT system is often used as a backoff mechanism when the TM fails to retrieve high fuzzy matches above a certain threshold (Rossi and Chevrot, 2019; Federico et al., 2012), even though it has been shown that this basic integration method is not always the most optimal TM-MT combination strategy (Simard and Isabelle, 2009). Our aim in this paper is to integrate the advantages of TMs into NMT systems in order to improve MT quality by utilizing existing translations for highly similar source sentences in a given TM. We propose a simple method for TM-NMT integration that is based on augmenting the source data with retrieved fuzzy TM targets by means of concatenation. We train both dedicated Neural Fuzzy Repair (NFR) systems that deal specifically with query sentences for which a (sufficiently high-scoring) match is found in the TM as well as unified systems capable of translating any query sentence. Several configurations are tested on the DGT-TM data set (Steinberger et al., 2013) for the language directions English into Dutch (EN→NL) and English into Hungarian (EN→HU). In the next section, we provide an overview of previous research on TM-MT integration. Section 3 details the approach proposed in this paper. The experimental setup is presented in section 4, and the results in section 5. This is followed by the discussion (section 6) and conclusion (section 7).",
        "charge": false
      },
      {
        "title": "Boosting Neural Machine Translation with Similar Translations",
        "author": [
          "Jitao Xu",
          "Josep Crego",
          "Jean Senellart"
        ],
        "year": 2020,
        "abstract": "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with “copy” information while translations based on embedding similarities tend to extend the translation “context”. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",
        "introduction": "For decades, the localization industry has been proposing Fuzzy Matching technology in CAT tools allowing the human translator to visualize one or several fuzzy matches from translation memory when translating a sentence leading to higher productivity and consistency (Yamada, 2011). Hence, even though the concept of fuzzy match scores is not standardized and differs between CAT tools (Bloodgood and Strauss, 2014), translators generally accept discounted translation rate for sentences with ”high” fuzzy matches. With improving machine translation technology and training of models on translation memories, machine translated output has been progressively introduced as a substitute for fuzzy matches when no sufficiently “good” fuzzy match is found and proved to also increase translator productivity given appropriate post-editing environment (Plitt and Masselot, 2010). These two technologies are entirely different in their finality - indeed, for a given source sentence, fuzzy matching is just a database retrieval and scoring technique always returning a pair of source and target segments, while machine translation is actually building an original translation. However, with Statistical Machine Translation, the two technologies are sharing the same simple idea about managing and retrieving optimal combination of longest translated n-grams and this property led to the development of several techniques like use of fuzzy matches in SMT decoding (Koehn and Senellart, 2010; Wang et al., 2013), adaptive machine translation (Zaretskaya et al., 2015) or “fuzzy match repairing” (Ortega et al., 2016). With Neural Machine Translation (NMT), the integration of Fuzzy Matching is less obvious since NMT does not keep nor build a database of aligned sequences and does not explicitly use n-gram language models for decoding. The only obvious and important use of translation memory is to use them to train an NMT model from scratch or to adapt a generic translation model to a specific domain (fine-tuning) (Chu and Wang, 2018). While some works propose architecture changes (Zhang et al., 2018) or decoding constraints (Gu et al., 2018); a recent work (Bulte and Tezcan ´ , 2019; Bulte et al. ´ , 2018) has proposed a simple and elegant framework where, like for human translation, translation of fuzzy matches are presented simultaneously with source sentence and the network learns to use this additional information. Even though this method has showed huge gains in quality, it also opens many questions. In this work, we are pushing the concept further a) by proposing and evaluating new integration methods, b) by extending the notion of similarity and showing that fuzzy matches can be extended to embedding-based similarities, c) by analyzing how online fuzzy matching compares and combines with offline fine-tuning. Finally, our results also show that introducing similar sentence translation is helping NMT by providing sequences to copy (copy effect), but also providing additional context for the translation (context effect).",
        "charge": false
      },
      {
        "title": "Fast Nearest Neighbor Machine Translation",
        "author": [
          "Yuxian Meng",
          "Xiaoya Li",
          "Xiayu Zheng",
          "Fei Wu",
          "Xiaofei Sun",
          "Tianwei Zhang",
          "Jiwei Li"
        ],
        "year": 2022,
        "abstract": "Though nearest neighbor Machine Translation (kNN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast kNN-MT to address this issue. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast kNN-MT is two-orders faster than kNN-MT, and is only two times slower than the standard NMT model. Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications. The code is available at https://github.com/ShannonAI/fast-knn-nmt.",
        "introduction": "Machine translation (MT) is a fundamental task in natural language processing (Brown et al., 1993; Och and Ney, 2003), and the prevalence of deep neural networks has spurred a diverse array of neural machine translation (NMT) models to improve translation quality (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). The recently proposed k nearest neighbor (kNN) MT model (Khandelwal et al., 2020) has proved to introduce significant performance boosts over standard neural MT systems. The basic idea behind kNN-MT is that at each decoding step, the model is allowed to refer to reference target tokens with similar translation contexts in a large datastore of cached examples. The corresponding reference target tokens provide important insights on the translation token likely to appear next. One notable limitation of kNN-MT is that it is prohibitively slow: it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-orders slower than vanilla MT models. The original paper of kNNMT (Khandelwal et al., 2020) suggests using fewer searching clusters, smaller beams and smaller datastores for generation speedup, but to achieve satisfactory results, carefully tuning on these factors under different tasks and datasets is still required according to analyses in (Khandelwal et al., 2020). The computational overhead introduced by kNNMT makes it hard to be deployed on real-world online services, which usually require both model performance and runtime efficiency. In this work, we propose a fast version of kNNMT – Fast kNN-MT, to tackle the aforementioned issues. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens of the same token type. Then at each decoding step, in contrast to consulting the entire corpus for nearest neighbor search, the datastore for the currently decoding token is limited within the tokens of reference targets corresponding to the previously selected reference source tokens, as shown in Figure 1. The chain of mappings from the target token to the source token, then to its nearest source reference tokens, and last to corresponding target reference tokens, can be obtained using FastAlign (Dyer et al., 2013). Fast kNN-MT provides several important advantages against vanilla kNN-MT in terms of speedup: (1) the datastore in the KNN search is limited to target tokens corresponding to previously selected reference source tokens, instead of the entire corpus. This significantly improves decoding efficiency; (2) for source nearest neighbor retrieval, we propose to restrict the reference sources tokens that are the same as the query token, which further improves nearest-neighbor search efficiency. Without loss of performance, Fast kNN-MT is two-orders faster than kNN-MT, and is only two times slower than standard MT model. Under the settings of bilingual translation and domain adaptation, Fast kNN-MT achieves comparable results to kNN-MT, leading to a SacreBLEU score of 39.3 on WMT’19 De-En, 41.7 on WMT’14 En-Fr, and an average score of 41.4 on the domain adaptation task.",
        "charge": false
      },
      {
        "title": "Chunk-based Nearest Neighbor Machine Translation",
        "author": [
          "Pedro Henrique Martins",
          "Zita Marinho",
          "André F. T. Martins"
        ],
        "year": 2022,
        "abstract": "Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and “on-the-fly” domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.",
        "introduction": "Machine translation has seen remarkable advances due to increasingly powerful neural models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). Most deployed systems are fully-parametric (the training data is fully compressed into the parameters of a neural model), but they often struggle when translating rare words or out-of-domain sentences (Koehn and Knowles, 2017), commonly requiring several stages of finetuning to adapt to data drift or to new domains. Recently, semi-parametric methods have shown great promise, by combining the strengths of parametric models with external databases of parall sentences, such as translation memories (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019a; Meng et al., 2021; Zheng et al., 2021a; Jiang et al., 2021; Martins et al., 2022). One of the most prominent semi-parametric models for machine translation is the k-Nearest Neighbor Machine Translation model (kNN-MT) (Khandelwal et al., 2021), which has led to impressive results, particularly in domain adaptation settings, without requiring fine-tuning. The kNN-MT model constructs domain-specific datastores of parallel sentences and, at inference time, it retrieves similar examples from these datastores, which are used to improve the generation process, through the interpolation of probability distributions. However, kNN-MT only retrieves single tokens—this is inefficient, since the model needs to consult the datastore at every generation step, an expensive operation. Consequently, its decoding speed is around 8 times slower than that of a fully-parametric model. Recent work has introduced several techniques to speed up kNN-MT. Meng et al. (2021) proposed Fast kNN-MT, which constructs a different datastore for each example, by first searching for the nearest neighbors of the source tokens. Wang et al. (2021) introduced Faster kNN-MT, similar to Fast kNN-MT but with reduced memory requirements. Martins et al. (2022) proposed pruning the datastore, reducing the keys’ representation size, and using a cache of retrieval distributions. However, despite leading to some decoding speedups, these methods are limited as they still retrieve a single token at each time step. In this paper, we propose a simple and efficient chunk-based kNN-MT model. Inspired by RETRO (Borgeaud et al., 2021), the chunk-based kNN-MT model retrieves chunks of tokens, instead of single tokens. But, similarly to kNN-MT and unlike RETRO, it does not require any training or finetuning of the parametric component: it simply uses a combination of caching and interpolation of probability distributions to incorporate the retrieved tokens. By doing this, the model leads to a similar translation quality while having to search for neighbors in the datastore less often. This leads to decoding speeds up to 4 times faster than the ones achieved using the vanilla kNN-MT model and only twice as slow as a fully-parametric model, but with considerably higher translation quality. In sum, our main contributions are: • We introduce a chunk-based kNN-MT model, which retrieves chunks of tokens from a datastore of examples. • We propose and compare several approaches to deal with the retrieved chunks’ tokens and to select the steps in which the model performs retrieval from the datastore. • We compare the translation quality and decoding efficiency on domain adaptation, which shows the benefits of chunk-based kNN-MT. • We propose using chunk-based kNN-MT for on-the-fly adaptation.",
        "charge": false
      },
      {
        "title": "Simple and Scalable Nearest Neighbor Machine Translation",
        "author": [
          "Yuhan Dai",
          "Zhirui Zhang",
          "Qiuzhi Liu",
          "Qu Cui",
          "Weihua Li",
          "Yichao Du",
          "Tong Xu"
        ],
        "year": 2023,
        "abstract": "",
        "introduction": "",
        "charge": false
      },
      {
        "title": "Adaptive Nearest Neighbor Machine Translation",
        "author": [
          "Xin Zheng",
          "Zhirui Zhang",
          "Junliang Guo",
          "Shujian Huang",
          "Boxing Chen",
          "Weihua Luo",
          "Jiajun Chen"
        ],
        "year": 2021,
        "abstract": "kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (NMT) model with token-level k-nearest-neighbor (kNN) retrieval to improve the translation accuracy. However, the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. In this paper, we propose Adaptive kNN-MT to dynamically determine the number of k for each target token. We achieve this by introducing a light-weight Meta-k Network, which can be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt.",
        "introduction": "Retrieval-based methods (Gu et al., 2018; Zhang et al., 2018; Bapna and Firat, 2019; Khandelwal et al., 2020a) are increasingly receiving attentions from the machine translation (MT) community recently. These approaches complement advanced neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Hassan et al., 2018) to alleviate the performance degradation when translating out-of-domain sentences (Dou et al., 2019; Wei et al., 2020), rare words (Koehn and Knowles, 2017), etc. The ability of accessing any provided datastore during translation makes them scalable, adaptable and interpretable. kNN-MT, recently proposed in (Khandelwal et al., 2020a), equips a pre-trained NMT model with a kNN classifier over a datastore of cached context representations and corresponding target tokens, providing a simple yet effective strategy to utilize cached contextual information in inference. However, the hyper-parameter k is fixed for all cases, which raises some potential problems. Intuitively, the retrieved neighbors may include noises when the target token is relatively hard to determine (e.g., relevant context is not enough in the datastore). And empirically, we find that the translation quality is very sensitive to the choice of k, results in the poor robustness and generalization performance. To tackle this problem, we propose Adaptive kNN-MT that determines the choice of k regarding each target token adaptively. Specifically, instead of utilizing a fixed k, we consider a set of possible k that are smaller than an upper bound K. Then, given the retrieval results of the current target token, we propose a light-weight Meta-k Network to estimate the importance of all possible k-Nearest Neighbor results, based on which they are aggregated to obtain the final decision of the model. In this way, our method dynamically evaluate and utilize the neighbor information conditioned on different target tokens, therefore improve the translation performance of the model. We conduct experiments on multi-domain machine translation datasets. Across four domains, our approach can achieve 1.44∼2.97 BLEU score improvements over the vanilla kNN-MT on average when K ≥ 4. The introduced light-weight Meta-k Network only requires thousands of parameters and can be easily trained with a few training samples. In addition, we find that the Meta-k Network trained on one domain can be directly applied to other domains and obtain strong performance, showing the generality and robustness of the proposed method.",
        "charge": false
      },
      {
        "title": "Generalization through Memorization: Nearest Neighbor Language Models",
        "author": [
          "Urvashi Khandelwal",
          "Omer Levy",
          "Dan Jurafsky",
          "Luke Zettlemoyer",
          "Mike Lewis"
        ],
        "year": 2020,
        "abstract": "",
        "introduction": "",
        "charge": false
      },
      {
        "title": "Efficient Nearest Neighbor Language Models",
        "author": [
          "Junxian He",
          "Graham Neubig",
          "Taylor Berg-Kirkpatrick"
        ],
        "year": 2021,
        "abstract": "Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.",
        "introduction": "Language models (LMs) are one of the most fundamental technologies in NLP, with applications spanning text generation (Bahdanau et al., 2015; Rush et al., 2015), representation learning (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019), and few-shot learning (Radford et al., 2019; Brown et al., 2020). Modern neural language models (NLMs) based on recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) or selfattentional (Vaswani et al., 2017; Al-Rfou et al., 2019) neural networks are mostly parametric, where the predictions are solely dependent on the model parameters given the input data. In contrast, recent non-parametric LMs (Guu et al., 2018; Khandelwal et al., 2019; He et al., 2020) model text distributions by referencing both the parameters of the underlying model and examples from an external datastore. Non-parametric LMs are appealing since they allow for effective language modeling – particularly for rarer patterns – through explicit memorization via a datastore, which mitigates the burden on model parameters to learn to encode all information from a large dataset. One effective and representative example is the k-nearest neighbors LM (kNN-LM, Khandelwal et al. (2019)). The kNN-LM computes the probability of the next token by interpolating a parametric LM with a distribution calculated from the k nearest context-token pairs in the datastore, as demonstrated in Figure 2. This model is particularly notable for its large improvements in performance – it outperforms the previous best parametric LMs by a large margin in standard language modeling benchmarks, in domain adaptation settings, and on other conditional generation tasks such as machine translation (Khandelwal et al., 2020). However, one downside to the kNN-LM is that the datastore stores high-dimensional dense vectors for each token in the training data; this can easily scale to hundreds of millions or even billions of records. As a result, the extra retrieval step from such datastores greatly decreases model efficiency at test time. For example, a 100M-entry datastore can lead to an over 10x slow-down compared to parametric models (§3.3) as shown in Figure 1. This issue poses a serious hurdle for the practical deployment of non-parametric LMs, despite their effectiveness. In this paper, we attempt to address this issue of test-time inefficiency and make non-parametric LMs more applicable in real-world settings. We take kNN-LM as an example, first analyzing the evaluation overhead, and raise three questions that we aim to answer in this paper: (1) Do we really need to perform retrieval on the prediction of every single token? (2) Can we identify and prune redundant records from the datastore? (3) Is it possible to further compress the datastore by reducing the vector dimensionality without losing performance? We propose and explore potential solutions for each question to aid efficiency. Specifically, we (1) show that a lightweight network can be learned to automatically prune unnecessary retrieval operations (adaptive retrieval, §4.1), (2) explore several different methods for datastore pruning based on clustering, importance-guided filtering, or greedy merging (§4.2), and (3) empirically demonstrate that simple dimension reduction techniques are able to improve both the performance and speed (§4.3). Figure 1 illustrate the overall performance of these methods. Our experiments on the WikiText-103 language modeling benchmark (Merity et al., 2017) and a training-free domain-adaptation setting demonstrate speed improvements of up to 6x with comparable perplexity to the kNN-LM. On a higher level, we expect the empirical results and analysis in the paper to help researchers better understand the speed-performance tradeoff in non-parametric NLMs, and provide a springboard for future research on more efficient non-parametric LMs.",
        "charge": false
      },
      {
        "title": "Improving language models by retrieving from trillions of tokens",
        "author": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "year": 2022,
        "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
        "introduction": "Language modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions \uD835\uDC5D(\uD835\uDC651, . . . , \uD835\uDC65\uD835\uDC5B) = Î \uD835\uDC56 \uD835\uDC5D(\uD835\uDC65\uD835\uDC56 |\uD835\uDC65<\uD835\uDC56). Neural networks have proven to be powerful language models, first in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020). The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data. In this work, we endeavor to decouple these, by exploring efficient means of augmenting language models with a massive-scale memory without significantly increasing computations. Specifically, we suggest retrieval from a large text database as a complementary path to scaling language models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions—a semi-parametric approach. At a high level, our Retrieval Transformer (Retro) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing retrieval for language modelling work only considers small transformers (100 millions parameters) and databases of limited size (up to billions of tokens) (Guu et al., 2020; Khandelwal et al., 2020; Lewis et al., 2020; Yogatama et al., 2021). To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main contributions are the following. • We introduce Retro, a retrieval-enhanced autoregressive language model (§2.2). We use a chunked cross-attention module to incorporate the retrieved text (§2.4), with time complexity linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen Bert model (§2.3) works at scale, removing the need for training and updating a retriever network. • We show that our method scales well with model size and database size (Fig. 1): Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (§4). We show that Retro can be fine-tuned to achieve competitive performance on downstream tasks such as question answering (§4.3). • We propose an evaluation aware of proximity of test documents with the training set (§2.6), addressing the problem of test set leakage (Lee et al., 2021). This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation. Using this methodology, we show that the performance of Retro comes from both explicit neighbour copying and general knowledge extraction (§4.4).",
        "charge": false
      },
      {"title": "Reconfigurable Inverted Index","author": ["Yusuke Matsui", "Ryota Hinami", "Shin'ichi Satoh"],"year": 2018,"abstract": "", "introduction": "","charge": false},
      {
        "title": "Cache locality is not enough: high-performance nearest neighbor search with product quantization fast scan",
        "author": [
          "Fabien André",
          "Anne-Marie Kermarrec",
          "Nicolas Le Scouarnec"
        ],
        "year": 2015,
        "abstract": "",
        "introduction": "",
        "charge": true
      },
      {
        "title": "ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor Search on ARM",
        "author": [
          "Yusuke Matsui",
          "Yoshiki Imaizumi",
          "Naoya Miyamoto",
          "Naoki Yoshifuji"
        ],
        "year": 2022,
        "abstract": "",
        "introduction": "",
        "charge": false
      }
    ]
}
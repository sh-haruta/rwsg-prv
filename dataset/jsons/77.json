{"root":
  {
    "title": "MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction",
    "author": ["Jing Wang", "Aixin Sun", "Hao Zhang", "Xiaoli Li"],
    "abstract": "Given a query, the task of Natural Language Video Localization (NLVL) is to localize a temporal moment in an untrimmed video that semantically matches the query. In this paper, we adopt a proposal-based solution that generates proposals (i.e., candidate moments) and then select the best matching proposal. On top of modeling the cross-modal interaction between candidate moments and the query, our proposed Moment Sampling DETR (MS-DETR) enables efficient moment-moment relation modeling. The core idea is to sample a subset of moments guided by the learnable templates with an adopted DETR (DEtection TRansformer) framework. To achieve this, we design a multi-scale visual-linguistic encoder, and an anchor-guided moment decoder paired with a set of learnable templates. Experimental results on three public datasets demonstrate the superior performance of MS-DETR.",
    "introduction": "Natural language video localization (NLVL) aims to retrieve a temporal moment from an untrimmed video that semantically corresponds to a given language query, see Fig. 1 for an example. This task is also known as temporal sentence grounding in video, and video moment retrieval. As a fundamental video-language task, it has a wide range of applications, such as video question answering (Fan et al., 2019; Yu et al., 2018; Li et al., 2019), video retrieval (Gabeur et al., 2020; Liu et al., 2019; Chen et al., 2020), and video grounded dialogue (Le et al., 2019; Kim et al., 2021).\n\nGenerally speaking, in NLVL models, a video is first split to a sequence of many small fixed-length segments. Video features are then extracted from these segments to interact with the text query. Conceptually, each video segment can be viewed as a form of “video token”. There are mainly two genres of approaches to NLVL. Proposal-free methods directly model the interaction between video tokens and text, and aim to identify start/end boundaries along the video token sequence. Proposal-based methods generate candidate moments as proposals and then select the best matching proposal2 as the answer. Each proposal is a continuous span of video tokens.\n\nTo generate proposals, some methods enumerate all possible moment candidates via pre-defined anchors. Anchors are reference start/end positions along the video. Fig. 2 shows three 2D-Map examples. Each cell in a 2D-Map corresponds to a candidate moment defined by its start/end time along the two axes. Some other methods produce moment candidates with a proposal generator guided by text query and then refine them independently. The interaction between text and video is mainly modeled between text and video moments; each moment is characterized by the video segments that compose it. Very few studies have considered moment-moment interaction. Consequently, it is challenging to discriminate among moments if there are multiple moments that all demonstrate high level of semantic matching with the text query. For instance, the two candidate moments in Fig. 1 have very similar video content and share similar semantic correspondence with the query. In this paper, we adopt the proposal-based approach for its capability of cross-modal interaction at both segment level and moment level. We propose MS-DETR to facilitate effective text-moment alignment and efficient moment-moment interaction. For text-moment alignment, we devise a multi-scale vision-language transformer backbone to conduct segment-word and segment-segment interactions at different segment scales. For moment-moment interaction, our main focus is on which moments should be sampled for interaction, due to the large number of possible pairs. Recall that a moment is a span of segments. Let O(N) be the magnitude of segment space; the magnitude of moments is O(N^2). Then moment-moment interaction has a space of O(N^4). In practice, not every pair of moments are relevant to each other, and are needed to be discriminated for a given query. Existing methods (Zhang et al., 2020b, 2021b; Wang et al., 2021a) mainly rely on a strong assumption that only the overlapping or adjacent moments are more likely to be relevant, i.e., moment locality. An example of moment locality is shown in Fig. 1, where two adjacent candidate moments share high level of visual similarity. The local interaction strategy is illustrated in Fig. 2, where the reference moment only interacts with the surrounding moments in the 2D-Map. However, not all relevant moments are overlapping or located close to each other. Following the example in Fig. 1, if the person plays saxophone again in the later part of the video (not showing for the sake of space), and the query becomes 'He plays saxophone again,' then there will be at least two highly relevant moments for playing saxophone, separated by his action of talking in between. To correctly locate the answer, the model needs to understand that 'again' refers to the second moment of playing saxophone. This calls for a better way of sampling moments for efficient moment-moment interaction, to avoid the full global interaction as shown in Fig. 2. The proposed MS-DETR samples moments for interaction using learnable templates and anchors, illustrated in the third 2D-Map in Fig. 2. We design an anchor-guided moment decoder to interact and aggregate moment features from the encoder in an adaptive and progressive manner. A fixed number of learnable templates paired with dynamic anchors are used to match the moment content and its location. Here, the templates are used to match video content in a moment, and anchors specify the reference start/end positions of the moment because multiple moments may share similar visual features. We then revise the anchors based on the predictions from the last decoder block in an iterative manner. We remark that our method has no assumption on moment locality: the moments can be scattered in diverse locations of the video. Our key contributions are threefold. First, we propose a novel multi-scale visual-linguistic encoder (Section 4.1) to align textual and video features as well as to aggregate language-enhanced semantics of video frames, in a hierarchical manner. Second, we introduce a new anchor-guided moment decoder (Section 4.2) to decode learnable templates into moment candidates, in which we propose an anchor highlight mechanism to guide the decoding. Third, we conduct extensive experiments (Section 5) on three benchmark datasets: ActivityNet Captions, TACoS, and Charades-STA. Our results demonstrate the effectiveness of the proposed MS-DETR.",
    "relatedwork": "We first briefly review existing NLVL approaches and highlight the differences between our work and other proposal-based solutions. Next, we briefly introduce object detection to provide background for the concept of learnable templates.\n\nNatural Language Video Localization.\n\n NLVL was first introduced in Hendricks et al. (2017), and since then a good number of solutions have been proposed (Zhang et al., 2022c). As aforementioned, existing methods can be largely grouped into proposal-based and proposal-free methods. Proposals, or candidate moments, can be either predefined (Gao et al., 2017; Hendricks et al., 2017) or computed by proposal generator (Xiao et al., 2021a,b; Liu et al., 2021a). Proposal-free methods output time span (Zhang et al., 2020a, 2022b, 2021a; Liu et al., 2021b) or timestamps (Yuan et al.,2019; Ghosh et al., 2019; Li et al., 2021; Zhou et al., 2021) directly on top of video tokens, without considering the notion of candidate moments. Most proposal-based methods conduct multimodal interaction between video segments and text, then encode moments from the segment features. Typically there is no further interactions among moments. 2D-TAN (Zhang et al., 2020b) is the first to demonstrate the effectiveness of moment-level interaction. However, 2D-TAN assumes moment locality and only enables local interactions among moments as shown in Fig. 2. However, similar moments requiring careful discrimination may be scattered all over the video. This motivates us to go beyond the moment locality assumption and propose moment sampling for interaction, which is a key difference and also a contribution of our work. In this paper, we adapt the concept of learnable templates from DETR framework to achieve dynamic moment sampling. DETR was originally introduced for object detection in computer vision (CV), to be briefed shortly. Most similar to our work is Xiao et al. (2021a), which also uses learnable templates. However, their work directly adopts learnable templates without any adaption to the specific requirements of NLVL. For instance, the answer moment in NLVL needs to match the given text query, whereas in object detection, there is no such requirement. We bridge the gap between NLVL and object detection by introducing a hierarchical encoder and a decoder with an anchor highlight mechanism. These designs greatly improve performance and unveil the potential of DETR for NLVL. At the same time, these designs also make our model much different from the original DETR.\n\nTransformer-based Object Detection.\n\n Object detection is a fundamental CV task. Transformer-based methods now set a new paradigm that uses learnable templates to sparsely localize objects in images. The core idea is to aggregate encoder features globally, by using (randomly initialized) learnable templates. To achieve end-to-end detection, object detection is reformulated as a set prediction problem, e.g., certain template combinations can be used to identify some specific image objects. Early solutions match predictions with ground-truth one by one using bipartite matching, leading to unstable matching and slow convergence. Recent work alleviates this issue by designing many-to-one assignment (Chen et al., 2022; Jia et al., 2022) or the self-supervision task specifically for learnable templates (Li et al., 2022; Zhang et al., 2022a). Introducing learnable templates to NLVL poses two challenges: supervision sparsity and scale mismatching. An image typically contains multiple objects and these co-occurred objects all serve as detection objects for supervision. In NLVL, given a good number of candidate moments in a video, there is only one ground-truth. We refer to this phenomenon as supervision sparsity. The scale extremity in NLVL is more severe than that in object detection. The ground truth moments in videos, analogous to objects in images, vary from 3% to 90% in terms of video length. The diverse scales bring the issue of scale mismatching when the learned templates are decoded to cover all encoder features, i.e., the entire video. Hence in MS-DETR, we adapt learnable templates mainly for the purpose of sparsely sampling moments for interaction, rather than as the main backbone."},
  "leaves":
   [
     {
       "title": "Localizing Moments in Video with Natural Language",
       "author": ["Lisa Anne Hendricks", "Oliver Wang", "Eli Shechtman", "Josef Sivic", "Trevor Darrell", "Brayan C. Russell"],
       "year": 2017,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "Temporal Sentence Grounding in Videos: A Survey and Future Directions",
       "author":["Hao Zhang", "Aixin Sun", "Wei Jing", "Joey Tianyi Zhou."],
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false },
     {
       "title": "TALL: Temporal Activity Localization via Language Query",
       "author": ["Jiyang Gao", "Chen Sun", "Zhenheng Yang", "Ram Nevatia"],
       "year": 2017,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "Natural Language Video Localization with Learnable Moment Proposals",
       "author": ["Shaoning Xiao", "Long Chen", "Jian Shao", "Yueting Zhuang","Jun Xiao"],
       "year": 2021,
       "abstract": "Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that the performance of propose-and-rank models are underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods.",
       "introduction": "Natural Language Video Localization (NLVL), aka, video grounding or video moment localization, has got unprecedented attention in both CV and NLP communities (Gao et al., 2017; Hendricks et al., 2017). As shown in Figure 1, NLVL aims to localize the video segment relevant to the query by locating the start and end timestamps in an untrimmed video. It is challenging since it needs to not only understand the video and the sentence content but also find out the precise temporal boundaries. Moreover, NLVL is helpful to numerous downstream video understanding tasks, e.g., content retrieval (Shao et al., 2018), relation detection (Gao et al., 2021), and VQA (Lei et al., 2018; Ye et al., 2017).\n\nCurrently, state-of-the-art NLVL methods can be roughly grouped into two categories according to how the video segments are detected, namely propose-and-rank and proposal-free methods:\n\nThe idea of the propose-and-rank approach (Gao et al., 2017; Hendricks et al., 2018; Liu et al., 2018b; Chen et al., 2018; Ge et al., 2019; Xu et al., 2019; Zhang et al., 2019) is intuitive, which follows the same spirits of anchor-based object detectors, e.g., Faster R-CNN (Ren et al., 2015). This kind of methods firstly defines a series of manually-designed temporal bounding boxes as moment proposals. Then, they match each candidate with the sentence in a common feature space and compute matching scores for all the candidates. Thus the localization problem is reformulated into a ranking problem. However, these methods suffer from two inherent drawbacks due to the predefined manners: 1) Even though they elaborately design a series of hyperparameters (e.g., temporal scales and sample rates), these hand-designed rules are hard to guarantee the complete coverage of targeted video segments, and consequently tend to produce inaccurate boundaries. 2) A vast number of proposals are required to achieve high recall, which causes redundant computation and degrades the results of the ranking process.\n\nAnother type of solution, proposal-free approach (Chen and Jiang, 2019; Yuan et al., 2019; Lu et al., 2019; Chen et al., 2020; Zhang et al., 2020a), mitigates these defects. Instead of predefining a series of temporal proposals, they directly predict the start and the end boundaries or regress the locations of the query-related video segments. Benefit from such design, proposal-free methods get rid of placing superfluous temporal anchors, i.e., they are more computation-efficient. Furthermore, without fixing the position and length of the moment proposals, these methods are flexible to adapt to video segments with diverse lengths. Compared to propose-and-rank methods, there are two main limitations of proposal-free methods (Xiao et al., 2021): 1) They overlook the rich information between start and end boundaries because they are hard to model the segment-level interaction. 2) They always suffer from severe imbalance between the positive and negative training samples. Up to now, almost all the propose-and-rank methods have inferior performance. We argue that the performance of the propose-and-rank methods are underestimated due to current predefined designs.\n\nIn this paper, we propose a novel propose-and-rank model with learnable moment proposals, termed LPNet. Without fixed dense proposals, only a sparse set of proposals are required to obtain decent performance. In addition, there is no need to worry about the design of hyper-parameters because it is adaptable to targeted segments with diverse positions and lengths. Obviously, as a propose-and-rank method, LPNet also avoids the defects of the proposal-free approach.\n\nSpecifically, LPNet places a fixed set of learnable temporal proposals represented by 2-d coordinates indicating the centers and lengths of video segments. These proposals are used to extract visual features of Moment of Interest (MoI). In order to model the relative relations among candidates, a module has been proposed to make the candidates interact with each other using the self-attention mechanism. Then an individual classifier is used to predict the matching score between these proposals and the sentence query. During the training process, the coordinates of the proposal with the maximum score are adjusted by a dynamic adjustor at each iteration. After sufficient iterations, these learned moment proposals will statistically represent the prior distributions of ground-truth segments on the dataset. In addition, we empirically find that the propose-and-rank models always obtain sub-optimal results without frame-level supervision. A boundary-aware predictor has been proposed to regularize the model to utilize frame-level information, which further boosts the grounding performance.\n\nWe demonstrate the effectiveness of our model on two challenging NLVL benchmarks (CharadesSTA, and ActivityNet Captions) by extensive ablative studies. Particularly, our model achieves new state-of-the-art performance over all datasets and evaluation metrics.",
       "charge": false
     },
     {
       "title": " Boundary Proposal Network for Two-stage Natural Language Video Localization.",
       "author": ["Shaoning Xiao", "Long Chen", "Songyang Zhang", "Wei Ji", "Jian Shao", "Lu Ye", "Jun Xiao"],
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge":  false
     },
     {
       "title": "Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos",
       "author": ["Daizong Liu", "Xiaoye Qu", "Jianfeng Dong", "Pan Zhou"],
       "year": 2021,
       "abstract": "We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with pre-defined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently, bottom-up framework attracts increasing attention due to its superior efficiency. It directly predicts the probabilities for each frame as a boundary. However, the performance of bottom-up model is inferior to the top-down counterpart as it fails to exploit the segment-level interaction. In this paper, we propose an Adaptive Proposal Generation Network (APGN) to maintain the segment-level interaction while speeding up the efficiency. Specifically, we first perform a foreground-background classification upon the video and regress on the foreground frames to adaptively generate proposals. In this way, the handcrafted proposal design is discarded and the redundant proposals are decreased. Then, a proposal consolidation module is further developed to enhance the semantics of the generated proposals. Finally, we locate the target moments with these generated proposals following the top-down framework. Extensive experiments show that our proposed APGN significantly outperforms previous state-of-the-art methods on three challenging benchmarks.",
       "introduction": "Temporal sentence localization in videos is an important yet challenging task in natural language processing, which has drawn increasing attention over the last few years due to its vast potential applications in information retrieval (Dong et al., 2019; Yang et al., 2020) and human-computer interaction (Singha et al., 2018). It aims to ground the most relevant video segment according to a given sentence query. As shown in Figure 1 (a), most parts of video contents are irrelevant to the query (background) while only a short segment matches it (foreground). Therefore, video and query information need to be deeply incorporated to distinguish the fine-grained details of different video segments.\n\nMost previous works (Gao et al., 2017; Chen et al., 2018; Zhang et al., 2019; Yuan et al., 2019a; Zhang et al., 2020b; Liu et al., 2021, 2020a, b) follow the top-down framework which pre-defines a large set of segment candidates (a.k.a proposals) in the video with sliding windows, and measures the similarity between the query and each candidate. The best segment is then selected according to the similarity. Although these methods achieve significant performance, they are sensitive to the proposal quality and present slow localization speed due to redundant proposals.\n\nRecently, several works (Rodriguez et al., 2020; Zhang et al., 2020a; Yuan et al., 2019b) exploit the bottom-up framework which directly predicts the probabilities of each frame as the start or end boundaries of the segment. These methods are proposal-free and much more efficient. However, they neglect the rich information between start and end boundaries without capturing the segment-level interaction. Thus, the performance of bottom-up models is behind the performance of top-down counterpart thus far.\n\nTo avoid the inherent drawbacks of proposal design in the top-down framework and maintain the localization performance, in this paper, we propose an adaptive proposal generation network (APGN) for an efficient and effective localization approach. Firstly, we perform boundary regression on the foreground frames to generate proposals, where foreground frames are obtained by a foreground-background classification on the entire video. In this way, the noisy responses on the background frames are attenuated, and the generated proposals are more adaptive and discriminative compared to the pre-defined ones.\n\nSecondly, we perform proposal ranking to select the target segment in a top-down manner upon these generative proposals. As the number of proposals is much fewer than the predefined methods, the ranking stage is more efficient. Furthermore, we additionally consider the proposal-wise relations to distinguish their fine-grained semantic details before the proposal ranking stage.\n\nTo achieve the above framework, APGN first generates query-guided video representations after encoding video and query features and then predicts the foreground frames using a binary classification module. Subsequently, a regression module is utilized to generate a proposal on each foreground frame by regressing the distances from itself to start and end segment boundaries. After that, each generated proposal contains independent coarse semantic. To capture higher-level interactions among proposals, we encode proposal-wise features by incorporating both positional and semantic information, and represent these proposals as nodes to construct a proposal graph for reasoning correlations among them. Consequently, each updated proposal obtains more fine-grained details for the following boundary refinement process.\n\nOur contributions are summarized as follows:\n• We propose an adaptive proposal generation network (APGN) for TSLV task, which adaptively generates discriminative proposals without handcrafted design, thus making localization both effective and efficient.\n• To further refine the semantics of the generated proposals, we introduce a proposal graph to consolidate proposal-wise features by reasoning their higher-order relations.\n• We conduct experiments on three challenging datasets (ActivityNet Captions, TACoS, and Charades-STA), and results show that our proposed APGN significantly outperforms the existing state-of-the-art methods.",
       "charge": false
     },
     {
       "title": "Span-based Localizing Network for Natural Language Video Localization",
       "author": ["Hao Zhang", "Aixin Sun", "Wei Jing", "Joey Tianyi Zhou"],
       "year": 2020,
       "abstract": "Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL. The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to search for matching video span within a highlighted region. Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.",
       "introduction": "Given an untrimmed video, natural language video localization (NLVL) is to retrieve or localize a temporal moment that semantically corresponds to a given language query. An example is shown in Figure 1. As an important vision-language understanding task, NLVL involves both computer vision and natural language processing techniques (Krishna et al., 2017; Hendricks et al., 2017; Gao et al., 2018; Le et al., 2019; Yu et al., 2019). Clearly, cross-modal reasoning is essential for NLVL to correctly locate the target moment from a video. Prior works primarily treat NLVL as a ranking task, which is solved by applying multimodal matching architecture to find the best matching video segment for a given language query (Gao et al., 2017; Hendricks et al., 2018; Liu et al., 2018a; Ge et al., 2019; Xu et al., 2019; Chen and Jiang, 2019; Zhang et al., 2019). Recently, some works explore to model cross-interactions between video and query, and to regress the temporal locations of the target moment directly (Yuan et al., 2019b; Lu et al., 2019a). There are also studies to formulate NLVL as a sequence decision-making problem and to solve it by reinforcement learning (Wang et al., 2019; He et al., 2019).\n\nWe address the NLVL task from a different perspective. The essence of NLVL is to search for a video moment as the answer to a given language query from an untrimmed video. By treating the video as a text passage, and the target moment as the answer span, NLVL shares significant similarities with span-based question answering (QA) tasks. The span-based QA framework (Seo et al., 2017; Wang et al., 2017; Huang et al., 2018) can be adopted for NLVL. Hence, we attempt to solve this task with a multimodal span-based QA approach.\n\nThere are two main differences between traditional text span-based QA and NLVL tasks. First, video is continuous, and causal relations between video events are usually adjacent. Natural language, on the other hand, is inconsecutive, and words in a sentence demonstrate syntactic structure. For instance, changes between adjacent video frames are usually very small, while adjacent word tokens may carry distinctive meanings. As a result, many events in a video are directly correlated and can even cause one another (Krishna et al., 2017). Causalities between word spans or sentences are usually indirect and can be far apart. Second, compared to word spans in text, humans are insensitive to small shifting between video frames. In other words, small offsets between video frames do not affect the understanding of video content, but the differences of a few words or even one word could change the meaning of a sentence.\n\nAs a baseline, we first solve the NLVL task with a standard span-based QA framework named VSLBase. Specifically, visual features are analogous to that of a text passage; the target moment is regarded as the answer span. VSLBase is trained to predict the start and end boundaries of the answer span. Note that VSLBase does not address the two aforementioned major differences between video and natural language. To this end, we propose an improved version named VSLNet (Video Span Localizing Network). VSLNet introduces a Query-Guided Highlighting (QGH) strategy in addition to VSLBase. Here, we regard the target moment and its adjacent contexts as foreground, while the rest as background, i.e., the foreground covers a slightly longer span than the answer span. With QGH, VSLNet is guided to search for the target moment within a highlighted region. Through region highlighting, VSLNet well addresses the two differences. First, the longer region provides additional contexts for locating the answer span due to the continuous nature of video content. Second, the highlighted region helps the network to focus on subtle differences between video frames because the search space is reduced compared to the full video. Experimental results on three benchmark datasets show that adopting a span-based QA framework is suitable for NLVL. With a simple network architecture, VSLBase delivers comparable performance to strong baselines. In addition, VSLNet further boosts the performance and achieves the best among all evaluated methods.",
       "charge": false
     },
     {
       "title": "Natural Language Video Localization: A Revisit in Span-Based Question Answering Framework",
       "author": ["Hao Zhang", "Aixin Sun", "Wei Jing", "Liangli Zhen","Joey Tianyi Zhou", "Rick Siow Mong Goh"],
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "Parallel Attention Network with Sequence Matching for Video Grounding",
       "author": ["Hao Zhang", "Aixin Sun", "Wei Jing", "Liangli Zhen","Joey Tianyi Zhou", "Rick Siow Mong Goh"],
       "year": 2021,
       "abstract": "Given a video, video grounding aims to retrieve a temporal moment that semantically corresponds to a language query. In this work, we propose a Parallel Attention Network with Sequence matching (SeqPAN) to address the challenges in this task: multi-modal representation learning, and target moment boundary prediction. We design a self-guided parallel attention module to effectively capture self-modal contexts and cross-modal attentive information between video and text. Inspired by sequence labeling tasks in natural language processing, we split the ground truth moment into begin, inside, and end regions. We then propose a sequence matching strategy to guide start/end boundary predictions using region labels. Experimental results on three datasets show that SeqPAN is superior to state-of-the-art methods. Furthermore, the effectiveness of the self-guided parallel attention module and the sequence matching module is verified.",
       "introduction": "Video grounding is a fundamental and challenging problem in vision-language understanding research area (Hu et al., 2019; Yu et al., 2019; Zhu and Yang, 2020). It aims to retrieve a temporal video moment that semantically corresponds to a given language query, as shown in Figure 1. This task requires techniques from both computer vision (Tran et al., 2015; Shou et al., 2016; Feichtenhofer et al., 2019), natural language processing (Yu et al., 2018; Yang et al., 2019), and more importantly, the cross-modal interactions between the two. Many existing solutions (Chen et al., 2018; Liu et al., 2018a; Xu et al., 2019) tackle video grounding problem with proposal-based approach. This approach generates proposals with pre-set sliding windows or anchors, computes the similarity between the query and each proposal. The proposal with the highest score is selected as the answer. These methods are sensitive to the quality of proposals and are inefficient because all proposal-query pairs are compared. Recently, several one-stage proposal-free solutions (Chen et al., 2019; Lu et al., 2019a; Mun et al., 2020) are proposed to directly predict start/end boundaries of target moments, through modeling video-text interactions. Our solution, SeqPAN, is a proposal-free method; hence our key focuses are video-text interaction modeling and moment boundary prediction.\n\nVideo-text interaction modeling. In order to model video-text interaction, various attention-based methods have been proposed (Gao et al., 2017; Yuan et al., 2019a; Mun et al., 2020). In particular, the transformer block (Vaswani et al., 2017) is widely used in vision-language tasks and proved to be effective for multimodal learning (Tan and Bansal, 2019; Lu et al., 2019b; Su et al., 2020; Li et al., 2020). In the video grounding task, fine-grain scale unimodal representations are important to achieve good localization performance. However, existing solutions do not refine unimodal representations of video and text when doing cross-modal reasoning, and thus limit the performance.\n\nTo better capture informative features for multimodalities, we encode both self-attentive contexts and cross-modal interactions from video and query. That is, instead of solely relying on sophisticated cross-modal learning as in most existing studies, we learn both intra- and inter-modal representations simultaneously, with improved attention modules.\n\nMoment boundary prediction. In terms of the length, the target moment is usually a very small portion of the video, making positive (frames in the target moment) and negative (frames not in the target moment) samples imbalanced. Further, we aim to predict the exact start/end boundaries (i.e., two video frames) of the target moment. If we view from the space of video frames, sparsity is a major concern, e.g., catching two frames among thousands. Recent studies attempt to address this issue by auxiliary objectives, e.g., to discriminate whether each frame is foreground (positive) or background (negative) (Yuan et al., 2019b; Mun et al., 2020), or to regress distances of each frame within the target moment to ground truth boundaries (Lu et al., 2019a; Zeng et al., 2020). However, the \"sequence\" nature of frames or videos is not considered.\n\nWe emphasize the \"sequence\" nature of video frames and adopt the concept of sequence labeling in NLP for video grounding. We use named entity recognition (NER) (Lample et al., 2016; Ma and Hovy, 2016) as an example sequence labeling task for illustration in Figure 2. Video grounding is to retrieve a sequence of frames with start/end boundaries of the target moment from the video. This is analogous to extracting a multi-word named entity from a sentence. The main difference is that words are discrete, so word annotations (i.e., B, I, E, and O tags) in a sentence are discrete. In contrast, video is continuous, and the changes between consecutive frames are smooth. Hence, it is difficult (and also not necessary) to precisely annotate each frame. We relax the annotations on the video sequence by specifying video regions, instead of frames. With respect to the target moment, we label B, I, E, and O (BIEO) regions on video (see Figure 3) and introduce label embeddings to model these regions.\n\nOur contributions. In this research, we propose a Parallel Attention Network with Sequence Matching (SeqPAN) for the video grounding task. We first design a self-guided parallel attention (SGPA) module to capture both self- and cross-modal attentive information for each modality simultaneously. In the SGPA module, a cross-gating strategy with self-guided head is further used to fuse self- and cross-modal representations. We then propose a sequence matching (sq-match) strategy to identify BIEO regions in the video. The label embeddings are incorporated to represent the label of frames in each region for region recognition. The sq-match guides SeqPAN to search for boundaries of the target moment within constrained regions, leading to more precise localization results. Experimental results on three benchmarks demonstrate that both SGPA and sq-match consistently improve the performance, and SeqPAN surpasses the state-of-the-art methods.",
       "charge": false
     },
     {
       "title": "Context-aware Biaffine Localizing Network for Temporal Sentence Grounding",
       "author": ["Daizong Liu", "Xiaoye Qu", "Jianfeng Dong", "Pan Zhou", "Yu Cheng", "Wei Wei", "Zichuan Xu", "Yulai Xie"],
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression",
       "author": ["Yitian Yuan","Tao Mei", "Wenwu Zhu"],
       "year": 2019,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "ExCL: Extractive Clip Localization Using Natural Language Descriptions",
       "author": ["Soham Ghosh", "Anuva Agarwal", "Zarana Parekh", "Alexander Hauptmann"],
       "year": 2019,
       "abstract": "The task of retrieving clips within videos based on a given natural language query requires cross-modal reasoning over multiple frames. Prior approaches such as sliding window classifiers are inefficient, while text-clip similarity driven ranking-based approaches such as segment proposal networks are far more complicated. In order to select the most relevant video clip corresponding to the given text description, we propose a novel extractive approach that predicts the start and end frames by leveraging cross-modal interactions between the text and video - this removes the need to retrieve and re-rank multiple proposal segments. Using recurrent networks we encode the two modalities into a joint representation which is then used in different variants of start-end frame predictor networks. Through extensive experimentation and ablative analysis, we demonstrate that our simple and elegant approach significantly outperforms state of the art on two datasets and has comparable performance on a third.",
       "introduction": "Clip Localization is the task of selecting the relevant span of temporal frames in a video corresponding to a natural language description and has recently piqued interest in research that lies at the intersection of visual and textual modalities. An example of this task is demonstrated in Figure 1. It requires cross-modal reasoning to ground freeform text inside the video and calls for models capable of segmenting a video into action segments (Singh et al., 2016; Yeung et al., 2016; Xu et al., 2017) as well as measuring multi-modal semantic similarity (Karpathy and Fei-Fei, 2015).\n\nThis task is inherently discriminative, i.e., there is only a single most relevant clip pertaining to a given query in the corresponding video. However, most prior works (Hendricks et al., 2017, 2018; Liu et al., 2018; Chen et al., 2018; Zhang et al., 2018) explore this as a ranking task over a fixed number of moments by uniformly sampling clips within a video. Moreover, these approaches are restrictive in scope since they use predefined clips as candidates for a video and cannot be easily extended to videos with considerable variance in length.\n\nGao et al. (2017); Xu et al. (2019) apply two-stage methods that rank candidate clips using a learned similarity metric. Gao et al. (2017); Ge et al. (2019) propose a sliding window approach with alignment and offset regression learning objective, but it is limited by the coarseness of the windows and is thus inefficient and inflexible. Xu et al. (2019) address this through a query-guided segment proposal network (QSPN). However, the similarity metric used by these approaches is difficult to learn as it is sensitive to the choice of negative samples (Yu et al., 2018) and it still does not consider the discriminative nature of the task.\n\nHence, we propose an elegant and fairly simple extractive approach. Our technique is similar to text-based Machine Comprehension (Chen et al., 2017) but in a multimodal setting where the video is analogous to the text passage and the target clip is analogous to the text span corresponding to the correct answer. We verify empirically that our method significantly outperforms prior work on two benchmark datasets - TACoS, ActivityNet and comparably well on the third, Charades-STA. Our flexible, modular approach to Extractive Clip Localization (ExCL) can easily be extended to incorporate attention models and different variants of encoders for both visual and text modality to improve performance further.",
       "charge":  false
     },
     {
       "title": "Proposal-Free Video Grounding with Contextual Pyramid Network",
       "author": ["Kun Li", "Dan Guo", "Meng Wang"],
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge":  false
     },
     {
       "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal Grounding",
       "author": ["Hao Zhou", "Chongyang Zhang", "Yan Luo", "Yanjun Chen", "Chuanping Hu"],
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language",
       "author": ["Songyang Zhang", "Houwen Peng", "Jianlong Fu", "Jiebo"],
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment",
       "author": ["Qiang Chen", "Xiaokang Chen", "Jian Wang", "Shan Zhang", "Kun Yao", "Haocheng Feng", "Junyu Han", "Errui Ding", "Gang Zeng", "Jingdong Wang"],
       "year": 2022,
       "abstract": "Detection transformer (DETR) relies on one-to-one assignment, assigning one ground-truth object to one prediction, for end-to-end detection without NMS post-processing. It is known that one-to-many assignment, assigning one ground-truth object to multiple predictions, succeeds in detection methods such as Faster R-CNN and FCOS. While the naive one-to-many assignment does not work for DETR, and it remains challenging to apply one-to-many assignment for DETR training. In this paper, we introduce Group DETR, a simple yet efficient DETR training approach that introduces a group-wise way for one-to-many assignment. This approach involves using multiple groups of object queries, conducting one-to-one assignment within each group, and performing decoder self-attention separately. It resembles data augmentation with automatically-learned object query augmentation. It is also equivalent to simultaneously training parameter-sharing networks of the same architecture, introducing more supervision and thus improving DETR training. The inference process is the same as DETR trained normally and only needs one group of queries without any architecture modification. Group DETR is versatile and is applicable to various DETR variants. The experiments show that Group DETR significantly speeds up the training convergence and improves the performance of various DETR-based models. Code will be available at https://github.com/Atten4Vis/GroupDETR",
       "introduction": "Detection Transformer (DETR) [4] conducts end-to-end object detection without the need for many hand-crafted components, such as non-maximum suppression (NMS) [23] and anchor generation [44, 33, 43]. The architecture consists of a CNN [22] and transformer encoder [53], and a transformer decoder that consists of self-attention, cross-attention, and FFNs, followed by class and box prediction FFNs. During training, one-to-one assignment, where one ground-truth object is assigned to one single prediction, is applied for learning to only promote the predictions assigned to ground-truth objects and demote duplicate predictions.\n\nThis work explores the solutions to accelerate the DETR training process. Previous solutions contain two main lines. The one line is to modify cross-attention so that informative image regions are selected for effectively and efficiently collecting the information from image features. Example methods include sparse sampling, through deformable attention [70], and spatial modulations with modifying object queries [16, 41, 8, 57, 61, 36, 17]. The other line is to stabilize one-to-one assignment during training, e.g., feeding ground-truth bounding boxes with noises into the transformer decoder [29, 65].\n\nWe are interested in the second line. Instead of focusing on stabilizing the assignment like DN-DETR [29], we study the assignment scheme for efficient DETR training from a new perspective: introducing more supervision. It has been proven that assigning one ground-truth object to multiple predictions, i.e., one-to-many assignment, is successful in traditional object detection methods, e.g., Faster R-CNN [44] and FCOS [52] with more anchors and pixels assigned to one ground-truth object. Unfortunately, naive one-to-many assignment does not work for DETR training. It remains a challenge to apply one-to-many assignment to DETR training.\n\nWe present a simple yet efficient DETR training approach that uses a group-wise way for one-to-many assignments, called Group DETR. Our approach is based on that end-to-end detection with successful removal of NMS post-processing for DETR comes from the joint effect of two components [4, 41]: decoder self-attention, which collects the information of other predictions, and one-to-one assignment, which expects to learn to score one prediction higher and other duplicate predictions lower for one ground-truth object.\n\nOur approach adopts K groups of object queries and introduces group-wise one-to-many assignments. This assignment scheme conducts one-to-one assignment within each group of object queries, resulting in one ground-truth object being assigned to multiple predictions. It is encouraged that the prediction assigned to the ground-truth object gets a high score, and other duplicate predictions from the same group of queries get low scores. In other words, the predictions make competition within each group. Thus, our approach uses separate self-attention, i.e., self-attention is done for each group separately, eliminating the influence of predictions from other groups and easing DETR training.\n\nRegarding inference, it is the same as DETR trained normally and only needs a single group of object queries. The resulting architecture is equivalent to DETR with a group of parallel decoders, illustrated in Figure 2 (a). During training, the parallel decoders boost each other through sharing decoder parameters and using different object queries. On the other hand, using more groups of object queries resembles data augmentation and behaves as query augmentation. It introduces more supervision and improves the decoder training. In addition, it is empirically observed that the encoder training is also improved, presumably with the help of the improved decoder.\n\nGroup DETR is versatile and is applicable to various DETR variants. Extensive experiments demonstrate that our approach is effective in achieving fast training convergence, shown in Figure 1. Group DETR obtains consistent improvements on various DETR-based methods [41, 36, 29, 65]. For instance, Group DETR significantly improves Conditional DETR-C5 by 5.0 mAP with 12-epoch training on COCO [34]. The non-trivial improvements hold when we adopt longer training schedules (e.g., 36 epochs and 50 epochs). Furthermore, Group DETR outperforms baseline methods for multi-view 3D object detection [37, 38] and instance segmentation [9].",
       "charge":  false
     },
     {
       "title": "DETRs with Hybrid Matching",
       "author": ["Ding Jia", "Yuhui Yuan", "Haodi He", "Xiaopei Wu", "Haojun Yu", "Weihong Lin", "Lei Sun", "Chao Zhang", "Han Hu"],
       "year": 2022,
       "abstract": "One-to-one set matching is a key design for DETR to establish its end-to-end capability, so that object detection does not require a hand-crafted NMS (non-maximum suppression) to remove duplicate detections. This end-to-end signature is important for the versatility of DETR, and it has been generalized to broader vision tasks. However, we note that there are few queries assigned as positive samples and the one-to-one set matching significantly reduces the training efficacy of positive samples. We propose a simple yet effective method based on a hybrid matching scheme that combines the original one-to-one matching branch with an auxiliary one-to-many matching branch during training. Our hybrid strategy has been shown to significantly improve accuracy. In inference, only the original one-to-one match branch is used, thus maintaining the end-to-end merit and the same inference efficiency of DETR. The method is named H-DETR, and it shows that a wide range of representative DETR methods can be consistently improved across a wide range of visual tasks, including DeformableDETR, PETRv2, PETR, and TransTrack, among others. The code is available at https://github.com/HDETR ",
       "introduction": "Since the success of the pioneering work DEtection TRansformer (DETR) [5] on object detection tasks, DETR-based approaches have achieved significant progress on various fundamental vision recognition tasks such as object detection [50, 58, 82, 89], instance segmentation [13, 14, 25, 78], panoptic segmentation [9, 31, 64, 75, 79], referring expression segmentation [69, 74], video instance segmentation [8, 65, 70], pose estimation [26, 59, 60], multi-object tracking [7, 49, 61], monocular depth estimation [18, 29], text detection & layout analysis [46, 55, 56, 85], line segment detection [71], 3D object detection based on point clouds or multi-view images [1, 30, 52, 67], visual question answering [22, 47], and so on.\n\nMany follow-up efforts have improved DETR from various aspects, including redesigning more advanced transformer encoder [15, 89] or transformer decoder architectures [4, 16, 50, 81, 89] or query formulations [24, 37, 68, 82].\n\nDifferent from most of these previous efforts, we focus on the training efficacy issues caused by one-to-one matching, which only assigns one query to each ground truth. For example, Deformable-DETR typically only selects fewer than 30 queries from a pool of 300 queries to match with the ground truth for each image, as nearly 99% of the COCO images consist of fewer than 30 bounding boxes annotations, while the remaining more than 270 queries will be assigned as ∅ and are supervised with only classification loss, thus suffering from very limited localization supervision.\n\nTo overcome the drawbacks of one-to-one matching and unleash the benefits of exploring more positive queries, we present a very simple yet effective hybrid matching scheme, which introduces an additional one-to-many matching branch that assigns multiple queries to each positive sample. In inference, we only use the original one-to-one decoder branch supervised with the one-to-one matching loss. We find that this simple approach can substantially improve the training efficacy, especially regarding the fitting of positive queries. Since only the original one-to-one matching branch is used in inference, the merits of the original DETR framework are almost all maintained, for example, avoiding NMS. Our approach also has no additional computation overhead compared to the original version.\n\nWe dub the hybrid matching approach as H-DETR, and extensively verify its effectiveness using a variety of vision tasks that adopt DETR methods or the variants, as well as different model sizes ranging from ResNet-50/Swin-T to Swin-L. The visual tasks and the corresponding DETR-based approaches include Deformable-DETR [89] for image object detection, PETRv2 [40] for 3D object detection from multi-view images, PETR [59] for multi-person pose estimation, and TransTrack [61] for multi-object tracking.\n\nThe H-DETR achieves consistent gains over all of them, as shown in Figure 1. Specifically, our approach can improve the Deformable DETR framework (R50) on COCO object detection by +1.7% mAP (48.7% v.s. 47.0%), the PETR framework (R50) on COCO pose estimation by +1.6% mAP (70.9% v.s. 69.3%). In particular, we achieve 59.4% mAP on COCO object detection, which is the highest accuracy on COCO object detection among DETR-based methods that use the Swin-L model. We achieve 52.38% on nuScenes val, which is +1.7% higher than a very recent state-of-the-art approach of PETRv2.",
       "charge": false
     },
     {
       "title": "DN-DETR: Accelerate DETR Training by Introducing Query DeNoising",
       "author": ["Feng Li", "Hao Zhang", "Shilong Liu", "Jian Guo", "Lionel M. Ni", "Lei Zhang"],
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge":  true
     },
     {
       "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
       "author": ["Hao Zhang", "Feng Li", "Shilong Liu", "Lei Zhang", "Hang Su", "Jun Zhu", "Lionel M. Ni", "Heung-Yeung Shum"],
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false
     }
   ]
}
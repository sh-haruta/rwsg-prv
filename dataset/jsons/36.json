{"root":
  {
    "title": "When Does Translation Require Context? A Data-driven, Multilingual Exploration",
    "author": ["Patrick Fernandes", "Kayo Yin", "Emmy Liu", "André Martins", "Graham Neubig"],
    "abstract": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at https://github.com/neulab/contextual-mt",
    "introduction": "In order to properly translate discourse phenomena including anaphoric pronouns, lexical cohesion, and discourse markers, a machine translation (MT) model must use information from previous utterances (Guillou et al., 2018; Läubli et al., 2018; Toral et al., 2018). However, while generating proper translations of these phenomena is important for comprehension, they represent a small portion of words in natural language. Therefore, common metrics such as BLEU (Papineni et al., 2002) cannot be used to judge the quality of discourse translation.\n\nRecent work on neural machine translation (NMT) models that attempt to incorporate extrasentential context (Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Maruf and Haffari, 2018, inter alia) often perform targeted evaluation of certain discourse phenomena, mostly focusing on ellipsis, formality (Voita et al., 2019b,a), and pronoun translation (Müller et al., 2018; Bawden et al., 2018; Lopes et al., 2020). However, only a limited set of discourse phenomena for a few language pairs have been studied (see summary in Table 1). The difficulty of broadening these studies stems from the reliance of previous work on introspection and domain knowledge to identify the relevant discourse phenomena, frequently involving expert speakers, which then requires engineering complex language-specific methods to create test suites or manually designing data for evaluation.\n\nIn this paper, we identify sentences that contain discourse phenomena through a data-driven, semi-automatic methodology. We apply this method to create a multilingual benchmark testing discourse phenomena in the domain of MT. First, we develop P-CXMI (§2) as a metric to identify when context is helpful in MT, or more broadly text generation in general. Then, we perform a systematic analysis of words with high P-CXMI to find categories of translations where context is useful (§3). We identify novel discourse phenomena that to our knowledge have not been addressed previously (e.g. consistency of verb forms), without requiring a-priori language-specific knowledge. Finally, we design a series of methods to automatically tag words belonging to the identified classes of ambiguities (§4) and we evaluate existing translation models for different categories of ambiguous translations (§5).\n\nWe examine a parallel corpus spanning 14 language pairs, measuring translation ambiguity and model performance. We find that the context-aware methods, while improving on standard evaluation metrics, only perform significantly better than context-agnostic baselines for certain discourse phenomena in our benchmark. Our benchmark provides a more fine-grained evaluation of translation models and reveals weaknesses of context-aware models, such as verb form cohesion. We also find that DeepL, a commercial document-level translation system, does better in our benchmark than its sentence-level ablation and Google Translate. We hope that the released benchmark and code, as well as our findings, will spur targeted evaluation of discourse phenomena in MT to cover more languages and more phenomena in the future.",
    "relatedwork": "Several works have worked on measuring the performance of MT models on contextual discourse phenomena. The first example of this was done by Hardmeier et al. (2010), which evaluated automatically the precision and recall of pronoun translation in statistical MT systems. Jwalapuram et al. (2019) proposed evaluating models on pronoun translation based on a pairwise comparison between translations that were generated with and without context, and later Jwalapuram et al. (2020) extended this work to include more languages and phenomena in their automatic evaluation/test set creation. These works rely on prior domain knowledge and intuition to identify context-aware phenomena, whereas we take a systematic, data-driven approach.\n\nMost works have focused on evaluating performance in discourse phenomena through the use of contrastive datasets. Müller et al. (2018) automatically create a dataset for anaphoric pronoun resolution to evaluate MT models in EN → DE. Bawden et al. (2018) manually creates a dataset for both pronoun resolution and lexical choice in EN → FR. Voita et al. (2018, 2019b) creates a dataset for anaphora resolution, deixis, ellipsis and lexical cohesion in EN → RU. However, Yin et al. (2021) suggest that translating and disambiguating between two contrastive choices are inherently different, motivating our approach in measuring direct translation performance."},
  "leaves":
   [
     {
       "title": "Modelling pronominal anaphora in statistical machine translation",
       "author": ["Christian Hardmeier", "Marcello Fondazione", "Bruno Kessler"],
       "year": 2010,
       "abstract": "",
       "introduction": "",
       "charge":  false
     },
     {
       "title": "Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite",
       "author":["Prathyusha Jwalapuram", "Shafiq Joty", "Irina Temnikova", "Preslav Nakov"] ,
       "year": 2019,
       "abstract": "The ongoing neural revolution in machine translation has made it easier to model larger contexts beyond the sentence-level, which can potentially help resolve some discourse-level ambiguities such as pronominal anaphora, thus enabling better translations. Unfortunately, even when the resulting improvements are seen as substantial by humans, they remain virtually unnoticed by traditional automatic evaluation measures like BLEU, as only a few words end up being affected. Thus, specialized evaluation measures are needed. With this aim in mind, we contribute an extensive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. We also conduct a user study to report correlations with human judgments.",
       "introduction": "Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework.\n\nThen, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few.\n\nUnfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations of BLEU are well-known and have been discussed in detail in a recent study (Reiter, 2018). It has long been argued that as the quality of machine translation improves, there will be a singularity moment when existing evaluation measures would be unable to tell whether a given output was produced by a human or by a machine. Indeed, there have been recent claims that human parity has already been achieved (Hassan et al., 2018), but it has also been shown that it is easy to tell apart a human translation from a machine output when going beyond the sentence level (Laubli ¨ et al., 2018). Overall, it is clear that there is a need for machine translation evaluation measures that look beyond the sentence level, and thus can better appreciate the improvements that a discourse-aware MT system could potentially bring.\n\nAlternatively, one could use diagnostic test sets that are designed to evaluate how an MT system handles specific discourse phenomena (Bawden et al., 2018; Rios et al., 2018). There have also been proposals to use semi-automatic measures and test suites (Guillou and Hardmeier, 2018). Here we propose a targeted dataset for machine translation evaluation with a focus on anaphora. We further present a specialized evaluation measure trained on this dataset. The measure performs pairwise evaluations: it learns to distinguish good vs. bad translations of pronouns, without being given specific signals of the errors. It has been argued that pairwise evaluation is useful and sufficient for machine translation evaluation (Guzman´ et al., 2015, 2017). In particular, Duh (2008) has shown that ranking-based evaluation measures can achieve higher correlations with human judgments, as ranking judgments are easier to obtain from human judges and are also easy to use in training, while also directly achieving the purpose of comparing two systems.\n\nNote that while it may be possible to rank translations using strong pre-trained conditional language models such as GPT (Radford et al., 2018), all kinds of errors would influence the score, and it would not be targeted towards a specific source of error, such as anaphora here. Our model provides a way to do this, and we demonstrate that it indeed focuses on the translation of pronouns. Although our pronoun test suite naturally consists of the source text paired with a reference human translation, our pronoun evaluation measure is generally independent of the source language. Moreover, we use real machine translation output, which may contain various types of errors.\n\nOur contributions are as follows:\n\nWe create a dataset for pronoun translation that covers multiple source languages and various target English pronouns.\nWe propose a novel evaluation measure that differentiates good pronoun translations from bad ones irrespective of the source language they were translated from.\nUnlike previous work, both the dataset and the model are based on actual system outputs.\nOur evaluation measure achieves high agreement with human judgments.\nWe make both the dataset and the evaluation measure publicly available at https://ntunlpsg.github.io/project/discomt/evalanaphora/.",
       "charge": false
     },
     {
       "title": "Can Your Context-Aware MT System Pass the DiP Benchmark Tests? : Evaluation Benchmarks for Discourse Phenomena in Machine Translation",
       "author":["Prathyusha Jwalapuram", "Barbara Rychalska", "Shafiq Joty", "Dominika Basaj"] ,
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": false,
       "remarks": "Introduction & Related Work"
     },
     {
       "title": "A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation",
       "author":["Mathias Müller", "Annette Rios", "Elena Voita", "Rico Sennrich"] ,
       "year": 2018,
       "abstract": "The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence. Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU. However, metrics that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.",
       "introduction": "Even though machine translation has improved considerably with the advent of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015), the translation of pronouns remains a major issue. They are notoriously hard to translate since they often require context outside the current sentence.\n\nAs an example, consider the sentences in Figure 1. In both languages, there is a pronoun in the second sentence that refers to the European Central Bank. When the second sentence is translated from English to German, the translation of the pronoun \"it\" is ambiguous. This ambiguity can only be resolved with context awareness: if a translation system has access to the previous English sentence, the previous German translation, or both, it can determine the antecedent the pronoun refers to. In this German sentence, the antecedent \"Europäische Zentralbank\" dictates the feminine gender of the pronoun \"sie.\"\n\nIt is unfortunate, then, that current NMT systems generally operate on the sentence level (Vaswani et al., 2017; Gehring et al., 2017; Hieber et al., 2017). Documents are translated sentence-by-sentence for practical reasons, such as line-based processing in a pipeline and reduced computational complexity. Furthermore, improvements of larger-context models over baselines in terms of document-level metrics such as BLEU or RIBES have been moderate, so that their computational overhead does not seem justified, and so that it is hard to develop more effective context-aware architectures and empirically validate them.\n\nTo address this issue, we present an alternative way of evaluating larger-context models on a test set that allows measuring a model’s capability to correctly translate pronouns. The test suite consists of pairs of source and target sentences, in combination with contrastive translation variants (for evaluation by model scoring) and additional linguistic and contextual information (for further analysis). The resource is freely available. Additionally, we evaluate several context-aware models that have recently been proposed in the literature on this test set and extend existing models with parameter tying.\n\nThe main contributions of our paper are:\n\nWe present a large-scale test set to evaluate the accuracy with which NMT models translate the English pronoun \"it\" to its German counterparts \"es,\" \"sie,\" and \"er.\"\nWe evaluate several context-aware systems and show how targeted, contrastive evaluation is an effective tool to measure improvement in pronoun translation.\nWe empirically demonstrate the effectiveness of parameter tying in multi-encoder context-aware models.\nSection 2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation. Section 3 describes our test suite. The context-aware models we use in our experiments are detailed in Section 4. We discuss our experiments in Section 5 and the results in Section 6.",
       "charge": false
     },
     {
       "title": "Evaluating Discourse Phenomena in Neural Machine Translation",
       "author":["Rachel Bawden", "Rico Sennrich", "Alexandra Birch", "Barry Haddow"] ,
       "year": 2018,
       "abstract": "For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models’ ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5% for coreference and 57% for coherence/cohesion), highlighting the importance of target-side context.",
       "introduction": "Machine translation (MT) systems typically translate sentences independently of each other. However, certain textual elements cannot be correctly translated without linguistic context, which may appear outside the current sentence. The most obvious examples of context-dependent phenomena problematic for MT are coreference (Guillou, 2016), lexical cohesion (Carpuat, 2009), and lexical disambiguation (Rios Gonzales et al., 2017), an example for each of which is given in (1-3). In each case, the English element in italic is ambiguous in terms of its French translation. The correct translation choice (in bold) is determined by linguistic context (underlined), which can be outside the current sentence. This disambiguating context can be source or target-side; the correct translation of anaphoric pronouns \"it\" and \"they\" depends on the gender of the translated antecedent (1). In lexical cohesion, a translation may depend on target factors, but may also be triggered by source effects and linguistic mechanisms such as repetition or alignment (2). In lexical disambiguation, source or target information may provide the appropriate context (3).\n\n(1) The bee is busy. // It is making honey.\nL’abeille[f] est occupée. // Elle[f]/#il[m] fait du miel.\n\n(2) Do you fancy some soup? // Some soup?\nTu veux de la soupe? // De la soupe/#du potage?\n\n(3) And the code? // Still some bugs...\nEt le code? // Encore quelques bugs/#insectes...\n\nRecent work on multi-encoder neural machine translation (NMT) appears promising for the integration of linguistic context (Zoph and Knight, 2016; Libovicky and Helcl ´ , 2017; Jean et al., 2017a; Wang et al., 2017). However, models have almost only been evaluated using standard automatic metrics, which are poorly adapted to evaluating discourse phenomena. Targeted evaluation, in particular of coreference in MT, has proved to be time-consuming and laborious (Guillou, 2016). In this article, we address the evaluation of discourse phenomena for MT and propose a novel contextual model. We present two hand-crafted, discourse test sets designed to test models’ capacity to exploit linguistic context for coreference and coherence/cohesion for English to French translation. Using these sets, we review contextual NMT strategies trained on subtitles in a high-resource setting. Our new combination of strategies outperforms previous methods according to our targeted evaluation and the standard metric BLEU.",
       "charge": false
     },
     {
       "title": "Context-Aware Neural Machine Translation Learns Anaphora Resolution",
       "author":["Elena Voita", "Pavel Serdyukov", "Rico Sennrich", "Ivan Titov"] ,
       "year": 2018,
       "abstract": "Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).",
       "introduction": "It has long been argued that handling discourse phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation.\n\nEarlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-to-end fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018).\n\nWhile the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform the development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited.\n\nIn our work, we aim to enhance our understanding of the modeling of selected discourse phenomena in NMT. To this end, we construct a simple discourse-aware model, demonstrate that it achieves improvements over the discourse-agnostic baseline on an English-Russian subtitles dataset (Lison et al., 2018), and study which context information is being captured in the model. Specifically, we start with the Transformer (Vaswani et al., 2017), a state-of-the-art model for context-agnostic NMT, and modify it in such a way that it can handle additional context. In our model, a source sentence and a context sentence are first encoded independently, and then a single attention layer, in combination with a gating function, is used to produce a context-aware representation of the source sentence. The information from context can only flow through this attention layer. When compared to simply concatenating input sentences, as proposed by Tiedemann and Scherrer (2017), our architecture appears both more accurate (+0.6 BLEU) and also guarantees that the contextual information cannot bypass the attention layer and hence remain undetected in our analysis.\n\nWe analyze what types of contextual information are exploited by the translation model. While studying the attention weights, we observe that much of the information captured by the model has to do with pronoun translation. It is not entirely surprising, as we consider translation from a language without grammatical gender (English) to a language with grammatical gender (Russian). For Russian, translated pronouns need to agree in gender with their antecedents. Moreover, since in Russian verbs agree with subjects in gender and adjectives also agree in gender with pronouns in certain frequent constructions, mistakes in translating pronouns have a major effect on the words in the produced sentences. Consequently, the standard cross-entropy training objective sufficiently rewards the model for improving pronoun translation and extracting relevant information from the context.\n\nWe use automatic co-reference systems and human annotation to isolate anaphoric cases. We observe even more substantial improvements in performance on these subsets. By comparing attention distributions induced by our model against co-reference links, we conclude that the model implicitly captures coreference phenomena, even without having any kind of specialized features which could help it in this subtask. These observations also suggest potential directions for future work. For example, effective co-reference systems go beyond relying simply on embeddings of contexts. One option would be to integrate ‘global’ features summarizing properties of groups of mentions predicted as linked in a document (Wiseman et al., 2016), or to use latent relations to trace entities across documents (Ji et al., 2017). Our key contributions can be summarized as follows:\n\nWe introduce a context-aware neural model, which is effective and has a sufficiently simple and interpretable interface between the context and the rest of the translation model.\nWe analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the model.\nBy comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly captures anaphora.",
       "charge": false
     },
     {
       "title": "When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion",
       "author":["Elena Voita", "Rico Sennrich", "Ivan Titov"] ,
       "year": 2019,
       "abstract": "Though machine translation errors caused by the lack of context beyond one sentence have long been acknowledged, the development of context-aware NMT systems is hampered by several problems. Firstly, standard metrics are not sensitive to improvements in consistency in document-level translations. Secondly, previous work on context-aware NMT assumed that the sentence-aligned parallel data consisted of complete documents while in most practical scenarios such document-level data constitutes only a fraction of the available parallel data. To address the first issue, we perform a human study on an English-Russian subtitles dataset and identify deixis, ellipsis and lexical cohesion as three main sources of inconsistency. We then create test sets targeting these phenomena. To address the second shortcoming, we consider a set-up in which a much larger amount of sentence-level data is available compared to that aligned at the document level. We introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.",
       "introduction": "With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to context-aware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm are hampered by several important problems.\n\nFirstly, it is highly non-trivial to design metrics that would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012). For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occurrences of the name match the reference translation as making them all different from the reference.\n\nSecond, most previous work on context-aware NMT has made the assumption that all the bilingual data is available at the document level. However, isolated parallel sentences are a lot easier to acquire and hence only a fraction of the parallel data will be at the document level in any practical scenario. In other words, a context-aware model trained only on document-level parallel data is highly unlikely to outperform a context-agnostic model estimated from much larger sentence-level parallel corpus. This work aims to address both these shortcomings.\n\nA context-agnostic NMT system would often produce plausible translations of isolated sentences, however, when put together in a document, these translations end up being inconsistent with each other. We investigate which linguistic phenomena cause the inconsistencies using the OpenSubtitles (Lison et al., 2018) corpus for the English-Russian language pair. We identify deixis, ellipsis, and lexical cohesion as three main sources of the violations, together amounting to about 80% of the cases. We create test sets focusing specifically on the three identified phenomena (6000 examples in total).\n\nWe show that by using a limited amount of document-level parallel data, we can already achieve substantial improvements on these benchmarks without negatively affecting performance as measured with BLEU. Our approach is inspired by the Deliberation Networks (Xia et al., 2017). In our method, the initial translation produced by a baseline context-agnostic model is refined by a context-aware system which is trained on a small document-level subset of parallel data.\n\nThe key contributions are as follows:\n\nWe analyze which phenomena cause context-agnostic translations to be inconsistent with each other.\nWe create test sets specifically addressing the most frequent phenomena.\nWe consider a novel and realistic set-up where a much larger amount of sentence-level data is available compared to that aligned at the document level.\nWe introduce a model suitable for this scenario and demonstrate that it is effective on our new benchmarks without sacrificing performance as measured with BLEU.",
       "charge": false
     },
     {
       "title": "Do Context-Aware Translation Models Pay the Right Attention?",
       "author":["Kayo Yin", "Patrick Fernandes", "Danish Pruthi", "Aditi Chaudhary", "André F. T. Martins", "Graham Neubig"] ,
       "year": 2021,
       "abstract": "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model's attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.",
       "introduction": "There is a growing consensus in machine translation research that it is necessary to move beyond sentence-level translation and incorporate document-level context (Guillou et al., 2018; Laubli et al., 2018; Toral et al., 2018). While various methods to incorporate context in neural machine translation (NMT) have been proposed (Tiedemann and Scherrer (2017); Miculicich et al. (2018); Maruf and Haffari (2018), inter alia), it is unclear whether models rely on the “right” context that is actually sufficient to disambiguate difficult translations. Even when additional context is provided, models often perform poorly on the evaluation of relatively simple discourse phenomena (Muller et al., 2018; Bawden et al., 2018; Voita et al., 2019b,a; Lopes et al., 2020) and rely on spurious word co-occurrences during the translation of polysemous words (Emelin et al., 2020). Some evidence suggests that models attend to uninformative tokens (Voita et al., 2018) and do not use contextual information adequately (Kim et al., 2019).\n\nTo understand plausibly why current NMT models are unable to fully leverage the disambiguating context they are provided, and how we can develop models that use context more effectively, we pose the following research questions: (i) In context-aware translation, what context is intrinsically useful to disambiguate hard translation phenomena such as ambiguous pronouns or word senses?; (ii) Are context-aware MT models paying attention to the relevant context or not?; and (iii) If not, can we encourage them to do so?\n\nTo answer the first question, we collect annotations of context that human translators found useful in choosing between ambiguous translation options (§3). Specifically, we ask 20 professional translators to choose the correct French translation between two contrastive translations of an ambiguous word, given an English source sentence and the previous source- and target-side sentences. The translators additionally highlight the words they found most useful to make their decision, giving an idea of the context useful in making these decisions. We collect 14K such annotations and release SCAT (“Supporting Context for Ambiguous Translations”), the first dataset of human rationales for resolving ambiguity in document-level translation. Analysis reveals that inter-sentential target context is important for pronoun translation, whereas intra-sentential source context is often sufficient for word sense disambiguation.\n\nTo answer the second question, we quantify the similarity of the attention distribution of context-aware models and the human annotations in SCAT (§4). We measure alignment between the baseline context-aware model’s attention and human rationales across various model attention heads and layers. We observe a relatively high alignment between self-attention scores from the top encoder layers and the source-side supporting context marked by translators; however, the model’s attention is poorly aligned with target-side supporting context.\n\nFor the third question, we explore a method to regularize attention towards human-annotated disambiguating context (§5). We find that attention regularization is an effective technique to encourage models to pay more attention to words humans find useful to resolve ambiguity in translations. Our models with regularized attention outperform previous context-aware baselines, improving translation quality by 0.54 BLEU, and yielding a relative improvement of 14.7% in contrastive evaluation. An example of translations from a baseline and our model, along with the supporting rationale by a professional translator is illustrated in Table 1.",
       "charge": false
     }
  ]
}
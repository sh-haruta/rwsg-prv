{"root":
  {
    "title": "Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection",
    "author": ["Christopher Clarke", "Matthew Hall", "Gaurav Mittal", "Ye Yu", "Sandra Sajeev", "Jason Mars", "Mei Chen"],
    "abstract": "Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. In this paper, we present Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. We demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. Experimental results on 3 popular hate speech classification datasets show that RBE is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding.",
    "introduction": "Content moderation is a major challenge confronting the safety of online social platforms such as Facebook, Twitter, YouTube, Twitch, etc. (Vaidya et al., 2021). Major technology corporations are increasingly allocating valuable resources towards the development of automated systems for the detection and moderation of harmful content in addition to hiring and training expert human moderators to combat the growing menace of negativity and toxicity online (Wagner and Bloomberg, 2021; Liu et al., 2022).\n\nDespite the popularity of deep learning approaches, many practical solutions used in products today are comprised of rule-based techniques based on expertly curated signals such as block lists, key phrases, and regular expressions (Gillespie, 2018; Zhang, 2019; Dada et al., 2019). Such methods are widely used due to their transparency, ease of customization, and interpretability. However, they have the disadvantage of being difficult to maintain and scale, in addition to being inherently fragile and noisy (Zhang, 2019; Davidson et al., 2017; Lee, 2022; Lai et al., 2022). Figure 1 shows an example where logical rules, while explainable in nature, face the problem of being inflexible to their context of use in natural language. While a given rule may be too specific and fail to capture different variations of usage commonly found in content online, rules can also be too broad and incorrectly block lexically similar content.\n\nIn contrast to the challenges faced by rule-based methods, data-driven deep learning approaches have shown great promise across a wide range of content moderation tasks and modalities (Malik et al., 2022; Shido et al., 2022; Lai et al., 2022). Fueled by large amounts of data and deep neural networks, these complex models are capable of learning richer representations that better generalize to unseen data. The impressive performances of these models have resulted in significant industry investment in content moderation as-a-service. Several technology companies such as Google, OpenAI, and Microsoft use these models to offer services to aid in content moderation. However, despite their significant investment, they face adoption challenges due to the inability of customers to understand how these complex models reason about their decisions (Tarasov, 2021; Haimson et al., 2021; Juneja et al., 2020). Additionally, with the increasing attention around online content moderation and distrust amongst consumers, explainability and transparency are at the forefront of demands (Kemp and Ekins, 2021; Mukherjee et al., 2022). This presents the challenging open question of how we can leverage the robustness and predictive performance of complex deep-learning models whilst allowing the transparency, customizability, and interpretability that rule-based approaches provide.\n\nPrior works such as Awasthi et al. (2020); Seo et al. (2021); Pryzant et al. (2022) have explored learning from rules for tasks such as controlling neural network learning, assisting in human annotation, and improving self-supervised learning in low data scenarios. Awasthi et al. (2020) propose a rule-exemplar training method for noisy supervision using rules. While performant in denoising over-generalized rules in the network via a soft implication loss, similar to other ML approaches, this method lacks the ability to interpret model predictions at inference time. Pryzant et al. (2022) propose a general-purpose framework for the automatic discovery and integration of symbolic rules into pre-trained models. However, these symbolic rules are derived from low-capacity ML models on a reduced feature space. While less complex than large deep neural networks, these low-capacity models are still not easily interpretable by humans. Therefore, the task of combining the explainability of rules and the predictive power of deep learning models remains an open problem.\n\nIn order to tackle this problem, we introduce Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is comprised of two neural networks, a rule encoder, and a text encoder, which jointly learn rich embedding representations for hateful content and the logical rules that govern them. Through the use of contrastive learning, our framework uses a semantic similarity objective that pairs hateful examples with clusters of rule exemplars that govern it. Through this approach, RBE is able to provide more explainable predictions by allowing for what we define as Rule-grounding. This means that our model is able to ground its predictions by showing the corresponding explainable logical rule and the exemplars that constitute that rule.\n\nWe evaluate RBE in both supervised and unsupervised settings using a suite of rulesets. Our results show that with as little as one exemplar per rule, RBE is capable of outperforming state-of-the-art hateful text classifiers across three benchmark content moderation datasets in both settings. In summary, the contributions of this paper are:\n\n• Rule By Example (RBE): a novel exemplar-based contrastive learning approach to learn from logical rules for the task of textual content moderation.\n\n• We demonstrate how RBE can be easily integrated to boost model F1-score by up to 4% on three popular hate speech classification datasets.\n\n• A detailed analysis and insights into the customizability and interpretability features of RBE to address the problem of emerging hateful content and model transparency.",
    "relatedwork": "There has been active work on detecting hate speech in language (Poletto et al., 2021; AlMakhadmeh and Tolba, 2020; Schmidt and Wiegand, 2017). Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023). However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021), thus leading to the continued utilization of less powerful but more explainable methods such as rules.\n\nPrior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this method only focuses on decreasing rule scope to solve the overgeneralization problem. It does not simultaneously tackle the over-specificity problem demonstrated in Figure 1. Finally, this method does not provide a way for interpreting model predictions during inference.\n\nSeo et al. (2021) proposes a way to control neural network training and inference via rules; however, their framework represents rules as differentiable functions requiring complex perturbations to incorporate, making it more suitable for numerical rules such as those defined in healthcare and finance, as opposed to the complex nuances of language.\n\nPryzant et al. (2022) proposes a framework for the automatic induction of symbolic rules from a small set of labeled data. However, these rules are derived from low-capacity ML models and are as a result not human-readable or explainable."},
  "leaves":
   [
     {
       "title": "Resources and benchmark corpora for hate speech detection: a systematic review",
       "author": ["Fabio Poletto", "Valerio Basile", "Manuela Sanguinetti", "Cristina Bosco", "Viviana Patti"],
       "year": 2020,
       "abstract": "Hate speech in social media is a complex phenomenon, and its detection has recently gained significant traction in the Natural Language Processing community, as evidenced by several recent review works. Annotated corpora and benchmarks are key resources, considering the vast number of supervised approaches that have been proposed. Lexica play an important role as well for the development of hate speech detection systems. In this review, we systematically analyze the resources made available by the community at large, including their development methodology, topical focus, language coverage, and other factors. The results of our analysis highlight a heterogeneous, growing landscape, marked by several issues and venues for improvement.",
       "introduction": "Within the field of AI, and Natural Language Processing (NLP) in particular, techniques for tasks related to Sentiment Analysis and Opinion Mining (SA&OM) have grown in relevance over the past decades. These techniques are typically motivated by purposes such as extracting users’ opinions on a given product or polling political stances. Robust and effective approaches are made possible by the rapid progress in supervised learning technologies and the vast amount of user-generated content available online, especially on social media.\n\nMore recently, the NLP community has witnessed a growing interest in tasks related to social and ethical issues, encouraged by the global commitment to fighting extremism, violence, fake news, and other plagues affecting the online environment. One such phenomenon is hate speech, a toxic discourse stemming from prejudices and intolerance that can lead to episodes, and even structured policies, of violence, discrimination, and persecution.\n\nHate Speech (HS), lying at the intersection of multiple tensions as an expression of conflicts between different groups within and across societies, is a phenomenon that can easily proliferate on social media. It is an example of how technologies with transformative potential are loaded with both opportunities and challenges. HS implies a complex balance between freedom of expression and defense of human dignity, making it a hotly debated issue in the AI community, which can play a leading role in developing tools to confront dangerous trends such as the escalation of violence and hatred in online communication or the spread of fake news.\n\nThe motivation to study HS from a computational perspective is manifold. On one hand, as a linguistic and pragmatic phenomenon, computational linguistic techniques enable scholars to gain insights and empirical evidence on its intrinsic characteristics. On the other hand, various actors, including institutions and ICT companies complying with governments’ demands to counteract the HS phenomenon, have an increasing need for automatic support for moderation or for monitoring and mapping the dynamics and diffusion of HS dynamics over a territory (Capozzi et al. 2019), which is only possible at a large scale by employing computational methods.\n\nHS is a complex and multi-faceted notion that has proven difficult to recognize, both by humans and machines. Researchers tackling this issue from an NLP perspective are designing operational frameworks for HS, annotating corpora with several semantic frameworks, figuring out the most representative features, and testing automatic classifiers. The involvement of the scientific community has resulted in a number of evaluation tasks organized in different languages, releasing benchmark corpora and encouraging participants to develop their own classification systems. \n\n\nBeing the subject in a yet recent stage, it suffers from several weaknesses, related to both the specific targets and nuances of HS and the nature of the classification task at large, that prevent systems from reaching optimal results. One of the major issues consists in the intrinsic complexity in defining HS and in a widespread vagueness in the use of related terms (such as abusive, toxic, dangerous, offensive or aggressive language), that often overlap and are prone to strongly subjective interpretations. As we will also show in the present survey, this results in a sparsity of heterogeneous resources each reflecting a subjective perception, and in a variety of systems each trained on a different resource. Given the considerable amount of research produced in recent years, we undertook the task of writing a systematic and up-to-date review on the subject, focusing on shared tasks organized and resources released so far for HS detection. Purposes of a systematic survey include summarizing existing work, helping identify gaps and weaknesses in current research, suggesting areas for further investigation, and providing a solid framework for improving NLP research on HS detection. \nThis contribution aims at complementing other surveys proposed in this field, in particular by Lucas (2014), Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). In fact, we analyzed their work, bearing in mind a number of objective questions meant to help point out their strengths and weaknesses. In doing so, we focused in particular on the main reviews’ objectives, the sources and depth of the search of the reviewed studies, the inclusion/exclusion criteria adopted to select these studies, how data were extracted, synthesized and combined, and whether conclusions flow from the evidence. These reviews mention either explicit research questions, open issues or suggestions about future work, and are conducted with varying degrees of systematicity. Overall, their main objective is to provide an overview of the approaches proposed in literature for automatic HS detection, focusing either on high-level descriptions of methods (Lucas 2014) or on specific computational approaches, with a special emphasis on NLP (Fortuna and Nunes 2018; Schmidt and Wiegand 2017), thus analyzing models, features and algorithms. As regards the sources and depth of the search, in Schmidt and Wiegand (2017)\nthere is no explicit mention of how sources were explored, and in Lucas (2014) potential sources have been admittedly overlooked, while in Fortuna and Nunes (2018) the methodology was meant to be systematic and aimed at finding as many documents as possible in the areas of interest (computer science and engineering). Among these three surveys, the latter is also the only one that states explicit inclusion/exclusion criteria to select the studies and that reports numerical results from the surveyed papers. The conclusions drawn from such results are that it is not clear which approaches perform better, also due to differences in the datasets used (among other factors). The need for benchmark datasets that allow comparative studies is also highlighted in Schmidt and Wiegand (2017). However, it must be noted that many of the resources included in this survey had not yet been released when the previous surveys were published (or, at least, when their search was carried out), especially those released for shared tasks—which proves, once again, how dynamic and fast—growing the field is. What is more important, a large proportion of HS resources developed in the recent past includes data in languages other than English, thus broadening the HS detection scenario to a multiplicity of linguistic—as well as cultural—perspectives. Such linguistic diversity, on the other hand, also confirms the need to provide a complete picture of the resources available to the research community, especially for those aiming to adopt multilingual approaches. In this respect, it is worth mentioning a repository2 that attempts to gather all the corpora on HS and related phenomena that have been released so far, cataloguing them according to the language involved. Such repository, however, just provides a list with concise information on the datasets to those interested in using the data for computational purposes. To the best of our knowledge, a complete overview of such resources that would also take into account of different viewpoints and dimensions is still missing. This work aims therefore at providing a more\ncomprehensive view of the datasets, lexica and evaluation campaigns that are centered on the notion of HS. Furthermore, similarly to what has been done in Fortuna and Nunes (2018) with respect to papers on HS detection, we apply a systematic approach based on explicit research and evaluation criteria, in order to draw conclusions on the state of the art and suggestions for future work that can only emerge from a comprehensive analysis of the subject.\nThis paper describes first how the research was conducted, analyzing the criteria adopted and the search results (Sect. 2). It then provides an overview of the resources found (Sects. 3 and 4), also proposing a lexical analysis of some of them (Sect. 5), aiming to highlight how topic biases can be pervasive in such kind of resources. Some concluding remarks (Sect. 6), drawn from the survey findings, close the paper.",
       "charge": false
     },
     {
       "title": "Automatic hate speech detection using killer natural language processing optimizing ensemble deep learning approach",
       "author":["Zafer Al-Makhadmeh", "Amr Tolba"] ,
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": true
     },
     {
       "title": "A Survey on Hate Speech Detection using Natural Language Processing",
       "author":["Anna Schmidt", "Michael Wiegand"] ,
       "year": 2017,
       "abstract": "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
       "introduction": "Hate speech is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic (Nockleby, 2000). Examples are (1)-(3). (1) Go fucking kill yourself and die already useless ugly pile of shit scumbag. (2) The Jew Faggot Behind The Financial Collapse (3) Hope one of those bitches falls over and breaks her leg Due to the massive rise of user-generated web content, in particular on social media networks, the amount of hate speech is also steadily increasing. Over the past years, interest in online hate speech detection and particularly the automatization of this task has continuously grown, along with the societal impact of the phenomenon. Natural language processing focusing specifically on this phenomenon is required since basic word filters do not provide a sufficient remedy: What is considered a hate speech message might be influenced by aspects such as the domain of an utterance, its discourse context, as well as context consisting of co-occurring media objects (e.g. images, videos, audio), the exact time of posting and world events at this moment, identity of author and targeted recipient. This paper provides a short, comprehensive and structured overview of automatic hate speech detection, and outlines the existing approaches in a systematic manner, focusing on feature extraction in particular. It is mainly aimed at NLP researchers who are new to the field of hate speech detection and want to inform themselves about the state of the art.",
       "charge": false
     },
     {
       "title": "Introducing CAD: the Contextual Abuse Dataset",
       "author":["Bertie Vidgen", "Dong Nguyen", "Helen Margetts", "Patricia Rossini", "Rebekah Tromble"] ,
       "year": 2021,
       "abstract": "Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available.",
       "introduction": "Social media platforms have enabled unprecedented connectivity, communication and interaction for their users. However, they often harbor harmful content such as abuse and hate, inflicting myriad harms on online users (Waseem and Hovy, 2016; Schmidt and Wiegand, 2017a; Fortuna and Nunes, 2018; Vidgen et al., 2019c). Automated techniques for detecting and classifying such content increasingly play an important role in moderating online spaces.\n\nDetecting and classifying online abuse is a complex and nuanced task which, despite many advances in the power and availability of computational tools, has proven remarkably difficult (Vidgen et al., 2019a; Wiegand et al., 2019; Schmidt and Wiegand, 2017b; Waseem et al., 2017). As Jurgens et al. (2019) argued in a recent review, research has 'struggled to move beyond the most obvious tasks in abuse detection.' One of the biggest barriers to creating higher performing, more robust, nuanced and generalizable classification systems is the lack of clearly annotated, large and detailed training datasets. However, creating such datasets is time-consuming, complicated and expensive, and requires a mix of both social and computational expertise.\n\nWe present a new annotated dataset of ∼25,000 Reddit entries. It contains four innovations that address limitations of previous labeled abuse datasets. First, we present a taxonomy with six conceptually distinct primary categories (Identity-directed, Person-directed, Affiliation-directed, Counter Speech, Non-hateful Slurs and Neutral). We also provide salient subcategories, such as whether personal abuse is directed at a person in the conversation thread or to someone outside it. This taxonomy offers greater coverage and granularity of abuse than previous work. Each entry can be assigned to multiple primary and/or secondary categories (Section 3). Second, we annotate content in context, by which we mean that each entry is annotated in the context of the conversational thread it is part of. Every annotation has a label for whether contextual information was needed to make the annotation. To our knowledge, this is the first work on online abuse to incorporate a deep level of context. Third, annotators provided rationales. For each entry they highlighted the part of the text which contains the abuse (and the relevant parts for Counter Speech and Non-hateful Slurs). Fourth, we provide high-quality annotations by using a team of trained annotators and a time-intensive discussion-based process, facilitated by experts, for adjudicating disagreements (Section 4). This work addresses the need for granular and nuanced abusive content datasets, advancing efforts to create accurate, robust, and generalizable classification systems. We report several baseline models to benchmark the work of future researchers (Section 5). The annotated dataset, annotation codebook and code have been made available.1 A full description of the dataset is given in our data statement in the Appendix (Bender and Friedman, 2018).",
       "charge": false
     },
     {
       "title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection",
       "author":["Binny Mathew", "Punyajoy Saha", "Seid Muhie Yimam", "Chris Biemann", "Pawan Goyal", "Animesh Mukherjee"] ,
       "year": 2022,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "A curated dataset for hate speech detection on social media text",
       "author":["Devansh Mody", "YiDong Huang", "Thiago Eustaquio"] ,
       "year": 2023,
       "abstract": "Social media platforms have become the most prominent medium for spreading hate speech, primarily through hateful textual content. An extensive dataset containing emoticons, emojis, hashtags, slang, and contractions is required to detect hate speech on social media based on current trends. Therefore, our dataset is curated from various sources like Kaggle, GitHub, and other websites. This dataset contains hate speech sentences in English and is confined into two classes, one representing hateful content and the other representing non-hateful content. It has 451,709 sentences in total. 371,452 of these are hate speech, and 80,250 are non-hate speech. An augmented balanced dataset with 726,120 samples is also generated to create a custom vocabulary of 145,046 words. The total number of contractions considered in the dataset is 6403. The total number of bad words usually used in hateful content is 377. The text in each sentence of the final dataset, which is utilized for training and cross-validation, is limited to 180 words. The generated contractions dataset can be used for any projects in the area of NLP for data preprocessing. The augmented dataset can help to reduce the number of out-of-vocabulary words, and the hate speech dataset can be used as a classifier to detect hate or no hate on social media platforms.",
       "introduction": "Social media platforms have become the most prominent medium for spreading hate speech, primarily through hateful textual content. An extensive dataset containing emoticons, emojis, hashtags, slang, and contractions is required to detect hate speech on social media based on current trends. Therefore, our dataset is curated from various sources like Kaggle, GitHub, and other websites. This dataset contains hate speech sentences in English and is confined into two classes, one representing hateful content and the other representing non-hateful content. It has 451,709 sentences in total. 371,452 of these are hate speech, and 80,250 are non-hate speech. An augmented balanced dataset with 726,120 samples is also generated to create a custom vocabulary of 145,046 words. The total number of contractions considered in the dataset is 6403. The total number of bad words usually used in hateful content is 377. The text in each sentence of the final dataset, which is utilized for training and cross-validation, is limited to 180 words. The generated contractions dataset can be used for any projects in the area of NLP for data preprocessing. The augmented dataset can help to reduce the number of out-of-vocabulary words, and the hate speech dataset can be used as a classifier to detect hate or no hate on social media platforms.",
       "charge": false
     },
     {
       "title": "Why content moderation costs billions and is so tricky for Facebook, Twitter, YouTube and others",
       "author":["Katie Tarasov"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false,
       "remarks": "https://www.cnbc.com/2021/02/27/content-moderation-on-social-media.html"
     },
     {
       "title": "Disproportionate Removals and Differing Content Moderation Experiences for Conservative, Transgender, and Black Social Media Users: Marginalization and Moderation Gray Areas",
       "author":["Oliver L. Haimson", "Daniel Delmonaco", "Peipei Nie", "Andrea Wegner"] ,
       "year": 2021,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Learning from Rules Generalizing Labeled Exemplars",
       "author":["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"] ,
       "year": 2020,
       "abstract": "",
       "introduction": "",
       "charge": false
     },
     {
       "title": "Controlling Neural Networks with Rule Representations",
       "author":["Sungyong Seo", "Sercan O. Arik", "Jinsung Yoon", "Xiang Zhang", "Kihyuk Sohn", "Tomas Pfister"] ,
       "year": 2021,
       "abstract": "We propose a novel training method that integrates rules into deep learning, in a way the strengths of the rules are controllable at inference. Deep Neural Networks with Controllable Rule Representations (DeepCTRL) incorporates a rule encoder into the model coupled with a rule-based objective, enabling a shared representation for decision making. DeepCTRL is agnostic to data type and model architecture. It can be applied to any kind of rule defined for inputs and outputs. The key aspect of DeepCTRL is that it does not require retraining to adapt the rule strength -- at inference, the user can adjust it based on the desired operation point on accuracy vs. rule verification ratio. In real-world domains where incorporating rules is critical -- such as Physics, Retail and Healthcare -- we show the effectiveness of DeepCTRL in teaching rules for deep learning. DeepCTRL improves the trust and reliability of the trained models by significantly increasing their rule verification ratio, while also providing accuracy gains at downstream tasks. Additionally, DeepCTRL enables novel use cases such as hypothesis testing of the rules on data samples, and unsupervised adaptation based on shared rules between datasets.",
       "introduction": "Deep neural networks (DNNs) excel at numerous tasks such as image classification [28, 29], machine translation [22, 30], time series forecasting [11, 21], and tabular learning [2, 25]. DNNs get more accurate as the size and coverage of training data increase [17]. While investing in high-quality and large-scale labeled data is one path, another is utilizing prior knowledge – concisely referred to as ‘rules’: reasoning heuristics, equations, associative logic, constraints, or blacklists. In most scenarios, labeled datasets are not sufficient to teach all rules present about a task [4, 12, 23, 24]. Let us consider an example from Physics: the task of predicting the next state in a double pendulum system, visualized in Fig. 1. Although a ‘data-driven’ black-box model, fitted with conventional supervised learning, can fit a relatively accurate mapping from the current state to next, it can easily fail to capture the canonical rule of ‘energy conservation’. In this work, we focus on how to teach ‘rules’ in effective ways so that DNNs absorb the knowledge from them in addition to learning from the data for the downstream task.\n\nThe benefits of learning from rules are multifaceted. First, rules can provide extra information for cases with minimal data supervision, improving the test accuracy. Second, the rules can improve trust and reliability of DNNs. One major bottleneck for widespread use of DNNs is them being ‘black-box’. The lack of understanding of the rationale behind their reasoning and inconsistencies of their outputs with human judgment often reduce the trust of the users [3, 26]. By incorporating rules, such inconsistencies can be minimized, and the users’ trust can be improved. For example, if a DNN for loan delinquency prediction can absorb all the decision heuristics used at a bank, the loan officers of the bank can rely on the predictions more comfortably. Third, DNNs are sensitive to slight changes to the inputs that are human-imperceptible [15, 20, 31]. With rules, the impact of these changes can be minimized as the model search space is further constrained to reduce underspecification [7, 10].\n\nWhen ‘data-driven’ and ‘rule-driven’ learning are considered jointly, a fundamental question is how to balance the contribution from each. Even when a rule is known to hold 100% of the time (such as the principles in natural sciences), the contribution of rule-driven learning should not be increased arbitrarily. There is an optimal trade-off that depends not only on the dataset but also on each sample. If there are training samples that are very similar to a particular test sample, a weaker rule-driven contribution would be desirable at inference. On the other hand, if the rule is known to hold for only a subset of samples (e.g. in Retail, the varying impact of a price change on different products [6]), the strength of the rule-driven learning contribution should reflect that. Thus, a framework where the contributions of data- and rule-driven learning can be controlled would be valuable. Ideally, such control should be enabled at inference without the need for retraining to minimize the computational cost, shorten the deployment time, and to adjust to different samples or changing distributions flexibly.\n\nIn this paper, we propose DEEPCTRL that enables joint learning from labeled data and rules. DEEPCTRL employs separate encoders for data and rules with the outputs combined stochastically to cover intermediate representations coupling with corresponding objectives. This representation learning is the key to controllability, as it allows increasing/decreasing the rule strength gradually at inference without retraining. To convert any non-differentiable rules into differentiable objectives, we propose a novel perturbation-based method. DEEPCTRL is agnostic to the data type or the model architecture, and DEEPCTRL can be flexibly used in different tasks and with different rules. We demonstrate DEEPCTRL on important use cases from Physics, Retail, and Healthcare, and show that it: (i) improves the rule verification ratio significantly while yielding better accuracy by merely changing the rule strength at inference; (ii) enables hypotheses to be examined for each sample based on the optimal ratio of rule strength without retraining (for the first time in literature, to the best of our knowledge); and (iii) improves target task performance by changing the rule strength, a desired capability when different subsets of the data are known to satisfy rules with different strengths.",
       "charge": false
     },
     {
       "title": "Automatic Rule Induction for Efficient Semi-Supervised Learning",
       "author":["Reid Pryzant", "Ziyi Yang", "Yichong Xu", "Chenguang Zhu","Michael Zeng"] ,
       "year": 2022,
       "abstract": "Semi-supervised learning has shown promise in allowing NLP models to generalize from small amounts of labeled data. Meanwhile, pretrained transformer models act as black-box correlation engines that are difficult to explain and sometimes behave unreliably. In this paper, we propose tackling both of these challenges via Automatic Rule Induction (ARI), a simple and general-purpose framework for the automatic discovery and integration of symbolic rules into pretrained transformer models. First, we extract weak symbolic rules from low-capacity machine learning models trained on small amounts of labeled data. Next, we use an attention mechanism to integrate these rules into high-capacity pretrained transformer models. Last, the rule-augmented system becomes part of a self-training framework to boost supervision signal on unlabeled data. These steps can be layered beneath a variety of existing weak supervision and semi-supervised NLP algorithms in order to improve performance and interpretability. Experiments across nine sequence classification and relation extraction tasks suggest that ARI can improve state-of-the-art methods with no manual effort and minimal computational overhead.",
       "introduction": "Large-scale pretrained neural networks can struggle to generalize from small amounts of labeled data (Devlin et al., 2019), motivating approaches that leverage both labeled and unlabeled data. This is partially due to the black-box and correlational nature of neural networks, which confers the additional difficulties of uninterpretability (Bolukbasi et al., 2021) and unreliability (Sagawa et al., 2020). A growing body of research seeks to ameliorate these issues by augmenting neural networks with symbolic components: heuristics, logical formulas, program traces, network templating, blacklists, etc (Arabshahi et al., 2018; Galassi et al., 2020; Wang et al., 2021). In this paper, we refer to these components as rules. Symbolic reasoning has attractive properties. Rules need little or no data to systematically generalize, and rules are inherently interpretable with respect to their constituent operations.\n\nIn this paper, we propose a general-purpose framework for the automatic discovery and integration of symbolic rules into pretrained models. The framework contrasts with prior neuro-symbolic NLP research in two ways. First, we present a fully automatic rule generation procedure, whereas prior work has largely focused on manually crafted rules (Mekala and Shang, 2020; Awasthi et al., 2020; Li et al., 2021) or semi-manual rule generation procedures (Boecking et al., 2020; Galhotra et al., 2021; Zhang et al., 2022). With these existing techniques, practitioners must formulate and implement their rules by hand, creating a second-order “rule annotation” burden on top of the data labeling process. Second, the proposed framework is general-purpose and can be applied to any classification dataset. This contrasts with prior research that proposes task- and domain-specific symbolic logic, through weak supervision signals (Ratner et al., 2017; Awasthi et al., 2020; Safranchik et al., 2020), special loss functions (Xu et al., 2018), model architectures (Seo et al., 2021), and prompt templates (Schick and Schütze, 2020a).\n\nOur framework consists of two steps. First, we generate symbolic rules from data. This involves training low-capacity machine learning models on a reduced feature space, extracting artifacts from these models which are predictive of the class labels, then converting these artifacts into rules. After the rule induction step, we use the induced rules to amplify training signal in the unlabeled data. In particular, we adopt a rule-augmented self-training procedure, using an attention mechanism to aggregate the predictions of a backbone classifier (e.g. BERT) and the rules.\n\nWe evaluate the ARI framework across nine text classification and relation extraction tasks. The results suggest that the proposed algorithm can exceed state-of-the-art semi-supervised baselines, and that these gains may be because the model learns to rely more heavily on rules for difficult-to-predict examples. We also show that the proposed rule induction strategy can rival human-crafted rules in terms of their quality. Last, we demonstrate the interpretability of the overall system. In summary, the contributions of this paper are:\n\nMethods for automatically inducing and filtering symbolic rules from data.\nA self-training algorithm and attention mechanism for incorporating these rules into pretrained NLP models.\nEvidence suggesting the proposed framework can be layered beneath a number of existing algorithms to boost performance and interpretability.",
       "charge": false
     }
   ]
}